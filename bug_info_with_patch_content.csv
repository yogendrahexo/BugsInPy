"project_name","bug_id","buggy_commit_id","fixed_commit_id","buggy_files","fixed_files","diff_commands","test_file_bug_info","test_file_description","patch_content"
"fastapi","14","2ddb804940bbcad4ed730b2a910c8fd3c1167127","b7d184363fad5f6b55d15ec3a3a844aa81092bbd","fastapi/openapi/models.py","fastapi/openapi/models.py","diff --git a/fastapi/openapi/models.py b/fastapi/openapi/models.py","tests/test_additional_properties.py","","diff --git a/fastapi/openapi/models.py b/fastapi/openapi/models.py
index 6572c7c..a045bdf 100644
--- a/fastapi/openapi/models.py
+++ b/fastapi/openapi/models.py
@@ -99,7 +99,7 @@ class SchemaBase(BaseModel):
     not_: Optional[List[Any]] = PSchema(None, alias=""not"")  # type: ignore
     items: Optional[Any] = None
     properties: Optional[Dict[str, Any]] = None
-    additionalProperties: Optional[Union[bool, Any]] = None
+    additionalProperties: Optional[Union[Dict[str, Any], bool]] = None
     description: Optional[str] = None
     format: Optional[str] = None
     default: Optional[Any] = None
@@ -120,7 +120,7 @@ class Schema(SchemaBase):
     not_: Optional[List[SchemaBase]] = PSchema(None, alias=""not"")  # type: ignore
     items: Optional[SchemaBase] = None
     properties: Optional[Dict[str, SchemaBase]] = None
-    additionalProperties: Optional[Union[bool, SchemaBase]] = None
+    additionalProperties: Optional[Union[SchemaBase, bool]] = None
 
 
 class Example(BaseModel):
@@ -220,7 +220,7 @@ class Operation(BaseModel):
     operationId: Optional[str] = None
     parameters: Optional[List[Union[Parameter, Reference]]] = None
     requestBody: Optional[Union[RequestBody, Reference]] = None
-    responses: Union[Responses, Dict[Union[str], Response]]
+    responses: Union[Responses, Dict[str, Response]]
     # Workaround OpenAPI recursive reference
     callbacks: Optional[Dict[str, Union[Dict[str, Any], Reference]]] = None
     deprecated: Optional[bool] = None
"
"fastapi","3","869c7389e22dc9ad659940fa271da76c4f3ba3b1","aea04ee32ee1942e6e1a904527bb8da6ba76abd9","fastapi/routing.py","fastapi/routing.py","diff --git a/fastapi/routing.py b/fastapi/routing.py","tests/test_serialize_response_model.py","","diff --git a/fastapi/routing.py b/fastapi/routing.py
index b361048..b90935e 100644
--- a/fastapi/routing.py
+++ b/fastapi/routing.py
@@ -48,6 +48,28 @@ except ImportError:  # pragma: nocover
     from pydantic.fields import Field as ModelField  # type: ignore
 
 
+def _prepare_response_content(
+    res: Any, *, by_alias: bool = True, exclude_unset: bool
+) -> Any:
+    if isinstance(res, BaseModel):
+        if PYDANTIC_1:
+            return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)
+        else:
+            return res.dict(
+                by_alias=by_alias, skip_defaults=exclude_unset
+            )  # pragma: nocover
+    elif isinstance(res, list):
+        return [
+            _prepare_response_content(item, exclude_unset=exclude_unset) for item in res
+        ]
+    elif isinstance(res, dict):
+        return {
+            k: _prepare_response_content(v, exclude_unset=exclude_unset)
+            for k, v in res.items()
+        }
+    return res
+
+
 async def serialize_response(
     *,
     field: ModelField = None,
@@ -60,13 +82,9 @@ async def serialize_response(
 ) -> Any:
     if field:
         errors = []
-        if exclude_unset and isinstance(response_content, BaseModel):
-            if PYDANTIC_1:
-                response_content = response_content.dict(exclude_unset=exclude_unset)
-            else:
-                response_content = response_content.dict(
-                    skip_defaults=exclude_unset
-                )  # pragma: nocover
+        response_content = _prepare_response_content(
+            response_content, by_alias=by_alias, exclude_unset=exclude_unset
+        )
         if is_coroutine:
             value, errors_ = field.validate(response_content, {}, loc=(""response"",))
         else:
"
"fastapi","16","92c825be6a7362099400c9c3fe8b01ea13add3dc","9745a5d1ae86a7fefacf79bdde8e5dd2d59fa2f4","fastapi/encoders.py","fastapi/encoders.py","diff --git a/fastapi/encoders.py b/fastapi/encoders.py","tests/test_datetime_custom_encoder.py;tests/test_jsonable_encoder.py","","diff --git a/fastapi/encoders.py b/fastapi/encoders.py
index 82e3ffa..f5059a7 100644
--- a/fastapi/encoders.py
+++ b/fastapi/encoders.py
@@ -15,17 +15,12 @@ def jsonable_encoder(
     custom_encoder: dict = {},
 ) -> Any:
     if isinstance(obj, BaseModel):
-        if not obj.Config.json_encoders:
-            return jsonable_encoder(
-                obj.dict(include=include, exclude=exclude, by_alias=by_alias),
-                include_none=include_none,
-            )
-        else:
-            return jsonable_encoder(
-                obj.dict(include=include, exclude=exclude, by_alias=by_alias),
-                include_none=include_none,
-                custom_encoder=obj.Config.json_encoders,
-            )
+        encoder = getattr(obj.Config, ""json_encoders"", custom_encoder)
+        return jsonable_encoder(
+            obj.dict(include=include, exclude=exclude, by_alias=by_alias),
+            include_none=include_none,
+            custom_encoder=encoder,
+        )
     if isinstance(obj, Enum):
         return obj.value
     if isinstance(obj, (str, int, float, type(None))):
"
"fastapi","2","210af1fd3dc0f612a08fa02a0cb3f5adb81e5bfb","02441ff0313d5b471b662293244c53e712f1243f","fastapi/routing.py","fastapi/routing.py","diff --git a/fastapi/routing.py b/fastapi/routing.py","tests/test_ws_router.py","","diff --git a/fastapi/routing.py b/fastapi/routing.py
index b90935e..1ec0b69 100644
--- a/fastapi/routing.py
+++ b/fastapi/routing.py
@@ -498,7 +498,12 @@ class APIRouter(routing.Router):
     def add_api_websocket_route(
         self, path: str, endpoint: Callable, name: str = None
     ) -> None:
-        route = APIWebSocketRoute(path, endpoint=endpoint, name=name)
+        route = APIWebSocketRoute(
+            path,
+            endpoint=endpoint,
+            name=name,
+            dependency_overrides_provider=self.dependency_overrides_provider,
+        )
         self.routes.append(route)
 
     def websocket(self, path: str, name: str = None) -> Callable:
"
"fastapi","8","fdb6d43e103bcf7a7325d796e37c9435c9460e4c","dd963511d699b463b416408a0ad705b3dda0d067","fastapi/routing.py","fastapi/routing.py","diff --git a/fastapi/routing.py b/fastapi/routing.py","tests/test_custom_route_class.py","","diff --git a/fastapi/routing.py b/fastapi/routing.py
index 8f61ea5..b090231 100644
--- a/fastapi/routing.py
+++ b/fastapi/routing.py
@@ -348,8 +348,10 @@ class APIRouter(routing.Router):
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
+        route_class_override: Optional[Type[APIRoute]] = None,
     ) -> None:
-        route = self.route_class(
+        route_class = route_class_override or self.route_class
+        route = route_class(
             path,
             endpoint=endpoint,
             response_model=response_model,
@@ -487,6 +489,7 @@ class APIRouter(routing.Router):
                     include_in_schema=route.include_in_schema,
                     response_class=route.response_class or default_response_class,
                     name=route.name,
+                    route_class_override=type(route),
                 )
             elif isinstance(route, routing.Route):
                 self.add_route(
"
"fastapi","1","766157bfb4e7dfccba09ab398e8ec444d14e947c","3397d4d69a9c2d64c1219fcbf291ea5697a4abb8","fastapi/applications.py;fastapi/encoders.py;fastapi/openapi/utils.py;fastapi/routing.py;tests/test_skip_defaults.py","fastapi/applications.py;fastapi/encoders.py;fastapi/openapi/utils.py;fastapi/routing.py;tests/test_skip_defaults.py","diff --git a/fastapi/applications.py b/fastapi/applications.py;diff --git a/fastapi/encoders.py b/fastapi/encoders.py;diff --git a/fastapi/openapi/utils.py b/fastapi/openapi/utils.py;diff --git a/fastapi/routing.py b/fastapi/routing.py;diff --git a/tests/test_skip_defaults.py b/tests/test_skip_defaults.py","tests/test_jsonable_encoder.py","","diff --git a/fastapi/applications.py b/fastapi/applications.py
index 8270e54..84a1b6d 100644
--- a/fastapi/applications.py
+++ b/fastapi/applications.py
@@ -171,6 +171,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -197,6 +199,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -222,6 +226,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -250,6 +256,8 @@ class FastAPI(Starlette):
                 response_model_exclude_unset=bool(
                     response_model_exclude_unset or response_model_skip_defaults
                 ),
+                response_model_exclude_defaults=response_model_exclude_defaults,
+                response_model_exclude_none=response_model_exclude_none,
                 include_in_schema=include_in_schema,
                 response_class=response_class or self.default_response_class,
                 name=name,
@@ -309,6 +317,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -334,6 +344,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -359,6 +371,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -384,6 +398,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -409,6 +425,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -434,6 +452,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -459,6 +479,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -484,6 +506,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -509,6 +533,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -534,6 +560,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -559,6 +587,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -584,6 +614,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -609,6 +641,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -634,6 +668,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -659,6 +695,8 @@ class FastAPI(Starlette):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -684,6 +722,8 @@ class FastAPI(Starlette):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
diff --git a/fastapi/encoders.py b/fastapi/encoders.py
index ae4794b..26ceb21 100644
--- a/fastapi/encoders.py
+++ b/fastapi/encoders.py
@@ -34,7 +34,8 @@ def jsonable_encoder(
     by_alias: bool = True,
     skip_defaults: bool = None,
     exclude_unset: bool = False,
-    include_none: bool = True,
+    exclude_defaults: bool = False,
+    exclude_none: bool = False,
     custom_encoder: dict = {},
     sqlalchemy_safe: bool = True,
 ) -> Any:
@@ -58,8 +59,12 @@ def jsonable_encoder(
                 exclude=exclude,
                 by_alias=by_alias,
                 exclude_unset=bool(exclude_unset or skip_defaults),
+                exclude_none=exclude_none,
+                exclude_defaults=exclude_defaults,
             )
         else:  # pragma: nocover
+            if exclude_defaults:
+                raise ValueError(""Cannot use exclude_defaults"")
             obj_dict = obj.dict(
                 include=include,
                 exclude=exclude,
@@ -68,7 +73,8 @@ def jsonable_encoder(
             )
         return jsonable_encoder(
             obj_dict,
-            include_none=include_none,
+            exclude_none=exclude_none,
+            exclude_defaults=exclude_defaults,
             custom_encoder=encoder,
             sqlalchemy_safe=sqlalchemy_safe,
         )
@@ -87,14 +93,14 @@ def jsonable_encoder(
                     or (not isinstance(key, str))
                     or (not key.startswith(""_sa""))
                 )
-                and (value is not None or include_none)
+                and (value is not None or not exclude_none)
                 and ((include and key in include) or key not in exclude)
             ):
                 encoded_key = jsonable_encoder(
                     key,
                     by_alias=by_alias,
                     exclude_unset=exclude_unset,
-                    include_none=include_none,
+                    exclude_none=exclude_none,
                     custom_encoder=custom_encoder,
                     sqlalchemy_safe=sqlalchemy_safe,
                 )
@@ -102,7 +108,7 @@ def jsonable_encoder(
                     value,
                     by_alias=by_alias,
                     exclude_unset=exclude_unset,
-                    include_none=include_none,
+                    exclude_none=exclude_none,
                     custom_encoder=custom_encoder,
                     sqlalchemy_safe=sqlalchemy_safe,
                 )
@@ -118,7 +124,8 @@ def jsonable_encoder(
                     exclude=exclude,
                     by_alias=by_alias,
                     exclude_unset=exclude_unset,
-                    include_none=include_none,
+                    exclude_defaults=exclude_defaults,
+                    exclude_none=exclude_none,
                     custom_encoder=custom_encoder,
                     sqlalchemy_safe=sqlalchemy_safe,
                 )
@@ -153,7 +160,8 @@ def jsonable_encoder(
         data,
         by_alias=by_alias,
         exclude_unset=exclude_unset,
-        include_none=include_none,
+        exclude_defaults=exclude_defaults,
+        exclude_none=exclude_none,
         custom_encoder=custom_encoder,
         sqlalchemy_safe=sqlalchemy_safe,
     )
diff --git a/fastapi/openapi/utils.py b/fastapi/openapi/utils.py
index 91f90ec..c1e66fc 100644
--- a/fastapi/openapi/utils.py
+++ b/fastapi/openapi/utils.py
@@ -81,7 +81,7 @@ def get_openapi_security_definitions(flat_dependant: Dependant) -> Tuple[Dict, L
         security_definition = jsonable_encoder(
             security_requirement.security_scheme.model,
             by_alias=True,
-            include_none=False,
+            exclude_none=True,
         )
         security_name = security_requirement.security_scheme.scheme_name
         security_definitions[security_name] = security_definition
@@ -310,4 +310,4 @@ def get_openapi(
     if components:
         output[""components""] = components
     output[""paths""] = paths
-    return jsonable_encoder(OpenAPI(**output), by_alias=True, include_none=False)
+    return jsonable_encoder(OpenAPI(**output), by_alias=True, exclude_none=True)
diff --git a/fastapi/routing.py b/fastapi/routing.py
index 1ec0b69..3ac420e 100644
--- a/fastapi/routing.py
+++ b/fastapi/routing.py
@@ -49,22 +49,43 @@ except ImportError:  # pragma: nocover
 
 
 def _prepare_response_content(
-    res: Any, *, by_alias: bool = True, exclude_unset: bool
+    res: Any,
+    *,
+    by_alias: bool = True,
+    exclude_unset: bool,
+    exclude_defaults: bool = False,
+    exclude_none: bool = False,
 ) -> Any:
     if isinstance(res, BaseModel):
         if PYDANTIC_1:
-            return res.dict(by_alias=by_alias, exclude_unset=exclude_unset)
+            return res.dict(
+                by_alias=by_alias,
+                exclude_unset=exclude_unset,
+                exclude_defaults=exclude_defaults,
+                exclude_none=exclude_none,
+            )
         else:
             return res.dict(
-                by_alias=by_alias, skip_defaults=exclude_unset
+                by_alias=by_alias, skip_defaults=exclude_unset,
             )  # pragma: nocover
     elif isinstance(res, list):
         return [
-            _prepare_response_content(item, exclude_unset=exclude_unset) for item in res
+            _prepare_response_content(
+                item,
+                exclude_unset=exclude_unset,
+                exclude_defaults=exclude_defaults,
+                exclude_none=exclude_none,
+            )
+            for item in res
         ]
     elif isinstance(res, dict):
         return {
-            k: _prepare_response_content(v, exclude_unset=exclude_unset)
+            k: _prepare_response_content(
+                v,
+                exclude_unset=exclude_unset,
+                exclude_defaults=exclude_defaults,
+                exclude_none=exclude_none,
+            )
             for k, v in res.items()
         }
     return res
@@ -78,12 +99,18 @@ async def serialize_response(
     exclude: Union[SetIntStr, DictIntStrAny] = set(),
     by_alias: bool = True,
     exclude_unset: bool = False,
+    exclude_defaults: bool = False,
+    exclude_none: bool = False,
     is_coroutine: bool = True,
 ) -> Any:
     if field:
         errors = []
         response_content = _prepare_response_content(
-            response_content, by_alias=by_alias, exclude_unset=exclude_unset
+            response_content,
+            by_alias=by_alias,
+            exclude_unset=exclude_unset,
+            exclude_defaults=exclude_defaults,
+            exclude_none=exclude_none,
         )
         if is_coroutine:
             value, errors_ = field.validate(response_content, {}, loc=(""response"",))
@@ -103,6 +130,8 @@ async def serialize_response(
             exclude=exclude,
             by_alias=by_alias,
             exclude_unset=exclude_unset,
+            exclude_defaults=exclude_defaults,
+            exclude_none=exclude_none,
         )
     else:
         return jsonable_encoder(response_content)
@@ -131,6 +160,8 @@ def get_request_handler(
     response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),
     response_model_by_alias: bool = True,
     response_model_exclude_unset: bool = False,
+    response_model_exclude_defaults: bool = False,
+    response_model_exclude_none: bool = False,
     dependency_overrides_provider: Any = None,
 ) -> Callable:
     assert dependant.call is not None, ""dependant.call must be a function""
@@ -177,6 +208,8 @@ def get_request_handler(
                 exclude=response_model_exclude,
                 by_alias=response_model_by_alias,
                 exclude_unset=response_model_exclude_unset,
+                exclude_defaults=response_model_exclude_defaults,
+                exclude_none=response_model_exclude_none,
                 is_coroutine=is_coroutine,
             )
             response = response_class(
@@ -255,6 +288,8 @@ class APIRoute(routing.Route):
         response_model_exclude: Union[SetIntStr, DictIntStrAny] = set(),
         response_model_by_alias: bool = True,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Optional[Type[Response]] = None,
         dependency_overrides_provider: Any = None,
@@ -326,6 +361,8 @@ class APIRoute(routing.Route):
         self.response_model_exclude = response_model_exclude
         self.response_model_by_alias = response_model_by_alias
         self.response_model_exclude_unset = response_model_exclude_unset
+        self.response_model_exclude_defaults = response_model_exclude_defaults
+        self.response_model_exclude_none = response_model_exclude_none
         self.include_in_schema = include_in_schema
         self.response_class = response_class
 
@@ -352,6 +389,8 @@ class APIRoute(routing.Route):
             response_model_exclude=self.response_model_exclude,
             response_model_by_alias=self.response_model_by_alias,
             response_model_exclude_unset=self.response_model_exclude_unset,
+            response_model_exclude_defaults=self.response_model_exclude_defaults,
+            response_model_exclude_none=self.response_model_exclude_none,
             dependency_overrides_provider=self.dependency_overrides_provider,
         )
 
@@ -400,6 +439,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -429,6 +470,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -457,6 +500,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -486,6 +531,8 @@ class APIRouter(routing.Router):
                 response_model_exclude_unset=bool(
                     response_model_exclude_unset or response_model_skip_defaults
                 ),
+                response_model_exclude_defaults=response_model_exclude_defaults,
+                response_model_exclude_none=response_model_exclude_none,
                 include_in_schema=include_in_schema,
                 response_class=response_class or self.default_response_class,
                 name=name,
@@ -560,6 +607,8 @@ class APIRouter(routing.Router):
                     response_model_exclude=route.response_model_exclude,
                     response_model_by_alias=route.response_model_by_alias,
                     response_model_exclude_unset=route.response_model_exclude_unset,
+                    response_model_exclude_defaults=route.response_model_exclude_defaults,
+                    response_model_exclude_none=route.response_model_exclude_none,
                     include_in_schema=route.include_in_schema,
                     response_class=route.response_class or default_response_class,
                     name=route.name,
@@ -606,6 +655,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -632,6 +683,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -657,6 +710,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -683,6 +738,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -708,6 +765,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -734,6 +793,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -759,6 +820,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -785,6 +848,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -810,6 +875,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -836,6 +903,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -861,6 +930,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -887,6 +958,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -912,6 +985,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -938,6 +1013,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
@@ -963,6 +1040,8 @@ class APIRouter(routing.Router):
         response_model_by_alias: bool = True,
         response_model_skip_defaults: bool = None,
         response_model_exclude_unset: bool = False,
+        response_model_exclude_defaults: bool = False,
+        response_model_exclude_none: bool = False,
         include_in_schema: bool = True,
         response_class: Type[Response] = None,
         name: str = None,
@@ -989,6 +1068,8 @@ class APIRouter(routing.Router):
             response_model_exclude_unset=bool(
                 response_model_exclude_unset or response_model_skip_defaults
             ),
+            response_model_exclude_defaults=response_model_exclude_defaults,
+            response_model_exclude_none=response_model_exclude_none,
             include_in_schema=include_in_schema,
             response_class=response_class or self.default_response_class,
             name=name,
diff --git a/tests/test_skip_defaults.py b/tests/test_skip_defaults.py
index c9a85a3..ed84df6 100644
--- a/tests/test_skip_defaults.py
+++ b/tests/test_skip_defaults.py
@@ -18,11 +18,53 @@ class Model(BaseModel):
 
 class ModelSubclass(Model):
     y: int
+    z: int = 0
+    w: int = None
+
+
+class ModelDefaults(BaseModel):
+    w: Optional[str] = None
+    x: Optional[str] = None
+    y: str = ""y""
+    z: str = ""z""
 
 
 @app.get(""/"", response_model=Model, response_model_exclude_unset=True)
 def get() -> ModelSubclass:
-    return ModelSubclass(sub={}, y=1)
+    return ModelSubclass(sub={}, y=1, z=0)
+
+
+@app.get(
+    ""/exclude_unset"", response_model=ModelDefaults, response_model_exclude_unset=True
+)
+def get() -> ModelDefaults:
+    return ModelDefaults(x=None, y=""y"")
+
+
+@app.get(
+    ""/exclude_defaults"",
+    response_model=ModelDefaults,
+    response_model_exclude_defaults=True,
+)
+def get() -> ModelDefaults:
+    return ModelDefaults(x=None, y=""y"")
+
+
+@app.get(
+    ""/exclude_none"", response_model=ModelDefaults, response_model_exclude_none=True
+)
+def get() -> ModelDefaults:
+    return ModelDefaults(x=None, y=""y"")
+
+
+@app.get(
+    ""/exclude_unset_none"",
+    response_model=ModelDefaults,
+    response_model_exclude_unset=True,
+    response_model_exclude_none=True,
+)
+def get() -> ModelDefaults:
+    return ModelDefaults(x=None, y=""y"")
 
 
 client = TestClient(app)
@@ -31,3 +73,23 @@ client = TestClient(app)
 def test_return_defaults():
     response = client.get(""/"")
     assert response.json() == {""sub"": {}}
+
+
+def test_return_exclude_unset():
+    response = client.get(""/exclude_unset"")
+    assert response.json() == {""x"": None, ""y"": ""y""}
+
+
+def test_return_exclude_defaults():
+    response = client.get(""/exclude_defaults"")
+    assert response.json() == {}
+
+
+def test_return_exclude_none():
+    response = client.get(""/exclude_none"")
+    assert response.json() == {""y"": ""y"", ""z"": ""z""}
+
+
+def test_return_exclude_unset_none():
+    response = client.get(""/exclude_unset_none"")
+    assert response.json() == {""y"": ""y""}
"
"fastapi","7","cc4c13e4ae3f65fc76c23962b316df4a60e0c7e0","19c77e35bdde33aeec1eb2cfa680f95016492b69","fastapi/exception_handlers.py","fastapi/exception_handlers.py","diff --git a/fastapi/exception_handlers.py b/fastapi/exception_handlers.py","tests/test_multi_body_errors.py","","diff --git a/fastapi/exception_handlers.py b/fastapi/exception_handlers.py
index cda2f8c..2b286d7 100644
--- a/fastapi/exception_handlers.py
+++ b/fastapi/exception_handlers.py
@@ -1,3 +1,4 @@
+from fastapi.encoders import jsonable_encoder
 from fastapi.exceptions import RequestValidationError
 from starlette.exceptions import HTTPException
 from starlette.requests import Request
@@ -19,5 +20,6 @@ async def request_validation_exception_handler(
     request: Request, exc: RequestValidationError
 ) -> JSONResponse:
     return JSONResponse(
-        status_code=HTTP_422_UNPROCESSABLE_ENTITY, content={""detail"": exc.errors()}
+        status_code=HTTP_422_UNPROCESSABLE_ENTITY,
+        content={""detail"": jsonable_encoder(exc.errors())},
     )
"
"fastapi","10","b77a43bcac3ec8e7edbe82543e777c60ae85c178","38495fffa560fa57a8f0e6437d8e43c36c8e5612","fastapi/routing.py","fastapi/routing.py","diff --git a/fastapi/routing.py b/fastapi/routing.py","tests/test_skip_defaults.py","","diff --git a/fastapi/routing.py b/fastapi/routing.py
index 3c6774c..930cbe0 100644
--- a/fastapi/routing.py
+++ b/fastapi/routing.py
@@ -52,6 +52,8 @@ def serialize_response(
             errors.extend(errors_)
         if errors:
             raise ValidationError(errors)
+        if skip_defaults and isinstance(response, BaseModel):
+            value = response.dict(skip_defaults=skip_defaults)
         return jsonable_encoder(
             value,
             include=include,
"
"fastapi","13","6f7f9268f6b03f42831dcfeaa5c15ba9813333ec","c8df3ae57c57e119d115dd3c1f44efa78de1022a","fastapi/routing.py","fastapi/routing.py","diff --git a/fastapi/routing.py b/fastapi/routing.py","tests/test_additional_responses_router.py","","diff --git a/fastapi/routing.py b/fastapi/routing.py
index e768c3a..2bdf46d 100644
--- a/fastapi/routing.py
+++ b/fastapi/routing.py
@@ -285,11 +285,11 @@ class APIRouter(routing.Router):
             assert not prefix.endswith(
                 ""/""
             ), ""A path prefix must not end with '/', as the routes will start with '/'""
+        if responses is None:
+            responses = {}
         for route in router.routes:
             if isinstance(route, APIRoute):
-                if responses is None:
-                    responses = {}
-                responses = {**responses, **route.responses}
+                combined_responses = {**responses, **route.responses}
                 self.add_api_route(
                     prefix + route.path,
                     route.endpoint,
@@ -299,7 +299,7 @@ class APIRouter(routing.Router):
                     summary=route.summary,
                     description=route.description,
                     response_description=route.response_description,
-                    responses=responses,
+                    responses=combined_responses,
                     deprecated=route.deprecated,
                     methods=route.methods,
                     operation_id=route.operation_id,
"
"fastapi","9","a7a92bc63768ccee3f3afc2b73b2c581928dfe75","c5817912d2be25bb310bf9da517882f57bbe7bb5","fastapi/dependencies/utils.py","fastapi/dependencies/utils.py","diff --git a/fastapi/dependencies/utils.py b/fastapi/dependencies/utils.py","tests/test_request_body_parameters_media_type.py","","diff --git a/fastapi/dependencies/utils.py b/fastapi/dependencies/utils.py
index f9e42d0..7f0f590 100644
--- a/fastapi/dependencies/utils.py
+++ b/fastapi/dependencies/utils.py
@@ -559,6 +559,8 @@ def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:
     for f in flat_dependant.body_params:
         BodyModel.__fields__[f.name] = get_schema_compatible_field(field=f)
     required = any(True for f in flat_dependant.body_params if f.required)
+
+    BodySchema_kwargs: Dict[str, Any] = dict(default=None)
     if any(isinstance(f.schema, params.File) for f in flat_dependant.body_params):
         BodySchema: Type[params.Body] = params.File
     elif any(isinstance(f.schema, params.Form) for f in flat_dependant.body_params):
@@ -566,6 +568,14 @@ def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:
     else:
         BodySchema = params.Body
 
+        body_param_media_types = [
+            getattr(f.schema, ""media_type"")
+            for f in flat_dependant.body_params
+            if isinstance(f.schema, params.Body)
+        ]
+        if len(set(body_param_media_types)) == 1:
+            BodySchema_kwargs[""media_type""] = body_param_media_types[0]
+
     field = Field(
         name=""body"",
         type_=BodyModel,
@@ -574,6 +584,6 @@ def get_body_field(*, dependant: Dependant, name: str) -> Optional[Field]:
         model_config=BaseConfig,
         class_validators={},
         alias=""body"",
-        schema=BodySchema(None),
+        schema=BodySchema(**BodySchema_kwargs),
     )
     return field
"
"fastapi","15","b16ca54c30644667ab9f65a712704850666a039c","6d77e2ac5f2cadc63424f2d85d8d8cded2975922","fastapi/routing.py","fastapi/routing.py","diff --git a/fastapi/routing.py b/fastapi/routing.py","tests/test_ws_router.py","","diff --git a/fastapi/routing.py b/fastapi/routing.py
index 6d252d8..67619bd 100644
--- a/fastapi/routing.py
+++ b/fastapi/routing.py
@@ -271,6 +271,10 @@ class APIRouter(routing.Router):
                     include_in_schema=route.include_in_schema,
                     name=route.name,
                 )
+            elif isinstance(route, routing.WebSocketRoute):
+                self.add_websocket_route(
+                    prefix + route.path, route.endpoint, name=route.name
+                )
 
     def get(
         self,
"
"fastapi","11","bf229ad5d830eb5320f966d51a55e590e8d57008","06eb4219345a77d23484528c9d164eb8d2097fec","fastapi/dependencies/utils.py","fastapi/dependencies/utils.py","diff --git a/fastapi/dependencies/utils.py b/fastapi/dependencies/utils.py","tests/test_union_body.py;tests/test_union_inherited_body.py","","diff --git a/fastapi/dependencies/utils.py b/fastapi/dependencies/utils.py
index 28c57c2..c898ab7 100644
--- a/fastapi/dependencies/utils.py
+++ b/fastapi/dependencies/utils.py
@@ -131,12 +131,17 @@ def get_flat_dependant(dependant: Dependant) -> Dependant:
 
 
 def is_scalar_field(field: Field) -> bool:
-    return (
+    if not (
         field.shape == Shape.SINGLETON
         and not lenient_issubclass(field.type_, BaseModel)
         and not lenient_issubclass(field.type_, sequence_types + (dict,))
         and not isinstance(field.schema, params.Body)
-    )
+    ):
+        return False
+    if field.sub_fields:
+        if not all(is_scalar_field(f) for f in field.sub_fields):
+            return False
+    return True
 
 
 def is_scalar_sequence_field(field: Field) -> bool:
"
"fastapi","6","5db99a27cf640864b4793807811848698c5ff4a2","874d24181e779ebc6e1c52afb7d6598f863fd6a8","fastapi/dependencies/utils.py","fastapi/dependencies/utils.py","diff --git a/fastapi/dependencies/utils.py b/fastapi/dependencies/utils.py","tests/test_forms_from_non_typing_sequences.py","","diff --git a/fastapi/dependencies/utils.py b/fastapi/dependencies/utils.py
index 956ffff..a1cc0b9 100644
--- a/fastapi/dependencies/utils.py
+++ b/fastapi/dependencies/utils.py
@@ -629,9 +629,9 @@ async def request_body_to_args(
         for field in required_params:
             value: Any = None
             if received_body is not None:
-                if field.shape in sequence_shapes and isinstance(
-                    received_body, FormData
-                ):
+                if (
+                    field.shape in sequence_shapes or field.type_ in sequence_types
+                ) and isinstance(received_body, FormData):
                     value = received_body.getlist(field.alias)
                 else:
                     value = received_body.get(field.alias)
"
"fastapi","12","d61f5e4b555b123bf222503fc0e076cbae6a7ebc","d262f6e9296993e528e2327f0a73f7bf5514e7c6","fastapi/security/http.py","fastapi/security/http.py","diff --git a/fastapi/security/http.py b/fastapi/security/http.py","tests/test_security_http_bearer_optional.py","","diff --git a/fastapi/security/http.py b/fastapi/security/http.py
index f41d8d9..362390b 100644
--- a/fastapi/security/http.py
+++ b/fastapi/security/http.py
@@ -112,10 +112,13 @@ class HTTPBearer(HTTPBase):
             else:
                 return None
         if scheme.lower() != ""bearer"":
-            raise HTTPException(
-                status_code=HTTP_403_FORBIDDEN,
-                detail=""Invalid authentication credentials"",
-            )
+            if self.auto_error:
+                raise HTTPException(
+                    status_code=HTTP_403_FORBIDDEN,
+                    detail=""Invalid authentication credentials"",
+                )
+            else:
+                return None
         return HTTPAuthorizationCredentials(scheme=scheme, credentials=credentials)
 
 
"
"fastapi","4","7ccd81f70653857bd8f3a15ee946aa3fb0edc2cb","74c4d1c1dbe6bfdb05d6e4fc767ffe062398f0a3","fastapi/openapi/utils.py","fastapi/openapi/utils.py","diff --git a/fastapi/openapi/utils.py b/fastapi/openapi/utils.py","tests/test_param_in_path_and_dependency.py","","diff --git a/fastapi/openapi/utils.py b/fastapi/openapi/utils.py
index d53ee6b..91f90ec 100644
--- a/fastapi/openapi/utils.py
+++ b/fastapi/openapi/utils.py
@@ -180,7 +180,9 @@ def get_openapi_path(
             operation_parameters = get_openapi_operation_parameters(all_route_params)
             parameters.extend(operation_parameters)
             if parameters:
-                operation[""parameters""] = parameters
+                operation[""parameters""] = list(
+                    {param[""name""]: param for param in parameters}.values()
+                )
             if method in METHODS_WITH_BODY:
                 request_body_oai = get_openapi_operation_request_body(
                     body_field=route.body_field, model_name_map=model_name_map
"
"fastapi","5","7cea84b74ca3106a7f861b774e9d215e5228728f","75a07f24bf01a31225ee687f3e2b3fc1981b67ab","fastapi/utils.py","fastapi/utils.py","diff --git a/fastapi/utils.py b/fastapi/utils.py","tests/test_filter_pydantic_sub_model.py","","diff --git a/fastapi/utils.py b/fastapi/utils.py
index a068cc5..6a0c1bf 100644
--- a/fastapi/utils.py
+++ b/fastapi/utils.py
@@ -97,7 +97,7 @@ def create_cloned_field(field: ModelField) -> ModelField:
             original_type.__name__, __config__=original_type.__config__
         )
         for f in original_type.__fields__.values():
-            use_type.__fields__[f.name] = f
+            use_type.__fields__[f.name] = create_cloned_field(f)
         use_type.__validators__ = original_type.__validators__
     if PYDANTIC_1:
         new_field = ModelField(
"
"PySnooper","3","6e3d797be3fa0a746fb5b1b7c7fea78eb926c208","15555ed760000b049aff8fecc79d29339c1224c3","pysnooper/pysnooper.py","pysnooper/pysnooper.py","diff --git a/pysnooper/pysnooper.py b/pysnooper/pysnooper.py","tests/test_pysnooper.py","","diff --git a/pysnooper/pysnooper.py b/pysnooper/pysnooper.py
index 8d0c6a7..19139d3 100644
--- a/pysnooper/pysnooper.py
+++ b/pysnooper/pysnooper.py
@@ -23,7 +23,7 @@ def get_write_function(output):
             stderr.write(s)
     elif isinstance(output, (pycompat.PathLike, str)):
         def write(s):
-            with open(output_path, 'a') as output_file:
+            with open(output, 'a') as output_file:
                 output_file.write(s)
     else:
         assert isinstance(output, utils.WritableStream)
"
"PySnooper","2","e21a31162f4c54be693d8ca8260e42393b39abd3","814abc34a098c1b98cb327105ac396f985d2413e","pysnooper/tracer.py","pysnooper/tracer.py","diff --git a/pysnooper/tracer.py b/pysnooper/tracer.py","tests/test_pysnooper.py","","diff --git a/pysnooper/tracer.py b/pysnooper/tracer.py
index 089b817..fb6eddc 100644
--- a/pysnooper/tracer.py
+++ b/pysnooper/tracer.py
@@ -4,6 +4,7 @@
 import functools
 import inspect
 import opcode
+import os
 import sys
 import re
 import collections
@@ -14,16 +15,18 @@ import traceback
 
 from .variables import CommonVariable, Exploding, BaseVariable
 from . import utils, pycompat
+if pycompat.PY2:
+    from io import open
 
 
 ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')
 
 
-def get_local_reprs(frame, watch=()):
+def get_local_reprs(frame, watch=(), custom_repr=()):
     code = frame.f_code
     vars_order = code.co_varnames + code.co_cellvars + code.co_freevars + tuple(frame.f_locals.keys())
 
-    result_items = [(key, utils.get_shortish_repr(value)) for key, value in frame.f_locals.items()]
+    result_items = [(key, utils.get_shortish_repr(value, custom_repr=custom_repr)) for key, value in frame.f_locals.items()]
     result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))
     result = collections.OrderedDict(result_items)
 
@@ -84,7 +87,7 @@ def get_source_from_frame(frame):
     # apply tokenize.detect_encoding to decode the source into a
     # string, then we should do that ourselves.
     if isinstance(source[0], bytes):
-        encoding = 'ascii'
+        encoding = 'utf-8'
         for line in source[:2]:
             # File coding may be specified. Match pattern from PEP-263
             # (https://www.python.org/dev/peps/pep-0263/)
@@ -130,13 +133,14 @@ class FileWriter(object):
         self.overwrite = overwrite
 
     def write(self, s):
-        with open(self.path, 'w' if self.overwrite else 'a') as output_file:
+        with open(self.path, 'w' if self.overwrite else 'a',
+                  encoding='utf-8') as output_file:
             output_file.write(s)
         self.overwrite = False
 
 
 thread_global = threading.local()
-
+DISABLED = bool(os.getenv('PYSNOOPER_DISABLED', ''))
 
 class Tracer:
     '''
@@ -176,6 +180,10 @@ class Tracer:
 
         @pysnooper.snoop(thread_info=True)
 
+    Customize how values are represented as strings::
+
+        @pysnooper.snoop(custom_repr=((type1, custom_repr_func1), (condition2, custom_repr_func2), ...))
+
     '''
     def __init__(
             self,
@@ -186,6 +194,7 @@ class Tracer:
             prefix='',
             overwrite=False,
             thread_info=False,
+            custom_repr=(),
     ):
         self._write = get_write_function(output, overwrite)
 
@@ -205,8 +214,14 @@ class Tracer:
         self.target_codes = set()
         self.target_frames = set()
         self.thread_local = threading.local()
+        if len(custom_repr) == 2 and not all(isinstance(x,
+                      pycompat.collections_abc.Iterable) for x in custom_repr):
+            custom_repr = (custom_repr,)
+        self.custom_repr = custom_repr
 
     def __call__(self, function):
+        if DISABLED:
+            return function
         self.target_codes.add(function.__code__)
 
         @functools.wraps(function)
@@ -242,16 +257,22 @@ class Tracer:
         self._write(s)
 
     def __enter__(self):
+        if DISABLED:
+            return
         calling_frame = inspect.currentframe().f_back
         if not self._is_internal_frame(calling_frame):
             calling_frame.f_trace = self.trace
             self.target_frames.add(calling_frame)
 
-        stack = self.thread_local.__dict__.setdefault('original_trace_functions', [])
+        stack = self.thread_local.__dict__.setdefault(
+            'original_trace_functions', []
+        )
         stack.append(sys.gettrace())
         sys.settrace(self.trace)
 
     def __exit__(self, exc_type, exc_value, exc_traceback):
+        if DISABLED:
+            return
         stack = self.thread_local.original_trace_functions
         sys.settrace(stack.pop())
         calling_frame = inspect.currentframe().f_back
@@ -267,7 +288,6 @@ class Tracer:
                                        current_thread_len)
         return thread_info.ljust(self.thread_info_padding)
 
-
     def trace(self, frame, event, arg):
 
         ### Checking whether we should trace this line: #######################
@@ -307,7 +327,7 @@ class Tracer:
         #                                                                     #
         old_local_reprs = self.frame_to_local_reprs.get(frame, {})
         self.frame_to_local_reprs[frame] = local_reprs = \
-                                       get_local_reprs(frame, watch=self.watch)
+                                       get_local_reprs(frame, watch=self.watch, custom_repr=self.custom_repr)
 
         newish_string = ('Starting var:.. ' if event == 'call' else
                                                             'New var:....... ')
@@ -380,7 +400,7 @@ class Tracer:
             thread_global.depth -= 1
 
             if not ended_by_exception:
-                return_value_repr = utils.get_shortish_repr(arg)
+                return_value_repr = utils.get_shortish_repr(arg, custom_repr=self.custom_repr)
                 self.write('{indent}Return value:.. {return_value_repr}'.
                            format(**locals()))
 
"
"PySnooper","1","e21a31162f4c54be693d8ca8260e42393b39abd3","56f22f8ffe1c6b2be4d2cf3ad1987fdb66113da2","pysnooper/pycompat.py;pysnooper/tracer.py;tests/utils.py","pysnooper/pycompat.py;pysnooper/tracer.py;tests/utils.py","diff --git a/pysnooper/pycompat.py b/pysnooper/pycompat.py;diff --git a/pysnooper/tracer.py b/pysnooper/tracer.py;diff --git a/tests/utils.py b/tests/utils.py","tests/test_chinese.py","","diff --git a/pysnooper/pycompat.py b/pysnooper/pycompat.py
index 63dd3db..de0a472 100644
--- a/pysnooper/pycompat.py
+++ b/pysnooper/pycompat.py
@@ -8,6 +8,7 @@ import inspect
 import sys
 
 PY3 = (sys.version_info[0] == 3)
+PY2 = not PY3
 
 if hasattr(abc, 'ABC'):
     ABC = abc.ABC
diff --git a/pysnooper/tracer.py b/pysnooper/tracer.py
index 089b817..480d60a 100644
--- a/pysnooper/tracer.py
+++ b/pysnooper/tracer.py
@@ -14,6 +14,8 @@ import traceback
 
 from .variables import CommonVariable, Exploding, BaseVariable
 from . import utils, pycompat
+if pycompat.PY2:
+    from io import open
 
 
 ipython_filename_pattern = re.compile('^<ipython-input-([0-9]+)-.*>$')
@@ -84,7 +86,7 @@ def get_source_from_frame(frame):
     # apply tokenize.detect_encoding to decode the source into a
     # string, then we should do that ourselves.
     if isinstance(source[0], bytes):
-        encoding = 'ascii'
+        encoding = 'utf-8'
         for line in source[:2]:
             # File coding may be specified. Match pattern from PEP-263
             # (https://www.python.org/dev/peps/pep-0263/)
@@ -130,7 +132,8 @@ class FileWriter(object):
         self.overwrite = overwrite
 
     def write(self, s):
-        with open(self.path, 'w' if self.overwrite else 'a') as output_file:
+        with open(self.path, 'w' if self.overwrite else 'a',
+                  encoding='utf-8') as output_file:
             output_file.write(s)
         self.overwrite = False
 
diff --git a/tests/utils.py b/tests/utils.py
index b77e1f6..05185f0 100644
--- a/tests/utils.py
+++ b/tests/utils.py
@@ -254,7 +254,7 @@ def assert_output(output, expected_entries, prefix=None):
 
     any_mismatch = False
     result = ''
-    template = '\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))
+    template = u'\n{line!s:%s}   {expected_entry}  {arrow}' % max(map(len, lines))
     for expected_entry, line in zip_longest(expected_entries, lines, fillvalue=""""):
         mismatch = not (expected_entry and expected_entry.check(line))
         any_mismatch |= mismatch
@@ -273,7 +273,7 @@ def assert_sample_output(module):
     with sys_tools.OutputCapturer(stdout=False,
                                   stderr=True) as output_capturer:
         module.main()
-        
+
     time = '21:10:42.298924'
     time_pattern = re.sub(r'\d', r'\\d', time)
 
"
"tornado","14","81ee310adcd905fbdf7c98d9fb6ef0c5a46026c2","1d02ed606f1c52636462633d009bdcbaac644331","tornado/ioloop.py","tornado/ioloop.py","diff --git a/tornado/ioloop.py b/tornado/ioloop.py","tornado/test/ioloop_test.py","","diff --git a/tornado/ioloop.py b/tornado/ioloop.py
index 67e33b52..87d4168e 100644
--- a/tornado/ioloop.py
+++ b/tornado/ioloop.py
@@ -249,7 +249,7 @@ class IOLoop(Configurable):
             if IOLoop.current(instance=False) is None:
                 self.make_current()
         elif make_current:
-            if IOLoop.current(instance=False) is None:
+            if IOLoop.current(instance=False) is not None:
                 raise RuntimeError(""current IOLoop already exists"")
             self.make_current()
 
"
"tornado","3","940fd87fe9145d1154c8457221f86d56ea063c65","aa622e724f80e0f7fcee369f75d69d1db13d72f2","tornado/httpclient.py","tornado/httpclient.py","diff --git a/tornado/httpclient.py b/tornado/httpclient.py","tornado/test/httpclient_test.py","","diff --git a/tornado/httpclient.py b/tornado/httpclient.py
index d1c92a49..b6344cff 100644
--- a/tornado/httpclient.py
+++ b/tornado/httpclient.py
@@ -233,9 +233,14 @@ class AsyncHTTPClient(Configurable):
             return
         self._closed = True
         if self._instance_cache is not None:
-            if self._instance_cache.get(self.io_loop) is not self:
+            cached_val = self._instance_cache.pop(self.io_loop, None)
+            # If there's an object other than self in the instance
+            # cache for our IOLoop, something has gotten mixed up. A
+            # value of None appears to be possible when this is called
+            # from a destructor (HTTPClient.__del__) as the weakref
+            # gets cleared before the destructor runs.
+            if cached_val is not None and cached_val is not self:
                 raise RuntimeError(""inconsistent AsyncHTTPClient cache"")
-            del self._instance_cache[self.io_loop]
 
     def fetch(
         self,
"
"tornado","16","b450ff270df0395ee80f4ee5896a92bfe7f9b6ae","d1676810ef5972c4defb0a710a1d8f8a0018983b","tornado/gen.py","tornado/gen.py","diff --git a/tornado/gen.py b/tornado/gen.py","tornado/test/gen_test.py","","diff --git a/tornado/gen.py b/tornado/gen.py
index e184c433..86635c0d 100644
--- a/tornado/gen.py
+++ b/tornado/gen.py
@@ -336,6 +336,8 @@ class WaitIterator(object):
         self.current_index = self.current_future = None
         self._running_future = None
 
+        # Use a weak reference to self to avoid cycles that may delay
+        # garbage collection.
         self_ref = weakref.ref(self)
         for future in futures:
             future.add_done_callback(functools.partial(
@@ -356,6 +358,12 @@ class WaitIterator(object):
         the inputs.
         """"""
         self._running_future = TracebackFuture()
+        # As long as there is an active _running_future, we must
+        # ensure that the WaitIterator is not GC'd (due to the
+        # use of weak references in __init__). Add a callback that
+        # references self so there is a hard reference that will be
+        # cleared automatically when this Future finishes.
+        self._running_future.add_done_callback(lambda f: self)
 
         if self._finished:
             self._return_result(self._finished.popleft())
"
"tornado","2","2ca8821d006f6693f920a4b183a3a7c985a5c8ad","4f486a4aec746e9d66441600ee3b0743228b061c","tornado/http1connection.py","tornado/http1connection.py","diff --git a/tornado/http1connection.py b/tornado/http1connection.py","tornado/test/httpclient_test.py","","diff --git a/tornado/http1connection.py b/tornado/http1connection.py
index 30f2a172..fd63be62 100644
--- a/tornado/http1connection.py
+++ b/tornado/http1connection.py
@@ -390,7 +390,10 @@ class HTTP1Connection(httputil.HTTPConnection):
             self._chunking_output = (
                 start_line.method in (""POST"", ""PUT"", ""PATCH"")
                 and ""Content-Length"" not in headers
-                and ""Transfer-Encoding"" not in headers
+                and (
+                    ""Transfer-Encoding"" not in headers
+                    or headers[""Transfer-Encoding""] == ""chunked""
+                )
             )
         else:
             assert isinstance(start_line, httputil.ResponseStartLine)
"
"tornado","8","34c43f4775971ab9b2b8ed43356f218add6387b2","5d4a9ab26372efd255bbb29fde55c41395ed17b1","tornado/websocket.py","tornado/websocket.py","diff --git a/tornado/websocket.py b/tornado/websocket.py","tornado/test/websocket_test.py","","diff --git a/tornado/websocket.py b/tornado/websocket.py
index d5a7fa89..c6804ca0 100644
--- a/tornado/websocket.py
+++ b/tornado/websocket.py
@@ -616,6 +616,14 @@ class WebSocketProtocol13(WebSocketProtocol):
     def accept_connection(self):
         try:
             self._handle_websocket_headers()
+        except ValueError:
+            self.handler.set_status(400)
+            log_msg = ""Missing/Invalid WebSocket headers""
+            self.handler.finish(log_msg)
+            gen_log.debug(log_msg)
+            return
+
+        try:
             self._accept_connection()
         except ValueError:
             gen_log.debug(""Malformed WebSocket request received"",
"
"tornado","1","6a5a0bfa370b6c0d3dbbf9589a560a98202d2baa","4677c54cc18bbfbdf0f4dadf11610fab6203fd63","tornado/websocket.py","tornado/websocket.py","diff --git a/tornado/websocket.py b/tornado/websocket.py","tornado/test/websocket_test.py","","diff --git a/tornado/websocket.py b/tornado/websocket.py
index 00d08bab..d991fee5 100644
--- a/tornado/websocket.py
+++ b/tornado/websocket.py
@@ -558,8 +558,8 @@ class WebSocketHandler(tornado.web.RequestHandler):
 
         .. versionadded:: 3.1
         """"""
-        assert self.stream is not None
-        self.stream.set_nodelay(value)
+        assert self.ws_connection is not None
+        self.ws_connection.set_nodelay(value)
 
     def on_connection_close(self) -> None:
         if self.ws_connection:
@@ -714,6 +714,10 @@ class WebSocketProtocol(abc.ABC):
     async def _receive_frame_loop(self) -> None:
         raise NotImplementedError()
 
+    @abc.abstractmethod
+    def set_nodelay(self, x: bool) -> None:
+        raise NotImplementedError()
+
 
 class _PerMessageDeflateCompressor(object):
     def __init__(
@@ -1345,6 +1349,9 @@ class WebSocketProtocol13(WebSocketProtocol):
         self.write_ping(b"""")
         self.last_ping = now
 
+    def set_nodelay(self, x: bool) -> None:
+        self.stream.set_nodelay(x)
+
 
 class WebSocketClientConnection(simple_httpclient._HTTPConnection):
     """"""WebSocket client connection.
"
"tornado","7","fa3409179588536f2f743d16e537f6f9827fa92f","a3b44cd701e0e82693363701bc0346b0125d2362","tornado/ioloop.py","tornado/ioloop.py","diff --git a/tornado/ioloop.py b/tornado/ioloop.py","tornado/test/ioloop_test.py","","diff --git a/tornado/ioloop.py b/tornado/ioloop.py
index 5686576c..a60632e4 100644
--- a/tornado/ioloop.py
+++ b/tornado/ioloop.py
@@ -44,7 +44,7 @@ import time
 import traceback
 import math
 
-from tornado.concurrent import TracebackFuture, is_future
+from tornado.concurrent import TracebackFuture, is_future, chain_future
 from tornado.log import app_log, gen_log
 from tornado.platform.auto import set_close_exec, Waker
 from tornado import stack_context
@@ -655,8 +655,12 @@ class IOLoop(Configurable):
                 from tornado.process import cpu_count
                 self._executor = ThreadPoolExecutor(max_workers=(cpu_count() * 5))
             executor = self._executor
-
-        return executor.submit(func, *args)
+        c_future = executor.submit(func, *args)
+        # Concurrent Futures are not usable with await. Wrap this in a
+        # Tornado Future instead, using self.add_future for thread-safety.
+        t_future = TracebackFuture()
+        self.add_future(c_future, lambda f: chain_future(f, t_future))
+        return t_future
 
     def set_default_executor(self, executor):
         """"""Sets the default executor to use with :meth:`run_in_executor`.""""""
"
"tornado","10","ecd8968c5135b810cd607b5902dda2cd32122b39","5931d913b4ea250891a0b582f1f8b2901b868c79","tornado/web.py;tornado/websocket.py","tornado/web.py;tornado/websocket.py","diff --git a/tornado/web.py b/tornado/web.py;diff --git a/tornado/websocket.py b/tornado/websocket.py","tornado/test/websocket_test.py","","diff --git a/tornado/web.py b/tornado/web.py
index 8ff52e9c..d79889fa 100644
--- a/tornado/web.py
+++ b/tornado/web.py
@@ -993,6 +993,9 @@ class RequestHandler(object):
         self._log()
         self._finished = True
         self.on_finish()
+        self._break_cycles()
+
+    def _break_cycles(self):
         # Break up a reference cycle between this handler and the
         # _ui_module closures to allow for faster GC on CPython.
         self.ui = None
diff --git a/tornado/websocket.py b/tornado/websocket.py
index ce13d262..69437ee4 100644
--- a/tornado/websocket.py
+++ b/tornado/websocket.py
@@ -434,6 +434,16 @@ class WebSocketHandler(tornado.web.RequestHandler):
         if not self._on_close_called:
             self._on_close_called = True
             self.on_close()
+            self._break_cycles()
+
+    def _break_cycles(self):
+        # WebSocketHandlers call finish() early, but we don't want to
+        # break up reference cycles (which makes it impossible to call
+        # self.render_string) until after we've really closed the
+        # connection (if it was established in the first place,
+        # indicated by status code 101).
+        if self.get_status() != 101 or self._on_close_called:
+            super(WebSocketHandler, self)._break_cycles()
 
     def send_error(self, *args, **kwargs):
         if self.stream is None:
"
"tornado","13","d7d9c467cda38f4c9352172ba7411edc29a85196","34903f9e1a99441b2729bbe6f1d65d46cf352ea7","tornado/http1connection.py;tornado/test/runtests.py","tornado/http1connection.py;tornado/test/runtests.py","diff --git a/tornado/http1connection.py b/tornado/http1connection.py;diff --git a/tornado/test/runtests.py b/tornado/test/runtests.py","tornado/test/http1connection_test.py","","diff --git a/tornado/http1connection.py b/tornado/http1connection.py
index 71f0790d..76322145 100644
--- a/tornado/http1connection.py
+++ b/tornado/http1connection.py
@@ -481,7 +481,9 @@ class HTTP1Connection(httputil.HTTPConnection):
             return connection_header != ""close""
         elif (""Content-Length"" in headers
               or headers.get(""Transfer-Encoding"", """").lower() == ""chunked""
-              or start_line.method in (""HEAD"", ""GET"")):
+              or getattr(start_line, 'method', None) in (""HEAD"", ""GET"")):
+            # start_line may be a request or reponse start line; only
+            # the former has a method attribute.
             return connection_header == ""keep-alive""
         return False
 
diff --git a/tornado/test/runtests.py b/tornado/test/runtests.py
index ad9b0b83..3b22d396 100644
--- a/tornado/test/runtests.py
+++ b/tornado/test/runtests.py
@@ -29,6 +29,7 @@ TEST_MODULES = [
     'tornado.test.curl_httpclient_test',
     'tornado.test.escape_test',
     'tornado.test.gen_test',
+    'tornado.test.http1connection_test',
     'tornado.test.httpclient_test',
     'tornado.test.httpserver_test',
     'tornado.test.httputil_test',
"
"tornado","9","c9d2a3fa573987629ad576e991c2f3b65f4daab4","86cc31f52992fb9d11f92de6fd5496842fea2265","tornado/httputil.py","tornado/httputil.py","diff --git a/tornado/httputil.py b/tornado/httputil.py","tornado/test/httputil_test.py","","diff --git a/tornado/httputil.py b/tornado/httputil.py
index dc206fc8..818ea914 100644
--- a/tornado/httputil.py
+++ b/tornado/httputil.py
@@ -603,6 +603,8 @@ def url_concat(url, args):
     >>> url_concat(""http://example.com/foo?a=b"", [(""c"", ""d""), (""c"", ""d2"")])
     'http://example.com/foo?a=b&c=d&c=d2'
     """"""
+    if args is None:
+        return url
     parsed_url = urlparse(url)
     if isinstance(args, dict):
         parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)
"
"tornado","15","fdfaf3dffa49479c7461050eacca07bc5ee8d207","ecb3ea7543cc942659faf3d2144853018afa6139","setup.py;tornado/web.py","setup.py;tornado/web.py","diff --git a/setup.py b/setup.py;diff --git a/tornado/web.py b/tornado/web.py","tornado/test/web_test.py","","diff --git a/setup.py b/setup.py
index 9b81b2b8..9e5ea7fa 100644
--- a/setup.py
+++ b/setup.py
@@ -147,6 +147,7 @@ setup(
             ""options_test.cfg"",
             ""static/robots.txt"",
             ""static/dir/index.html"",
+            ""static_foo.txt"",
             ""templates/utf8.html"",
             ""test.crt"",
             ""test.key"",
diff --git a/tornado/web.py b/tornado/web.py
index 0a50f793..9847bb02 100644
--- a/tornado/web.py
+++ b/tornado/web.py
@@ -2376,9 +2376,13 @@ class StaticFileHandler(RequestHandler):
 
         .. versionadded:: 3.1
         """"""
-        root = os.path.abspath(root)
-        # os.path.abspath strips a trailing /
-        # it needs to be temporarily added back for requests to root/
+        # os.path.abspath strips a trailing /.
+        # We must add it back to `root` so that we only match files
+        # in a directory named `root` instead of files starting with
+        # that prefix.
+        root = os.path.abspath(root) + os.path.sep
+        # The trailing slash also needs to be temporarily added back
+        # the requested path so a request to root/ will match.
         if not (absolute_path + os.path.sep).startswith(root):
             raise HTTPError(403, ""%s is not in root static directory"",
                             self.path)
"
"tornado","11","79ef301eb05cac82c075198e502d94dad296f6aa","1131c9b50a6a4c0868d0d6fa5e0be077cf8fd1ca","tornado/http1connection.py","tornado/http1connection.py","diff --git a/tornado/http1connection.py b/tornado/http1connection.py","tornado/test/httpserver_test.py","","diff --git a/tornado/http1connection.py b/tornado/http1connection.py
index d0e91b82..7ee83161 100644
--- a/tornado/http1connection.py
+++ b/tornado/http1connection.py
@@ -565,7 +565,7 @@ class HTTP1Connection(httputil.HTTPConnection):
 
         if content_length is not None:
             return self._read_fixed_body(content_length, delegate)
-        if headers.get(""Transfer-Encoding"") == ""chunked"":
+        if headers.get(""Transfer-Encoding"", """").lower() == ""chunked"":
             return self._read_chunked_body(delegate)
         if self.is_client:
             return self._read_body_until_close(delegate)
"
"tornado","6","fb74e4816ccfa7fc6a7abd8c8aab1f415cfc1b13","2905ee4fb3c283d40b10f609359e189c83a0dc06","tornado/ioloop.py;tornado/platform/asyncio.py","tornado/ioloop.py;tornado/platform/asyncio.py","diff --git a/tornado/ioloop.py b/tornado/ioloop.py;diff --git a/tornado/platform/asyncio.py b/tornado/platform/asyncio.py","tornado/test/asyncio_test.py","","diff --git a/tornado/ioloop.py b/tornado/ioloop.py
index 839e7ee5..f6ec177b 100644
--- a/tornado/ioloop.py
+++ b/tornado/ioloop.py
@@ -47,7 +47,6 @@ import threading
 import time
 import traceback
 import math
-import weakref
 
 from tornado.concurrent import Future, is_future, chain_future, future_set_exc_info, future_add_done_callback  # noqa: E501
 from tornado.log import app_log, gen_log
@@ -185,7 +184,7 @@ class IOLoop(Configurable):
     _current = threading.local()
 
     # In Python 3, _ioloop_for_asyncio maps from asyncio loops to IOLoops.
-    _ioloop_for_asyncio = weakref.WeakKeyDictionary()
+    _ioloop_for_asyncio = dict()
 
     @classmethod
     def configure(cls, impl, **kwargs):
diff --git a/tornado/platform/asyncio.py b/tornado/platform/asyncio.py
index b2ad9fe6..b6a490af 100644
--- a/tornado/platform/asyncio.py
+++ b/tornado/platform/asyncio.py
@@ -38,6 +38,20 @@ class BaseAsyncIOLoop(IOLoop):
         self.readers = set()
         self.writers = set()
         self.closing = False
+        # If an asyncio loop was closed through an asyncio interface
+        # instead of IOLoop.close(), we'd never hear about it and may
+        # have left a dangling reference in our map. In case an
+        # application (or, more likely, a test suite) creates and
+        # destroys a lot of event loops in this way, check here to
+        # ensure that we don't have a lot of dead loops building up in
+        # the map.
+        #
+        # TODO(bdarnell): consider making self.asyncio_loop a weakref
+        # for AsyncIOMainLoop and make _ioloop_for_asyncio a
+        # WeakKeyDictionary.
+        for loop in list(IOLoop._ioloop_for_asyncio):
+            if loop.is_closed():
+                del IOLoop._ioloop_for_asyncio[loop]
         IOLoop._ioloop_for_asyncio[asyncio_loop] = self
         super(BaseAsyncIOLoop, self).initialize(**kwargs)
 
@@ -49,6 +63,7 @@ class BaseAsyncIOLoop(IOLoop):
             if all_fds:
                 self.close_fd(fileobj)
         self.asyncio_loop.close()
+        del IOLoop._ioloop_for_asyncio[self.asyncio_loop]
 
     def add_handler(self, fd, handler, events):
         fd, fileobj = self.split_fd(fd)
"
"tornado","12","4ee9ba94de11aaa4f932560fa2b3d8ceb8c61d2a","301f52b532c071a0d2fec1eb7c23f2714bb38567","tornado/auth.py","tornado/auth.py","diff --git a/tornado/auth.py b/tornado/auth.py","tornado/test/auth_test.py","","diff --git a/tornado/auth.py b/tornado/auth.py
index 32d0e226..ff7172aa 100644
--- a/tornado/auth.py
+++ b/tornado/auth.py
@@ -75,7 +75,7 @@ import hmac
 import time
 import uuid
 
-from tornado.concurrent import TracebackFuture, return_future
+from tornado.concurrent import TracebackFuture, return_future, chain_future
 from tornado import gen
 from tornado import httpclient
 from tornado import escape
@@ -985,7 +985,7 @@ class FacebookGraphMixin(OAuth2Mixin):
             future.set_exception(AuthError('Facebook auth error: %s' % str(response)))
             return
 
-        args = escape.parse_qs_bytes(escape.native_str(response.body))
+        args = urlparse.parse_qs(escape.native_str(response.body))
         session = {
             ""access_token"": args[""access_token""][-1],
             ""expires"": args.get(""expires"")
@@ -1062,8 +1062,13 @@ class FacebookGraphMixin(OAuth2Mixin):
            Added the ability to override ``self._FACEBOOK_BASE_URL``.
         """"""
         url = self._FACEBOOK_BASE_URL + path
-        return self.oauth2_request(url, callback, access_token,
-                                   post_args, **args)
+        # Thanks to the _auth_return_future decorator, our ""callback""
+        # argument is a Future, which we cannot pass as a callback to
+        # oauth2_request. Instead, have oauth2_request return a
+        # future and chain them together.
+        oauth_future = self.oauth2_request(url, access_token=access_token,
+                                           post_args=post_args, **args)
+        chain_future(oauth_future, callback)
 
 
 def _oauth_signature(consumer_token, method, url, parameters={}, token=None):
"
"tornado","4","a8420fc681c5423d072978f00eab4d0645057d16","db529031a1e1a6e951826aba0b7d0b18f05cd4c7","tornado/web.py","tornado/web.py","diff --git a/tornado/web.py b/tornado/web.py","tornado/test/web_test.py","","diff --git a/tornado/web.py b/tornado/web.py
index 2affd8e3..263f429b 100644
--- a/tornado/web.py
+++ b/tornado/web.py
@@ -2600,16 +2600,24 @@ class StaticFileHandler(RequestHandler):
         size = self.get_content_size()
         if request_range:
             start, end = request_range
-            if (start is not None and start >= size) or end == 0:
+            if start is not None and start < 0:
+                start += size
+                if start < 0:
+                    start = 0
+            if (
+                start is not None
+                and (start >= size or (end is not None and start >= end))
+            ) or end == 0:
                 # As per RFC 2616 14.35.1, a range is not satisfiable only: if
                 # the first requested byte is equal to or greater than the
-                # content, or when a suffix with length 0 is specified
+                # content, or when a suffix with length 0 is specified.
+                # https://tools.ietf.org/html/rfc7233#section-2.1
+                # A byte-range-spec is invalid if the last-byte-pos value is present
+                # and less than the first-byte-pos.
                 self.set_status(416)  # Range Not Satisfiable
                 self.set_header(""Content-Type"", ""text/plain"")
                 self.set_header(""Content-Range"", ""bytes */%s"" % (size,))
                 return
-            if start is not None and start < 0:
-                start += size
             if end is not None and end > size:
                 # Clients sometimes blindly use a large range to limit their
                 # download size; cap the endpoint at the actual file size.
"
"tornado","5","2d4053daa56c609d642b214399e046671d4a593e","886643965b5cb782503d8d7b374b7a794ec2077b","tornado/ioloop.py","tornado/ioloop.py","diff --git a/tornado/ioloop.py b/tornado/ioloop.py","tornado/test/ioloop_test.py","","diff --git a/tornado/ioloop.py b/tornado/ioloop.py
index 45d2d223..48700139 100644
--- a/tornado/ioloop.py
+++ b/tornado/ioloop.py
@@ -1217,7 +1217,27 @@ class PeriodicCallback(object):
             self._timeout = self.io_loop.add_timeout(self._next_timeout, self._run)
 
     def _update_next(self, current_time):
+        callback_time_sec = self.callback_time / 1000.0
         if self._next_timeout <= current_time:
-            callback_time_sec = self.callback_time / 1000.0
+            # The period should be measured from the start of one call
+            # to the start of the next. If one call takes too long,
+            # skip cycles to get back to a multiple of the original
+            # schedule.
             self._next_timeout += (math.floor((current_time - self._next_timeout) /
                                               callback_time_sec) + 1) * callback_time_sec
+        else:
+            # If the clock moved backwards, ensure we advance the next
+            # timeout instead of recomputing the same value again.
+            # This may result in long gaps between callbacks if the
+            # clock jumps backwards by a lot, but the far more common
+            # scenario is a small NTP adjustment that should just be
+            # ignored.
+            #
+            # Note that on some systems if time.time() runs slower
+            # than time.monotonic() (most common on windows), we
+            # effectively experience a small backwards time jump on
+            # every iteration because PeriodicCallback uses
+            # time.time() while asyncio schedules callbacks using
+            # time.monotonic().
+            # https://github.com/tornadoweb/tornado/issues/2333
+            self._next_timeout += callback_time_sec
"
"pandas","120","2b0cac7","c5a1f9e2c373ced9ef2f02ab64d11eaa7b4248f2","pandas/core/groupby/generic.py;pandas/core/groupby/groupby.py","pandas/core/groupby/generic.py;pandas/core/groupby/groupby.py","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py;diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py","pandas/tests/groupby/test_categorical.py","","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py
index 0ca6ef043..d1894faad 100644
--- a/pandas/core/groupby/generic.py
+++ b/pandas/core/groupby/generic.py
@@ -557,7 +557,8 @@ class SeriesGroupBy(GroupBy):
             res, out = np.zeros(len(ri), dtype=out.dtype), res
             res[ids[idx]] = out
 
-        return Series(res, index=ri, name=self._selection_name)
+        result = Series(res, index=ri, name=self._selection_name)
+        return self._reindex_output(result, fill_value=0)
 
     @Appender(Series.describe.__doc__)
     def describe(self, **kwargs):
@@ -709,12 +710,13 @@ class SeriesGroupBy(GroupBy):
         minlength = ngroups or 0
         out = np.bincount(ids[mask], minlength=minlength)
 
-        return Series(
+        result = Series(
             out,
             index=self.grouper.result_index,
             name=self._selection_name,
             dtype=""int64"",
         )
+        return self._reindex_output(result, fill_value=0)
 
     def _apply_to_column_groupbys(self, func):
         """""" return a pass thru """"""
diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py
index 99a4942df..75bb818ea 100644
--- a/pandas/core/groupby/groupby.py
+++ b/pandas/core/groupby/groupby.py
@@ -39,6 +39,7 @@ from pandas.core.dtypes.common import (
 )
 from pandas.core.dtypes.missing import isna, notna
 
+from pandas._typing import FrameOrSeries, Scalar
 from pandas.core import nanops
 import pandas.core.algorithms as algorithms
 from pandas.core.arrays import Categorical, try_cast_to_ea
@@ -1296,7 +1297,7 @@ class GroupBy(_GroupBy):
 
         if isinstance(self.obj, Series):
             result.name = self.obj.name
-        return result
+        return self._reindex_output(result, fill_value=0)
 
     @classmethod
     def _add_numeric_operations(cls):
@@ -1740,6 +1741,7 @@ class GroupBy(_GroupBy):
             if not self.observed and isinstance(result_index, CategoricalIndex):
                 out = out.reindex(result_index)
 
+            out = self._reindex_output(out)
             return out.sort_index() if self.sort else out
 
         # dropna is truthy
@@ -2380,7 +2382,9 @@ class GroupBy(_GroupBy):
         mask = self._cumcount_array(ascending=False) < n
         return self._selected_obj[mask]
 
-    def _reindex_output(self, output):
+    def _reindex_output(
+        self, output: FrameOrSeries, fill_value: Scalar = np.NaN
+    ) -> FrameOrSeries:
         """"""
         If we have categorical groupers, then we might want to make sure that
         we have a fully re-indexed output to the levels. This means expanding
@@ -2394,8 +2398,10 @@ class GroupBy(_GroupBy):
 
         Parameters
         ----------
-        output: Series or DataFrame
+        output : Series or DataFrame
             Object resulting from grouping and applying an operation.
+        fill_value : scalar, default np.NaN
+            Value to use for unobserved categories if self.observed is False.
 
         Returns
         -------
@@ -2426,7 +2432,11 @@ class GroupBy(_GroupBy):
         ).sortlevel()
 
         if self.as_index:
-            d = {self.obj._get_axis_name(self.axis): index, ""copy"": False}
+            d = {
+                self.obj._get_axis_name(self.axis): index,
+                ""copy"": False,
+                ""fill_value"": fill_value,
+            }
             return output.reindex(**d)
 
         # GH 13204
@@ -2448,7 +2458,9 @@ class GroupBy(_GroupBy):
         output = output.drop(labels=list(g_names), axis=1)
 
         # Set a temp index and reindex (possibly expanding)
-        output = output.set_index(self.grouper.result_index).reindex(index, copy=False)
+        output = output.set_index(self.grouper.result_index).reindex(
+            index, copy=False, fill_value=fill_value
+        )
 
         # Reset in-axis grouper columns
         # (using level numbers `g_nums` because level names may not be unique)
"
"pandas","81","b529857","339edcdb7ecc6edc6fde1b7d1413dbb746d2bcca","pandas/core/arrays/integer.py","pandas/core/arrays/integer.py","diff --git a/pandas/core/arrays/integer.py b/pandas/core/arrays/integer.py","pandas/tests/arrays/test_integer.py","","diff --git a/pandas/core/arrays/integer.py b/pandas/core/arrays/integer.py
index 67036761b..022e6a732 100644
--- a/pandas/core/arrays/integer.py
+++ b/pandas/core/arrays/integer.py
@@ -19,6 +19,7 @@ from pandas.core.dtypes.common import (
     is_list_like,
     is_object_dtype,
     is_scalar,
+    pandas_dtype,
 )
 from pandas.core.dtypes.dtypes import register_extension_dtype
 from pandas.core.dtypes.missing import isna
@@ -440,11 +441,17 @@ class IntegerArray(BaseMaskedArray):
             if incompatible type with an IntegerDtype, equivalent of same_kind
             casting
         """"""
+        from pandas.core.arrays.boolean import BooleanArray, BooleanDtype
+
+        dtype = pandas_dtype(dtype)
 
         # if we are astyping to an existing IntegerDtype we can fastpath
         if isinstance(dtype, _IntegerDtype):
             result = self._data.astype(dtype.numpy_dtype, copy=False)
             return type(self)(result, mask=self._mask, copy=False)
+        elif isinstance(dtype, BooleanDtype):
+            result = self._data.astype(""bool"", copy=False)
+            return BooleanArray(result, mask=self._mask, copy=False)
 
         # coerce
         if is_float_dtype(dtype):
"
"pandas","69","426d445","948f95756c79543bb089a94a85e73011a3730b2d","pandas/core/indexing.py","pandas/core/indexing.py","diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py","pandas/tests/indexes/test_numeric.py","","diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index 6a6797082..7e56148b7 100755
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -2124,7 +2124,7 @@ class _AtIndexer(_ScalarAccessIndexer):
                         ""can only have integer indexers""
                     )
             else:
-                if is_integer(i) and not ax.holds_integer():
+                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):
                     raise ValueError(
                         ""At based indexing on an non-integer ""
                         ""index can only have non-integer ""
"
"pandas","115","ed20822","386494d0dc851be9e86b1576f30fa8705df4d47b","pandas/core/missing.py","pandas/core/missing.py","diff --git a/pandas/core/missing.py b/pandas/core/missing.py","pandas/tests/series/test_missing.py","","diff --git a/pandas/core/missing.py b/pandas/core/missing.py
index 744cde95c..9f4f74455 100644
--- a/pandas/core/missing.py
+++ b/pandas/core/missing.py
@@ -277,7 +277,11 @@ def interpolate_1d(
                 inds = lib.maybe_convert_objects(inds)
         else:
             inds = xvalues
-        result[invalid] = np.interp(inds[invalid], inds[valid], yvalues[valid])
+        # np.interp requires sorted X values, #21037
+        indexer = np.argsort(inds[valid])
+        result[invalid] = np.interp(
+            inds[invalid], inds[valid][indexer], yvalues[valid][indexer]
+        )
         result[preserve_nans] = np.nan
         return result
 
"
"pandas","139","7705cd2","0ffdbe36f0df732f2700cda4a84be758084ff901","pandas/core/groupby/grouper.py","pandas/core/groupby/grouper.py","diff --git a/pandas/core/groupby/grouper.py b/pandas/core/groupby/grouper.py","pandas/tests/groupby/test_categorical.py","","diff --git a/pandas/core/groupby/grouper.py b/pandas/core/groupby/grouper.py
index 2d37121d2..d7eaaca5a 100644
--- a/pandas/core/groupby/grouper.py
+++ b/pandas/core/groupby/grouper.py
@@ -330,7 +330,8 @@ class Grouping:
                 self._group_index = CategoricalIndex(
                     Categorical.from_codes(
                         codes=codes, categories=categories, ordered=self.grouper.ordered
-                    )
+                    ),
+                    name=self.name,
                 )
 
             # we are done
"
"pandas","48","9bc3ee0","9e7cb7c102655d0ba92d2561c178da9254d5cef5","pandas/core/groupby/generic.py","pandas/core/groupby/generic.py","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py","pandas/tests/groupby/test_function.py","","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py
index b7ac30486..a9a608e6f 100644
--- a/pandas/core/groupby/generic.py
+++ b/pandas/core/groupby/generic.py
@@ -1083,7 +1083,7 @@ class DataFrameGroupBy(GroupBy):
                         result = type(block.values)._from_sequence(
                             result.ravel(), dtype=block.values.dtype
                         )
-                    except ValueError:
+                    except (ValueError, TypeError):
                         # reshape to be valid for non-Extension Block
                         result = result.reshape(1, -1)
 
"
"pandas","49","113c255","37659d47a685ecc5f5117aa56526ece0106c6d0f","pandas/core/strings.py","pandas/core/strings.py","diff --git a/pandas/core/strings.py b/pandas/core/strings.py","pandas/tests/test_strings.py","","diff --git a/pandas/core/strings.py b/pandas/core/strings.py
index c4bdbaff1..71d9e8e7a 100644
--- a/pandas/core/strings.py
+++ b/pandas/core/strings.py
@@ -775,6 +775,8 @@ def str_repeat(arr, repeats):
     else:
 
         def rep(x, r):
+            if x is libmissing.NA:
+                return x
             try:
                 return bytes.__mul__(x, r)
             except TypeError:
"
"pandas","157","b1c871c","def01cf7bbb5ef8c9bf2e19737ea918e6a76a143","pandas/core/reshape/merge.py","pandas/core/reshape/merge.py","diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py","pandas/tests/reshape/merge/test_merge_asof.py","","diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py
index f45c7693b..225de3f11 100644
--- a/pandas/core/reshape/merge.py
+++ b/pandas/core/reshape/merge.py
@@ -22,7 +22,6 @@ from pandas.core.dtypes.common import (
     is_bool,
     is_bool_dtype,
     is_categorical_dtype,
-    is_datetime64_dtype,
     is_datetime64tz_dtype,
     is_datetimelike,
     is_dtype_equal,
@@ -1635,7 +1634,7 @@ class _AsOfMerge(_OrderedMerge):
                 )
             )
 
-            if is_datetime64_dtype(lt) or is_datetime64tz_dtype(lt):
+            if is_datetimelike(lt):
                 if not isinstance(self.tolerance, Timedelta):
                     raise MergeError(msg)
                 if self.tolerance < Timedelta(0):
"
"pandas","164","ac69333","61819aba14dd7b3996336aaed84d07cd936d92b5","pandas/core/tools/datetimes.py","pandas/core/tools/datetimes.py","diff --git a/pandas/core/tools/datetimes.py b/pandas/core/tools/datetimes.py","pandas/tests/indexes/datetimes/test_tools.py","","diff --git a/pandas/core/tools/datetimes.py b/pandas/core/tools/datetimes.py
index 172084e97..b07647cf5 100644
--- a/pandas/core/tools/datetimes.py
+++ b/pandas/core/tools/datetimes.py
@@ -334,6 +334,9 @@ def _convert_listlike_datetimes(
                 return DatetimeIndex(arg, tz=tz, name=name)
             except ValueError:
                 pass
+        elif tz:
+            # DatetimeArray, DatetimeIndex
+            return arg.tz_localize(tz)
 
         return arg
 
"
"pandas","73","f1d7ac6","6f93898d32c0f1fdb382d1e9dee434c158998374","pandas/core/frame.py;pandas/core/ops/missing.py","pandas/core/frame.py;pandas/core/ops/missing.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py;diff --git a/pandas/core/ops/missing.py b/pandas/core/ops/missing.py","pandas/tests/frame/test_arithmetic.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 012fb1d0c..9ff3f8c23 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -5341,7 +5341,7 @@ class DataFrame(NDFrame):
     # ----------------------------------------------------------------------
     # Arithmetic / combination related
 
-    def _combine_frame(self, other, func, fill_value=None, level=None):
+    def _combine_frame(self, other: ""DataFrame"", func, fill_value=None):
         # at this point we have `self._indexed_same(other)`
 
         if fill_value is None:
@@ -5368,7 +5368,7 @@ class DataFrame(NDFrame):
 
         return new_data
 
-    def _combine_match_index(self, other, func):
+    def _combine_match_index(self, other: Series, func):
         # at this point we have `self.index.equals(other.index)`
 
         if ops.should_series_dispatch(self, other, func):
@@ -5376,8 +5376,10 @@ class DataFrame(NDFrame):
             new_data = ops.dispatch_to_series(self, other, func)
         else:
             # fastpath --> operate directly on values
+            other_vals = other.values.reshape(-1, 1)
             with np.errstate(all=""ignore""):
-                new_data = func(self.values.T, other.values).T
+                new_data = func(self.values, other_vals)
+            new_data = dispatch_fill_zeros(func, self.values, other_vals, new_data)
         return new_data
 
     def _construct_result(self, result) -> ""DataFrame"":
diff --git a/pandas/core/ops/missing.py b/pandas/core/ops/missing.py
index 5039ffab3..854d6072e 100644
--- a/pandas/core/ops/missing.py
+++ b/pandas/core/ops/missing.py
@@ -109,26 +109,23 @@ def mask_zero_div_zero(x, y, result):
         return result
 
     if zmask.any():
-        shape = result.shape
 
         # Flip sign if necessary for -0.0
         zneg_mask = zmask & np.signbit(y)
         zpos_mask = zmask & ~zneg_mask
 
-        nan_mask = (zmask & (x == 0)).ravel()
+        nan_mask = zmask & (x == 0)
         with np.errstate(invalid=""ignore""):
-            neginf_mask = ((zpos_mask & (x < 0)) | (zneg_mask & (x > 0))).ravel()
-            posinf_mask = ((zpos_mask & (x > 0)) | (zneg_mask & (x < 0))).ravel()
+            neginf_mask = (zpos_mask & (x < 0)) | (zneg_mask & (x > 0))
+            posinf_mask = (zpos_mask & (x > 0)) | (zneg_mask & (x < 0))
 
         if nan_mask.any() or neginf_mask.any() or posinf_mask.any():
             # Fill negative/0 with -inf, positive/0 with +inf, 0/0 with NaN
-            result = result.astype(""float64"", copy=False).ravel()
-
-            np.putmask(result, nan_mask, np.nan)
-            np.putmask(result, posinf_mask, np.inf)
-            np.putmask(result, neginf_mask, -np.inf)
+            result = result.astype(""float64"", copy=False)
 
-            result = result.reshape(shape)
+            result[nan_mask] = np.nan
+            result[posinf_mask] = np.inf
+            result[neginf_mask] = -np.inf
 
     return result
 
"
"pandas","165","9fe8a0f","9b1c005142fed227081dd454eab1a414168d458e","pandas/core/arrays/datetimelike.py;pandas/tests/arithmetic/test_period.py;pandas/tests/arithmetic/test_timedelta64.py","pandas/core/arrays/datetimelike.py;pandas/tests/arithmetic/test_period.py;pandas/tests/arithmetic/test_timedelta64.py","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py;diff --git a/pandas/tests/arithmetic/test_period.py b/pandas/tests/arithmetic/test_period.py;diff --git a/pandas/tests/arithmetic/test_timedelta64.py b/pandas/tests/arithmetic/test_timedelta64.py","pandas/tests/arithmetic/test_datetime64.py","","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py
index 9b516c1b6..2747b1d7d 100644
--- a/pandas/core/arrays/datetimelike.py
+++ b/pandas/core/arrays/datetimelike.py
@@ -1207,7 +1207,7 @@ class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)
 
     def __add__(self, other):
         other = lib.item_from_zerodim(other)
-        if isinstance(other, (ABCSeries, ABCDataFrame)):
+        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):
             return NotImplemented
 
         # scalar others
@@ -1273,7 +1273,7 @@ class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)
 
     def __sub__(self, other):
         other = lib.item_from_zerodim(other)
-        if isinstance(other, (ABCSeries, ABCDataFrame)):
+        if isinstance(other, (ABCSeries, ABCDataFrame, ABCIndexClass)):
             return NotImplemented
 
         # scalar others
@@ -1340,7 +1340,7 @@ class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)
         return result
 
     def __rsub__(self, other):
-        if is_datetime64_dtype(other) and is_timedelta64_dtype(self):
+        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self):
             # ndarray[datetime64] cannot be subtracted from self, so
             # we need to wrap in DatetimeArray/Index and flip the operation
             if not isinstance(other, DatetimeLikeArrayMixin):
diff --git a/pandas/tests/arithmetic/test_period.py b/pandas/tests/arithmetic/test_period.py
index c1b32e8b1..4b58c290c 100644
--- a/pandas/tests/arithmetic/test_period.py
+++ b/pandas/tests/arithmetic/test_period.py
@@ -1041,6 +1041,18 @@ class TestPeriodIndexArithmetic:
         with pytest.raises(TypeError):
             other - obj
 
+    # ---------------------------------------------------------------
+    # Unsorted
+
+    def test_parr_add_sub_index(self):
+        # Check that PeriodArray defers to Index on arithmetic ops
+        pi = pd.period_range(""2000-12-31"", periods=3)
+        parr = pi.array
+
+        result = parr - pi
+        expected = pi - pi
+        tm.assert_index_equal(result, expected)
+
 
 class TestPeriodSeriesArithmetic:
     def test_ops_series_timedelta(self):
diff --git a/pandas/tests/arithmetic/test_timedelta64.py b/pandas/tests/arithmetic/test_timedelta64.py
index 326c56530..4f5e00bc5 100644
--- a/pandas/tests/arithmetic/test_timedelta64.py
+++ b/pandas/tests/arithmetic/test_timedelta64.py
@@ -480,6 +480,25 @@ class TestTimedelta64ArithmeticUnsorted:
         tm.assert_index_equal(result1, result4)
         tm.assert_index_equal(result2, result3)
 
+    def test_tda_add_sub_index(self):
+        # Check that TimedeltaArray defers to Index on arithmetic ops
+        tdi = TimedeltaIndex([""1 days"", pd.NaT, ""2 days""])
+        tda = tdi.array
+
+        dti = pd.date_range(""1999-12-31"", periods=3, freq=""D"")
+
+        result = tda + dti
+        expected = tdi + dti
+        tm.assert_index_equal(result, expected)
+
+        result = tda + tdi
+        expected = tdi + tdi
+        tm.assert_index_equal(result, expected)
+
+        result = tda - tdi
+        expected = tdi - tdi
+        tm.assert_index_equal(result, expected)
+
 
 class TestAddSubNaTMasking:
     # TODO: parametrize over boxes
"
"pandas","166","4056ded","d44fb07063e9a8bd8a209ddce35b40d8a56c8d02","pandas/core/frame.py","pandas/core/frame.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py","pandas/tests/frame/test_join.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index be870b9fc..02241eeaa 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -7210,10 +7210,14 @@ class DataFrame(NDFrame):
             # join indexes only using concat
             if can_concat:
                 if how == ""left"":
-                    res = concat(frames, axis=1, join=""outer"", verify_integrity=True)
+                    res = concat(
+                        frames, axis=1, join=""outer"", verify_integrity=True, sort=sort
+                    )
                     return res.reindex(self.index, copy=False)
                 else:
-                    return concat(frames, axis=1, join=how, verify_integrity=True)
+                    return concat(
+                        frames, axis=1, join=how, verify_integrity=True, sort=sort
+                    )
 
             joined = frames[0]
 
"
"pandas","22","38b669a","299e27da8a75d02d84870c1ca5971f4dd0f046e6","pandas/core/window/common.py;pandas/core/window/rolling.py","pandas/core/window/common.py;pandas/core/window/rolling.py","diff --git a/pandas/core/window/common.py b/pandas/core/window/common.py;diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py","pandas/tests/window/test_base_indexer.py","","diff --git a/pandas/core/window/common.py b/pandas/core/window/common.py
index 40f17126f..436585fe2 100644
--- a/pandas/core/window/common.py
+++ b/pandas/core/window/common.py
@@ -328,6 +328,7 @@ def get_weighted_roll_func(cfunc: Callable) -> Callable:
 def validate_baseindexer_support(func_name: Optional[str]) -> None:
     # GH 32865: These functions work correctly with a BaseIndexer subclass
     BASEINDEXER_WHITELIST = {
+        ""count"",
         ""min"",
         ""max"",
         ""mean"",
diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py
index 3fdf81c4b..62f470060 100644
--- a/pandas/core/window/rolling.py
+++ b/pandas/core/window/rolling.py
@@ -1171,8 +1171,9 @@ class _Rolling_and_Expanding(_Rolling):
     )
 
     def count(self):
-        if isinstance(self.window, BaseIndexer):
-            validate_baseindexer_support(""count"")
+        # GH 32865. Using count with custom BaseIndexer subclass
+        # implementations shouldn't end up here
+        assert not isinstance(self.window, BaseIndexer)
 
         blocks, obj = self._create_blocks()
         results = []
@@ -1939,7 +1940,9 @@ class Rolling(_Rolling_and_Expanding):
     def count(self):
 
         # different impl for freq counting
-        if self.is_freq_type:
+        # GH 32865. Use a custom count function implementation
+        # when using a BaseIndexer subclass as a window
+        if self.is_freq_type or isinstance(self.window, BaseIndexer):
             window_func = self._get_roll_func(""roll_count"")
             return self._apply(window_func, center=self.center, name=""count"")
 
"
"pandas","98","8105a7e","09e4b780f09c5aa72bb2a6ae2832612f81dc047f","pandas/core/indexes/base.py","pandas/core/indexes/base.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py","pandas/tests/indexes/period/test_constructors.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index c349b64e4..ed5c6b450 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -295,11 +295,15 @@ class Index(IndexOpsMixin, PandasObject):
             return CategoricalIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)
 
         # interval
-        elif (
-            is_interval_dtype(data) or is_interval_dtype(dtype)
-        ) and not is_object_dtype(dtype):
-            closed = kwargs.get(""closed"", None)
-            return IntervalIndex(data, dtype=dtype, name=name, copy=copy, closed=closed)
+        elif is_interval_dtype(data) or is_interval_dtype(dtype):
+            closed = kwargs.pop(""closed"", None)
+            if is_dtype_equal(_o_dtype, dtype):
+                return IntervalIndex(
+                    data, name=name, copy=copy, closed=closed, **kwargs
+                ).astype(object)
+            return IntervalIndex(
+                data, dtype=dtype, name=name, copy=copy, closed=closed, **kwargs
+            )
 
         elif (
             is_datetime64_any_dtype(data)
@@ -329,8 +333,10 @@ class Index(IndexOpsMixin, PandasObject):
             else:
                 return TimedeltaIndex(data, copy=copy, name=name, dtype=dtype, **kwargs)
 
-        elif is_period_dtype(data) and not is_object_dtype(dtype):
-            return PeriodIndex(data, copy=copy, name=name, **kwargs)
+        elif is_period_dtype(data) or is_period_dtype(dtype):
+            if is_dtype_equal(_o_dtype, dtype):
+                return PeriodIndex(data, copy=False, name=name, **kwargs).astype(object)
+            return PeriodIndex(data, dtype=dtype, copy=copy, name=name, **kwargs)
 
         # extension dtype
         elif is_extension_array_dtype(data) or is_extension_array_dtype(dtype):
"
"pandas","18","e008a0a","cb71376385c33270fa1922aec9eb6c49de4336f4","pandas/core/window/common.py;pandas/core/window/rolling.py","pandas/core/window/common.py;pandas/core/window/rolling.py","diff --git a/pandas/core/window/common.py b/pandas/core/window/common.py;diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py","pandas/tests/window/test_base_indexer.py","","diff --git a/pandas/core/window/common.py b/pandas/core/window/common.py
index 8707893dc..082c2f533 100644
--- a/pandas/core/window/common.py
+++ b/pandas/core/window/common.py
@@ -337,6 +337,7 @@ def validate_baseindexer_support(func_name: Optional[str]) -> None:
         ""median"",
         ""std"",
         ""var"",
+        ""skew"",
         ""kurt"",
         ""quantile"",
     }
diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py
index 05400f63d..24130c044 100644
--- a/pandas/core/window/rolling.py
+++ b/pandas/core/window/rolling.py
@@ -472,13 +472,13 @@ class _Window(PandasObject, ShallowMixin, SelectionMixin):
 
                 def calc(x):
                     x = np.concatenate((x, additional_nans))
-                    if not isinstance(window, BaseIndexer):
+                    if not isinstance(self.window, BaseIndexer):
                         min_periods = calculate_min_periods(
                             window, self.min_periods, len(x), require_min_periods, floor
                         )
                     else:
                         min_periods = calculate_min_periods(
-                            self.min_periods or 1,
+                            window_indexer.window_size,
                             self.min_periods,
                             len(x),
                             require_min_periods,
"
"pandas","135","0df22b6","f41219179de69fed5c2a4b7df821394af1aa6559","pandas/core/groupby/ops.py","pandas/core/groupby/ops.py","diff --git a/pandas/core/groupby/ops.py b/pandas/core/groupby/ops.py","pandas/tests/extension/decimal/test_decimal.py","","diff --git a/pandas/core/groupby/ops.py b/pandas/core/groupby/ops.py
index 00e7012b4..e6f4f2f05 100644
--- a/pandas/core/groupby/ops.py
+++ b/pandas/core/groupby/ops.py
@@ -672,7 +672,13 @@ class BaseGrouper:
                 pass
             else:
                 raise
-            return self._aggregate_series_pure_python(obj, func)
+        except TypeError as err:
+            if ""ndarray"" in str(err):
+                # raised in libreduction if obj's values is no ndarray
+                pass
+            else:
+                raise
+        return self._aggregate_series_pure_python(obj, func)
 
     def _aggregate_series_fast(self, obj, func):
         func = self._is_builtin_func(func)
"
"pandas","150","54e9b75","d38627b5889db3f663cad339fe8f995af823b76b","pandas/core/dtypes/missing.py","pandas/core/dtypes/missing.py","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py","pandas/tests/dtypes/test_missing.py","","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py
index 056cd2222..6dd032b92 100644
--- a/pandas/core/dtypes/missing.py
+++ b/pandas/core/dtypes/missing.py
@@ -445,7 +445,7 @@ def array_equivalent(left, right, strict_nan=False):
                 if not isinstance(right_value, float) or not np.isnan(right_value):
                     return False
             else:
-                if left_value != right_value:
+                if np.any(left_value != right_value):
                     return False
         return True
 
"
"pandas","155","4252ab7","0bde7cedf46209a9fd4fa8c7f9fbce8b49aa78cd","pandas/core/window/rolling.py","pandas/core/window/rolling.py","diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py","pandas/tests/window/test_rolling.py","","diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py
index a7e122fa3..29ef2e917 100644
--- a/pandas/core/window/rolling.py
+++ b/pandas/core/window/rolling.py
@@ -1653,7 +1653,10 @@ class Rolling(_Rolling_and_Expanding):
     def _on(self):
 
         if self.on is None:
-            return self.obj.index
+            if self.axis == 0:
+                return self.obj.index
+            elif self.axis == 1:
+                return self.obj.columns
         elif isinstance(self.obj, ABCDataFrame) and self.on in self.obj.columns:
             return Index(self.obj[self.on])
         else:
"
"pandas","94","8803056","613df15047887957f5964d2a6ce59ea20b0c4c91","pandas/core/indexes/datetimelike.py","pandas/core/indexes/datetimelike.py","diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py","pandas/tests/indexes/datetimes/test_constructors.py","","diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py
index 345600655..3a58794a8 100644
--- a/pandas/core/indexes/datetimelike.py
+++ b/pandas/core/indexes/datetimelike.py
@@ -28,7 +28,12 @@ from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
 
 from pandas.core import algorithms
 from pandas.core.accessor import PandasDelegate
-from pandas.core.arrays import ExtensionArray, ExtensionOpsMixin
+from pandas.core.arrays import (
+    DatetimeArray,
+    ExtensionArray,
+    ExtensionOpsMixin,
+    TimedeltaArray,
+)
 from pandas.core.arrays.datetimelike import (
     DatetimeLikeArrayMixin,
     _ensure_datetimelike_to_i8,
@@ -251,15 +256,10 @@ class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):
         if isinstance(maybe_slice, slice):
             return self[maybe_slice]
 
-        taken = ExtensionIndex.take(
+        return ExtensionIndex.take(
             self, indices, axis, allow_fill, fill_value, **kwargs
         )
 
-        # keep freq in PeriodArray/Index, reset otherwise
-        freq = self.freq if is_period_dtype(self) else None
-        assert taken.freq == freq, (taken.freq, freq, taken)
-        return self._shallow_copy(taken, freq=freq)
-
     _can_hold_na = True
 
     _na_value = NaT
@@ -486,8 +486,8 @@ class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):
     @Appender(_index_shared_docs[""repeat""] % _index_doc_kwargs)
     def repeat(self, repeats, axis=None):
         nv.validate_repeat(tuple(), dict(axis=axis))
-        freq = self.freq if is_period_dtype(self) else None
-        return self._shallow_copy(self.asi8.repeat(repeats), freq=freq)
+        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)
+        return self._shallow_copy(result)
 
     @Appender(_index_shared_docs[""where""] % _index_doc_kwargs)
     def where(self, cond, other=None):
@@ -650,6 +650,22 @@ class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):
 
         self._data._freq = freq
 
+    def _shallow_copy(self, values=None, **kwargs):
+        if values is None:
+            values = self._data
+        if isinstance(values, type(self)):
+            values = values._data
+
+        attributes = self._get_attributes_dict()
+
+        if ""freq"" not in kwargs and self.freq is not None:
+            if isinstance(values, (DatetimeArray, TimedeltaArray)):
+                if values.freq is None:
+                    del attributes[""freq""]
+
+        attributes.update(kwargs)
+        return self._simple_new(values, **attributes)
+
     # --------------------------------------------------------------------
     # Set Operation Methods
 
"
"pandas","60","6bc2dca","fcf7258c19b0a6a712f33fb0bcefdae426be7e7f","pandas/core/window/rolling.py","pandas/core/window/rolling.py","diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py","pandas/tests/window/test_grouper.py","","diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py
index 580c7cc05..8506b2ff6 100644
--- a/pandas/core/window/rolling.py
+++ b/pandas/core/window/rolling.py
@@ -1296,13 +1296,14 @@ class _Rolling_and_Expanding(_Rolling):
             raise ValueError(""engine must be either 'numba' or 'cython'"")
 
         # TODO: Why do we always pass center=False?
-        # name=func for WindowGroupByMixin._apply
+        # name=func & raw=raw for WindowGroupByMixin._apply
         return self._apply(
             apply_func,
             center=False,
             floor=0,
             name=func,
             use_numba_cache=engine == ""numba"",
+            raw=raw,
         )
 
     def _generate_cython_apply_func(self, args, kwargs, raw, offset, func):
"
"pandas","133","343544d","c983d52e3a3a8a191359814417f375b1dc8b04c1","pandas/core/generic.py","pandas/core/generic.py","diff --git a/pandas/core/generic.py b/pandas/core/generic.py","pandas/tests/frame/test_missing.py","","diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index d59ce8db9..fe0923f09 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -7048,14 +7048,15 @@ class NDFrame(PandasObject, SelectionMixin):
         """"""
         inplace = validate_bool_kwarg(inplace, ""inplace"")
 
+        axis = self._get_axis_number(axis)
+
         if axis == 0:
             ax = self._info_axis_name
             _maybe_transposed_self = self
         elif axis == 1:
             _maybe_transposed_self = self.T
             ax = 1
-        else:
-            _maybe_transposed_self = self
+
         ax = _maybe_transposed_self._get_axis_number(ax)
 
         if _maybe_transposed_self.ndim == 2:
"
"pandas","156","42d6ee7","05cc95971e56b503d4df9911a44cd60a7b74cc79","pandas/core/sparse/frame.py","pandas/core/sparse/frame.py","diff --git a/pandas/core/sparse/frame.py b/pandas/core/sparse/frame.py","pandas/tests/sparse/frame/test_frame.py","","diff --git a/pandas/core/sparse/frame.py b/pandas/core/sparse/frame.py
index 8fe6850c8..3d6ba0b8d 100644
--- a/pandas/core/sparse/frame.py
+++ b/pandas/core/sparse/frame.py
@@ -576,8 +576,8 @@ class SparseDataFrame(DataFrame):
         this, other = self.align(other, join=""outer"", axis=0, level=level, copy=False)
 
         new_data = {}
-        for col, series in this.items():
-            new_data[col] = func(series.values, other.values)
+        for col in this.columns:
+            new_data[col] = func(this[col], other)
 
         fill_value = self._get_op_result_fill_value(other, func)
 
@@ -603,7 +603,7 @@ class SparseDataFrame(DataFrame):
         new_data = {}
 
         for col in left.columns:
-            new_data[col] = func(left[col], float(right[col]))
+            new_data[col] = func(left[col], right[col])
 
         return self._constructor(
             new_data,
"
"pandas","131","73745be","bf5848f111c92fc5c6c11a93a3bc2480f138f1b1","pandas/core/indexes/accessors.py","pandas/core/indexes/accessors.py","diff --git a/pandas/core/indexes/accessors.py b/pandas/core/indexes/accessors.py","pandas/tests/series/test_datetime_values.py","","diff --git a/pandas/core/indexes/accessors.py b/pandas/core/indexes/accessors.py
index cc8ecc0e6..e8d2ba85e 100644
--- a/pandas/core/indexes/accessors.py
+++ b/pandas/core/indexes/accessors.py
@@ -16,7 +16,6 @@ from pandas.core.dtypes.common import (
 from pandas.core.dtypes.generic import ABCSeries
 
 from pandas.core.accessor import PandasDelegate, delegate_names
-from pandas.core.algorithms import take_1d
 from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray
 from pandas.core.base import NoNewAttributesMixin, PandasObject
 from pandas.core.indexes.datetimes import DatetimeIndex
@@ -75,9 +74,7 @@ class Properties(PandasDelegate, PandasObject, NoNewAttributesMixin):
 
         result = np.asarray(result)
 
-        # blow up if we operate on categories
         if self.orig is not None:
-            result = take_1d(result, self.orig.cat.codes)
             index = self.orig.index
         else:
             index = self._parent.index
@@ -324,7 +321,12 @@ class CombinedDatetimelikeProperties(
 
         orig = data if is_categorical_dtype(data) else None
         if orig is not None:
-            data = Series(orig.values.categories, name=orig.name, copy=False)
+            data = Series(
+                orig.array,
+                name=orig.name,
+                copy=False,
+                dtype=orig.values.categories.dtype,
+            )
 
         if is_datetime64_dtype(data.dtype):
             return DatetimeProperties(data, orig)
"
"pandas","58","634a41f","16684f2affaf901b42a12e50f9c29e7c034ad7ea","pandas/core/arrays/categorical.py","pandas/core/arrays/categorical.py","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py","pandas/tests/arrays/categorical/test_constructors.py","","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index d26ff7490..0e04354ae 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -644,7 +644,13 @@ class Categorical(ExtensionArray, PandasObject):
             )
             raise ValueError(msg)
 
-        codes = np.asarray(codes)  # #21767
+        if is_extension_array_dtype(codes) and is_integer_dtype(codes):
+            # Avoid the implicit conversion of Int to object
+            if isna(codes).any():
+                raise ValueError(""codes cannot contain NA values"")
+            codes = codes.to_numpy(dtype=np.int64)
+        else:
+            codes = np.asarray(codes)
         if len(codes) and not is_integer_dtype(codes):
             raise ValueError(""codes need to be array-like integers"")
 
"
"pandas","71","74a5edc","a5daff22e6e37af4946c614f85b110905e063be3","pandas/core/reshape/tile.py","pandas/core/reshape/tile.py","diff --git a/pandas/core/reshape/tile.py b/pandas/core/reshape/tile.py","pandas/tests/arrays/test_integer.py","","diff --git a/pandas/core/reshape/tile.py b/pandas/core/reshape/tile.py
index 5a444d908..00a7645d0 100644
--- a/pandas/core/reshape/tile.py
+++ b/pandas/core/reshape/tile.py
@@ -14,7 +14,9 @@ from pandas.core.dtypes.common import (
     is_datetime64_dtype,
     is_datetime64tz_dtype,
     is_datetime_or_timedelta_dtype,
+    is_extension_array_dtype,
     is_integer,
+    is_integer_dtype,
     is_list_like,
     is_scalar,
     is_timedelta64_dtype,
@@ -205,6 +207,12 @@ def cut(
     x = _preprocess_for_cut(x)
     x, dtype = _coerce_to_type(x)
 
+    # To support cut(IntegerArray), we convert to object dtype with NaN
+    # Will properly support in the future.
+    # https://github.com/pandas-dev/pandas/pull/31290
+    if is_extension_array_dtype(x.dtype) and is_integer_dtype(x.dtype):
+        x = x.to_numpy(dtype=object, na_value=np.nan)
+
     if not np.iterable(bins):
         if is_scalar(bins) and bins < 1:
             raise ValueError(""`bins` should be a positive integer."")
"
"pandas","148","4ac7f9d","95edcf1cbee630e42daca0404c44d8128ea156fb","pandas/core/apply.py","pandas/core/apply.py","diff --git a/pandas/core/apply.py b/pandas/core/apply.py","pandas/tests/frame/test_apply.py","","diff --git a/pandas/core/apply.py b/pandas/core/apply.py
index e6766a33a..61d093d19 100644
--- a/pandas/core/apply.py
+++ b/pandas/core/apply.py
@@ -204,17 +204,20 @@ class FrameApply:
         from pandas import Series
 
         if not should_reduce:
-
-            EMPTY_SERIES = Series([])
             try:
-                r = self.f(EMPTY_SERIES, *self.args, **self.kwds)
+                r = self.f(Series([]))
             except Exception:
                 pass
             else:
                 should_reduce = not isinstance(r, Series)
 
         if should_reduce:
-            return self.obj._constructor_sliced(np.nan, index=self.agg_axis)
+            if len(self.agg_axis):
+                r = self.f(Series([]))
+            else:
+                r = np.nan
+
+            return self.obj._constructor_sliced(r, index=self.agg_axis)
         else:
             return self.obj.copy()
 
"
"pandas","52","20a84a5","7017599821e02ba95282848c12f7d3b5f2ce670a","pandas/core/groupby/generic.py","pandas/core/groupby/generic.py","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py","pandas/tests/groupby/test_function.py","","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py
index 37b642916..1bb512aee 100644
--- a/pandas/core/groupby/generic.py
+++ b/pandas/core/groupby/generic.py
@@ -591,30 +591,18 @@ class SeriesGroupBy(GroupBy):
 
         val = self.obj._internal_get_values()
 
-        # GH 27951
-        # temporary fix while we wait for NumPy bug 12629 to be fixed
-        val[isna(val)] = np.datetime64(""NaT"")
-
-        try:
-            sorter = np.lexsort((val, ids))
-        except TypeError:  # catches object dtypes
-            msg = f""val.dtype must be object, got {val.dtype}""
-            assert val.dtype == object, msg
-            val, _ = algorithms.factorize(val, sort=False)
-            sorter = np.lexsort((val, ids))
-            _isna = lambda a: a == -1
-        else:
-            _isna = isna
-
-        ids, val = ids[sorter], val[sorter]
+        codes, _ = algorithms.factorize(val, sort=False)
+        sorter = np.lexsort((codes, ids))
+        codes = codes[sorter]
+        ids = ids[sorter]
 
         # group boundaries are where group ids change
         # unique observations are where sorted values change
         idx = np.r_[0, 1 + np.nonzero(ids[1:] != ids[:-1])[0]]
-        inc = np.r_[1, val[1:] != val[:-1]]
+        inc = np.r_[1, codes[1:] != codes[:-1]]
 
         # 1st item of each group is a new unique observation
-        mask = _isna(val)
+        mask = codes == -1
         if dropna:
             inc[idx] = 1
             inc[mask] = 0
"
"pandas","59","292a993","8dd9fabd2ad9104e747084437b9ad436d5be087a","pandas/core/window/rolling.py","pandas/core/window/rolling.py","diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py","pandas/tests/window/test_pairwise.py","","diff --git a/pandas/core/window/rolling.py b/pandas/core/window/rolling.py
index 8506b2ff6..5c18796de 100644
--- a/pandas/core/window/rolling.py
+++ b/pandas/core/window/rolling.py
@@ -1782,7 +1782,7 @@ class _Rolling_and_Expanding(_Rolling):
             # only default unset
             pairwise = True if pairwise is None else pairwise
         other = self._shallow_copy(other)
-        window = self._get_window(other)
+        window = self._get_window(other) if not self.is_freq_type else self.win_freq
 
         def _get_corr(a, b):
             a = a.rolling(
"
"pandas","14","e7b23d4","dd71064327721c1ec7366000f357b0c08bcec4d2","pandas/core/ops/dispatch.py;pandas/tests/arithmetic/conftest.py","pandas/core/ops/dispatch.py;pandas/tests/arithmetic/conftest.py","diff --git a/pandas/core/ops/dispatch.py b/pandas/core/ops/dispatch.py;diff --git a/pandas/tests/arithmetic/conftest.py b/pandas/tests/arithmetic/conftest.py","pandas/tests/arithmetic/test_datetime64.py;pandas/tests/arithmetic/test_timedelta64.py","","diff --git a/pandas/core/ops/dispatch.py b/pandas/core/ops/dispatch.py
index 2463a1f58..637f0fa1d 100644
--- a/pandas/core/ops/dispatch.py
+++ b/pandas/core/ops/dispatch.py
@@ -71,8 +71,10 @@ def should_series_dispatch(left, right, op):
         # numpy integer dtypes as timedelta64 dtypes in this scenario
         return True
 
-    if is_datetime64_dtype(ldtype) and is_object_dtype(rdtype):
-        # in particular case where right is an array of DateOffsets
+    if (is_datetime64_dtype(ldtype) and is_object_dtype(rdtype)) or (
+        is_datetime64_dtype(rdtype) and is_object_dtype(ldtype)
+    ):
+        # in particular case where one is an array of DateOffsets
         return True
 
     return False
diff --git a/pandas/tests/arithmetic/conftest.py b/pandas/tests/arithmetic/conftest.py
index 577093c0f..c20a9567e 100644
--- a/pandas/tests/arithmetic/conftest.py
+++ b/pandas/tests/arithmetic/conftest.py
@@ -17,6 +17,18 @@ def id_func(x):
 
 
 # ------------------------------------------------------------------
+@pytest.fixture(
+    params=[
+        (""foo"", None, None),
+        (""Egon"", ""Venkman"", None),
+        (""NCC1701D"", ""NCC1701D"", ""NCC1701D""),
+    ]
+)
+def names(request):
+    """"""
+    A 3-tuple of names, the first two for operands, the last for a result.
+    """"""
+    return request.param
 
 
 @pytest.fixture(params=[1, np.array(1, dtype=np.int64)])
"
"pandas","169","4d9016e","01babb590cb15ef5c6e9ad890ea580a5112e6999","pandas/core/frame.py","pandas/core/frame.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py","pandas/tests/frame/test_quantile.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index fc8983f10..9a84f1ddd 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -8215,6 +8215,13 @@ class DataFrame(NDFrame):
         if is_transposed:
             data = data.T
 
+        if len(data.columns) == 0:
+            # GH#23925 _get_numeric_data may have dropped all columns
+            cols = Index([], name=self.columns.name)
+            if is_list_like(q):
+                return self._constructor([], index=q, columns=cols)
+            return self._constructor_sliced([], index=cols, name=q)
+
         result = data._data.quantile(
             qs=q, axis=1, interpolation=interpolation, transposed=is_transposed
         )
"
"pandas","80","d0d93db","351760c0655b6c383e449cf857b9a718e3545229","pandas/core/arrays/masked.py;pandas/core/generic.py;pandas/core/indexing.py;pandas/tests/arrays/test_boolean.py;pandas/tests/extension/base/__init__.py;pandas/tests/extension/base/ops.py;pandas/tests/frame/test_operators.py","pandas/core/arrays/masked.py;pandas/core/generic.py;pandas/core/indexing.py;pandas/tests/arrays/test_boolean.py;pandas/tests/extension/base/__init__.py;pandas/tests/extension/base/ops.py;pandas/tests/frame/test_operators.py","diff --git a/pandas/core/arrays/masked.py b/pandas/core/arrays/masked.py;diff --git a/pandas/core/generic.py b/pandas/core/generic.py;diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py;diff --git a/pandas/tests/arrays/test_boolean.py b/pandas/tests/arrays/test_boolean.py;diff --git a/pandas/tests/extension/base/__init__.py b/pandas/tests/extension/base/__init__.py;diff --git a/pandas/tests/extension/base/ops.py b/pandas/tests/extension/base/ops.py;diff --git a/pandas/tests/frame/test_operators.py b/pandas/tests/frame/test_operators.py","pandas/tests/arrays/sparse/test_arithmetics.py","","diff --git a/pandas/core/arrays/masked.py b/pandas/core/arrays/masked.py
index 47605413f..5eaed7072 100644
--- a/pandas/core/arrays/masked.py
+++ b/pandas/core/arrays/masked.py
@@ -50,6 +50,9 @@ class BaseMaskedArray(ExtensionArray, ExtensionOpsMixin):
     def __len__(self) -> int:
         return len(self._data)
 
+    def __invert__(self):
+        return type(self)(~self._data, self._mask)
+
     def to_numpy(
         self, dtype=None, copy=False, na_value: ""Scalar"" = lib.no_default,
     ):
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 6c04212e2..cdaf12392 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -1393,8 +1393,9 @@ class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):
             # inv fails with 0 len
             return self
 
-        arr = operator.inv(com.values_from_object(self))
-        return self.__array_wrap__(arr)
+        new_data = self._data.apply(operator.invert)
+        result = self._constructor(new_data).__finalize__(self)
+        return result
 
     def __nonzero__(self):
         raise ValueError(
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index 722fe152e..17a817116 100755
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -2233,7 +2233,7 @@ def check_bool_indexer(index: Index, key) -> np.ndarray:
         result = result.astype(bool)._values
     else:
         if is_sparse(result):
-            result = result.to_dense()
+            result = np.asarray(result)
         result = check_bool_array_indexer(index, result)
 
     return result
diff --git a/pandas/tests/arrays/test_boolean.py b/pandas/tests/arrays/test_boolean.py
index 200446f79..0c6b187ea 100644
--- a/pandas/tests/arrays/test_boolean.py
+++ b/pandas/tests/arrays/test_boolean.py
@@ -1,15 +1,27 @@
-import operator
+""""""
+This file contains a minimal set of tests for compliance with the extension
+array interface test suite, and should contain no other tests.
+The test suite for the full functionality of the array is located in
+`pandas/tests/arrays/`.
 
+The tests in this file are inherited from the BaseExtensionTests, and only
+minimal tweaks should be applied to get the tests passing (by overwriting a
+parent method).
+
+Additional tests should either be added to one of the BaseExtensionTests
+classes (if they are relevant for the extension interface for all dtypes), or
+be added to the array-specific tests in `pandas/tests/arrays/`.
+
+""""""
 import numpy as np
 import pytest
 
-import pandas.util._test_decorators as td
+from pandas.compat.numpy import _np_version_under1p14
 
 import pandas as pd
 import pandas._testing as tm
-from pandas.arrays import BooleanArray
-from pandas.core.arrays.boolean import coerce_to_array
-from pandas.tests.extension.base import BaseOpsUtil
+from pandas.core.arrays.boolean import BooleanDtype
+from pandas.tests.extension import base
 
 
 def make_data():
@@ -18,7 +30,7 @@ def make_data():
 
 @pytest.fixture
 def dtype():
-    return pd.BooleanDtype()
+    return BooleanDtype()
 
 
 @pytest.fixture
@@ -26,888 +38,314 @@ def data(dtype):
     return pd.array(make_data(), dtype=dtype)
 
 
-def test_boolean_array_constructor():
-    values = np.array([True, False, True, False], dtype=""bool"")
-    mask = np.array([False, False, False, True], dtype=""bool"")
-
-    result = BooleanArray(values, mask)
-    expected = pd.array([True, False, True, None], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-    with pytest.raises(TypeError, match=""values should be boolean numpy array""):
-        BooleanArray(values.tolist(), mask)
-
-    with pytest.raises(TypeError, match=""mask should be boolean numpy array""):
-        BooleanArray(values, mask.tolist())
-
-    with pytest.raises(TypeError, match=""values should be boolean numpy array""):
-        BooleanArray(values.astype(int), mask)
-
-    with pytest.raises(TypeError, match=""mask should be boolean numpy array""):
-        BooleanArray(values, None)
+@pytest.fixture
+def data_for_twos(dtype):
+    return pd.array(np.ones(100), dtype=dtype)
 
-    with pytest.raises(ValueError, match=""values must be a 1D array""):
-        BooleanArray(values.reshape(1, -1), mask)
-
-    with pytest.raises(ValueError, match=""mask must be a 1D array""):
-        BooleanArray(values, mask.reshape(1, -1))
-
-
-def test_boolean_array_constructor_copy():
-    values = np.array([True, False, True, False], dtype=""bool"")
-    mask = np.array([False, False, False, True], dtype=""bool"")
 
-    result = BooleanArray(values, mask)
-    assert result._data is values
-    assert result._mask is mask
-
-    result = BooleanArray(values, mask, copy=True)
-    assert result._data is not values
-    assert result._mask is not mask
-
-
-def test_to_boolean_array():
-    expected = BooleanArray(
-        np.array([True, False, True]), np.array([False, False, False])
-    )
+@pytest.fixture
+def data_missing(dtype):
+    return pd.array([np.nan, True], dtype=dtype)
 
-    result = pd.array([True, False, True], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-    result = pd.array(np.array([True, False, True]), dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-    result = pd.array(np.array([True, False, True], dtype=object), dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-    # with missing values
-    expected = BooleanArray(
-        np.array([True, False, True]), np.array([False, False, True])
-    )
-
-    result = pd.array([True, False, None], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-    result = pd.array(np.array([True, False, None], dtype=object), dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-
-def test_to_boolean_array_all_none():
-    expected = BooleanArray(np.array([True, True, True]), np.array([True, True, True]))
-
-    result = pd.array([None, None, None], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-    result = pd.array(np.array([None, None, None], dtype=object), dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-
-@pytest.mark.parametrize(
-    ""a, b"",
-    [
-        ([True, False, None, np.nan, pd.NA], [True, False, None, None, None]),
-        ([True, np.nan], [True, None]),
-        ([True, pd.NA], [True, None]),
-        ([np.nan, np.nan], [None, None]),
-        (np.array([np.nan, np.nan], dtype=float), [None, None]),
-    ],
-)
-def test_to_boolean_array_missing_indicators(a, b):
-    result = pd.array(a, dtype=""boolean"")
-    expected = pd.array(b, dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-
-@pytest.mark.parametrize(
-    ""values"",
-    [
-        [""foo"", ""bar""],
-        [""1"", ""2""],
-        # ""foo"",
-        [1, 2],
-        [1.0, 2.0],
-        pd.date_range(""20130101"", periods=2),
-        np.array([""foo""]),
-        np.array([1, 2]),
-        np.array([1.0, 2.0]),
-        [np.nan, {""a"": 1}],
-    ],
-)
-def test_to_boolean_array_error(values):
-    # error in converting existing arrays to BooleanArray
-    with pytest.raises(TypeError):
-        pd.array(values, dtype=""boolean"")
-
-
-def test_to_boolean_array_from_integer_array():
-    result = pd.array(np.array([1, 0, 1, 0]), dtype=""boolean"")
-    expected = pd.array([True, False, True, False], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-    # with missing values
-    result = pd.array(np.array([1, 0, 1, None]), dtype=""boolean"")
-    expected = pd.array([True, False, True, None], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-
-def test_to_boolean_array_from_float_array():
-    result = pd.array(np.array([1.0, 0.0, 1.0, 0.0]), dtype=""boolean"")
-    expected = pd.array([True, False, True, False], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-    # with missing values
-    result = pd.array(np.array([1.0, 0.0, 1.0, np.nan]), dtype=""boolean"")
-    expected = pd.array([True, False, True, None], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-
-def test_to_boolean_array_integer_like():
-    # integers of 0's and 1's
-    result = pd.array([1, 0, 1, 0], dtype=""boolean"")
-    expected = pd.array([True, False, True, False], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-    # with missing values
-    result = pd.array([1, 0, 1, None], dtype=""boolean"")
-    expected = pd.array([True, False, True, None], dtype=""boolean"")
-    tm.assert_extension_array_equal(result, expected)
-
-
-def test_coerce_to_array():
-    # TODO this is currently not public API
-    values = np.array([True, False, True, False], dtype=""bool"")
-    mask = np.array([False, False, False, True], dtype=""bool"")
-    result = BooleanArray(*coerce_to_array(values, mask=mask))
-    expected = BooleanArray(values, mask)
-    tm.assert_extension_array_equal(result, expected)
-    assert result._data is values
-    assert result._mask is mask
-    result = BooleanArray(*coerce_to_array(values, mask=mask, copy=True))
-    expected = BooleanArray(values, mask)
-    tm.assert_extension_array_equal(result, expected)
-    assert result._data is not values
-    assert result._mask is not mask
-
-    # mixed missing from values and mask
-    values = [True, False, None, False]
-    mask = np.array([False, False, False, True], dtype=""bool"")
-    result = BooleanArray(*coerce_to_array(values, mask=mask))
-    expected = BooleanArray(
-        np.array([True, False, True, True]), np.array([False, False, True, True])
-    )
-    tm.assert_extension_array_equal(result, expected)
-    result = BooleanArray(*coerce_to_array(np.array(values, dtype=object), mask=mask))
-    tm.assert_extension_array_equal(result, expected)
-    result = BooleanArray(*coerce_to_array(values, mask=mask.tolist()))
-    tm.assert_extension_array_equal(result, expected)
-
-    # raise errors for wrong dimension
-    values = np.array([True, False, True, False], dtype=""bool"")
-    mask = np.array([False, False, False, True], dtype=""bool"")
-
-    with pytest.raises(ValueError, match=""values must be a 1D list-like""):
-        coerce_to_array(values.reshape(1, -1))
-
-    with pytest.raises(ValueError, match=""mask must be a 1D list-like""):
-        coerce_to_array(values, mask=mask.reshape(1, -1))
-
-
-def test_coerce_to_array_from_boolean_array():
-    # passing BooleanArray to coerce_to_array
-    values = np.array([True, False, True, False], dtype=""bool"")
-    mask = np.array([False, False, False, True], dtype=""bool"")
-    arr = BooleanArray(values, mask)
-    result = BooleanArray(*coerce_to_array(arr))
-    tm.assert_extension_array_equal(result, arr)
-    # no copy
-    assert result._data is arr._data
-    assert result._mask is arr._mask
-
-    result = BooleanArray(*coerce_to_array(arr), copy=True)
-    tm.assert_extension_array_equal(result, arr)
-    assert result._data is not arr._data
-    assert result._mask is not arr._mask
-
-    with pytest.raises(ValueError, match=""cannot pass mask for BooleanArray input""):
-        coerce_to_array(arr, mask=mask)
-
-
-def test_coerce_to_numpy_array():
-    # with missing values -> object dtype
-    arr = pd.array([True, False, None], dtype=""boolean"")
-    result = np.array(arr)
-    expected = np.array([True, False, pd.NA], dtype=""object"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    # also with no missing values -> object dtype
-    arr = pd.array([True, False, True], dtype=""boolean"")
-    result = np.array(arr)
-    expected = np.array([True, False, True], dtype=""object"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    # force bool dtype
-    result = np.array(arr, dtype=""bool"")
-    expected = np.array([True, False, True], dtype=""bool"")
-    tm.assert_numpy_array_equal(result, expected)
-    # with missing values will raise error
-    arr = pd.array([True, False, None], dtype=""boolean"")
-    with pytest.raises(ValueError):
-        np.array(arr, dtype=""bool"")
-
-
-def test_to_boolean_array_from_strings():
-    result = BooleanArray._from_sequence_of_strings(
-        np.array([""True"", ""False"", np.nan], dtype=object)
-    )
-    expected = BooleanArray(
-        np.array([True, False, False]), np.array([False, False, True])
-    )
-
-    tm.assert_extension_array_equal(result, expected)
-
-
-def test_to_boolean_array_from_strings_invalid_string():
-    with pytest.raises(ValueError, match=""cannot be cast""):
-        BooleanArray._from_sequence_of_strings([""donkey""])
-
-
-def test_repr():
-    df = pd.DataFrame({""A"": pd.array([True, False, None], dtype=""boolean"")})
-    expected = ""       A\n0   True\n1  False\n2   <NA>""
-    assert repr(df) == expected
-
-    expected = ""0     True\n1    False\n2     <NA>\nName: A, dtype: boolean""
-    assert repr(df.A) == expected
-
-    expected = ""<BooleanArray>\n[True, False, <NA>]\nLength: 3, dtype: boolean""
-    assert repr(df.A.array) == expected
-
-
-@pytest.mark.parametrize(""box"", [True, False], ids=[""series"", ""array""])
-def test_to_numpy(box):
-    con = pd.Series if box else pd.array
-    # default (with or without missing values) -> object dtype
-    arr = con([True, False, True], dtype=""boolean"")
-    result = arr.to_numpy()
-    expected = np.array([True, False, True], dtype=""object"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    arr = con([True, False, None], dtype=""boolean"")
-    result = arr.to_numpy()
-    expected = np.array([True, False, pd.NA], dtype=""object"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    arr = con([True, False, None], dtype=""boolean"")
-    result = arr.to_numpy(dtype=""str"")
-    expected = np.array([True, False, pd.NA], dtype=""<U5"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    # no missing values -> can convert to bool, otherwise raises
-    arr = con([True, False, True], dtype=""boolean"")
-    result = arr.to_numpy(dtype=""bool"")
-    expected = np.array([True, False, True], dtype=""bool"")
-    tm.assert_numpy_array_equal(result, expected)
 
-    arr = con([True, False, None], dtype=""boolean"")
-    with pytest.raises(ValueError, match=""cannot convert to 'bool'-dtype""):
-        result = arr.to_numpy(dtype=""bool"")
-
-    # specify dtype and na_value
-    arr = con([True, False, None], dtype=""boolean"")
-    result = arr.to_numpy(dtype=object, na_value=None)
-    expected = np.array([True, False, None], dtype=""object"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    result = arr.to_numpy(dtype=bool, na_value=False)
-    expected = np.array([True, False, False], dtype=""bool"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    result = arr.to_numpy(dtype=""int64"", na_value=-99)
-    expected = np.array([1, 0, -99], dtype=""int64"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    result = arr.to_numpy(dtype=""float64"", na_value=np.nan)
-    expected = np.array([1, 0, np.nan], dtype=""float64"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    # converting to int or float without specifying na_value raises
-    with pytest.raises(ValueError, match=""cannot convert to 'int64'-dtype""):
-        arr.to_numpy(dtype=""int64"")
-    with pytest.raises(ValueError, match=""cannot convert to 'float64'-dtype""):
-        arr.to_numpy(dtype=""float64"")
-
-
-def test_to_numpy_copy():
-    # to_numpy can be zero-copy if no missing values
-    arr = pd.array([True, False, True], dtype=""boolean"")
-    result = arr.to_numpy(dtype=bool)
-    result[0] = False
-    tm.assert_extension_array_equal(
-        arr, pd.array([False, False, True], dtype=""boolean"")
-    )
-
-    arr = pd.array([True, False, True], dtype=""boolean"")
-    result = arr.to_numpy(dtype=bool, copy=True)
-    result[0] = False
-    tm.assert_extension_array_equal(arr, pd.array([True, False, True], dtype=""boolean""))
-
-
-def test_astype():
-    # with missing values
-    arr = pd.array([True, False, None], dtype=""boolean"")
-
-    with pytest.raises(ValueError, match=""cannot convert NA to integer""):
-        arr.astype(""int64"")
-
-    with pytest.raises(ValueError, match=""cannot convert float NaN to""):
-        arr.astype(""bool"")
-
-    result = arr.astype(""float64"")
-    expected = np.array([1, 0, np.nan], dtype=""float64"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    result = arr.astype(""str"")
-    expected = np.array([""True"", ""False"", ""<NA>""], dtype=""object"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    # no missing values
-    arr = pd.array([True, False, True], dtype=""boolean"")
-    result = arr.astype(""int64"")
-    expected = np.array([1, 0, 1], dtype=""int64"")
-    tm.assert_numpy_array_equal(result, expected)
-
-    result = arr.astype(""bool"")
-    expected = np.array([True, False, True], dtype=""bool"")
-    tm.assert_numpy_array_equal(result, expected)
-
-
-def test_astype_to_boolean_array():
-    # astype to BooleanArray
-    arr = pd.array([True, False, None], dtype=""boolean"")
-
-    result = arr.astype(""boolean"")
-    tm.assert_extension_array_equal(result, arr)
-    result = arr.astype(pd.BooleanDtype())
-    tm.assert_extension_array_equal(result, arr)
-
-
-def test_astype_to_integer_array():
-    # astype to IntegerArray
-    arr = pd.array([True, False, None], dtype=""boolean"")
-
-    result = arr.astype(""Int64"")
-    expected = pd.array([1, 0, None], dtype=""Int64"")
-    tm.assert_extension_array_equal(result, expected)
-
-
-@pytest.mark.parametrize(""na"", [None, np.nan, pd.NA])
-def test_setitem_missing_values(na):
-    arr = pd.array([True, False, None], dtype=""boolean"")
-    expected = pd.array([True, None, None], dtype=""boolean"")
-    arr[1] = na
-    tm.assert_extension_array_equal(arr, expected)
-
-
-@pytest.mark.parametrize(
-    ""ufunc"", [np.add, np.logical_or, np.logical_and, np.logical_xor]
-)
-def test_ufuncs_binary(ufunc):
-    # two BooleanArrays
-    a = pd.array([True, False, None], dtype=""boolean"")
-    result = ufunc(a, a)
-    expected = pd.array(ufunc(a._data, a._data), dtype=""boolean"")
-    expected[a._mask] = np.nan
-    tm.assert_extension_array_equal(result, expected)
-
-    s = pd.Series(a)
-    result = ufunc(s, a)
-    expected = pd.Series(ufunc(a._data, a._data), dtype=""boolean"")
-    expected[a._mask] = np.nan
-    tm.assert_series_equal(result, expected)
-
-    # Boolean with numpy array
-    arr = np.array([True, True, False])
-    result = ufunc(a, arr)
-    expected = pd.array(ufunc(a._data, arr), dtype=""boolean"")
-    expected[a._mask] = np.nan
-    tm.assert_extension_array_equal(result, expected)
-
-    result = ufunc(arr, a)
-    expected = pd.array(ufunc(arr, a._data), dtype=""boolean"")
-    expected[a._mask] = np.nan
-    tm.assert_extension_array_equal(result, expected)
-
-    # BooleanArray with scalar
-    result = ufunc(a, True)
-    expected = pd.array(ufunc(a._data, True), dtype=""boolean"")
-    expected[a._mask] = np.nan
-    tm.assert_extension_array_equal(result, expected)
-
-    result = ufunc(True, a)
-    expected = pd.array(ufunc(True, a._data), dtype=""boolean"")
-    expected[a._mask] = np.nan
-    tm.assert_extension_array_equal(result, expected)
-
-    # not handled types
-    with pytest.raises(TypeError):
-        ufunc(a, ""test"")
-
-
-@pytest.mark.parametrize(""ufunc"", [np.logical_not])
-def test_ufuncs_unary(ufunc):
-    a = pd.array([True, False, None], dtype=""boolean"")
-    result = ufunc(a)
-    expected = pd.array(ufunc(a._data), dtype=""boolean"")
-    expected[a._mask] = np.nan
-    tm.assert_extension_array_equal(result, expected)
-
-    s = pd.Series(a)
-    result = ufunc(s)
-    expected = pd.Series(ufunc(a._data), dtype=""boolean"")
-    expected[a._mask] = np.nan
-    tm.assert_series_equal(result, expected)
-
-
-@pytest.mark.parametrize(""values"", [[True, False], [True, None]])
-def test_ufunc_reduce_raises(values):
-    a = pd.array(values, dtype=""boolean"")
-    with pytest.raises(NotImplementedError):
-        np.add.reduce(a)
-
-
-class TestLogicalOps(BaseOpsUtil):
-    def test_numpy_scalars_ok(self, all_logical_operators):
-        a = pd.array([True, False, None], dtype=""boolean"")
-        op = getattr(a, all_logical_operators)
-
-        tm.assert_extension_array_equal(op(True), op(np.bool(True)))
-        tm.assert_extension_array_equal(op(False), op(np.bool(False)))
-
-    def get_op_from_name(self, op_name):
-        short_opname = op_name.strip(""_"")
-        short_opname = short_opname if ""xor"" in short_opname else short_opname + ""_""
-        try:
-            op = getattr(operator, short_opname)
-        except AttributeError:
-            # Assume it is the reverse operator
-            rop = getattr(operator, short_opname[1:])
-            op = lambda x, y: rop(y, x)
-
-        return op
-
-    def test_empty_ok(self, all_logical_operators):
-        a = pd.array([], dtype=""boolean"")
-        op_name = all_logical_operators
-        result = getattr(a, op_name)(True)
-        tm.assert_extension_array_equal(a, result)
-
-        result = getattr(a, op_name)(False)
-        tm.assert_extension_array_equal(a, result)
-
-        # TODO: pd.NA
-        # result = getattr(a, op_name)(pd.NA)
-        # tm.assert_extension_array_equal(a, result)
-
-    def test_logical_length_mismatch_raises(self, all_logical_operators):
-        op_name = all_logical_operators
-        a = pd.array([True, False, None], dtype=""boolean"")
-        msg = ""Lengths must match to compare""
-
-        with pytest.raises(ValueError, match=msg):
-            getattr(a, op_name)([True, False])
-
-        with pytest.raises(ValueError, match=msg):
-            getattr(a, op_name)(np.array([True, False]))
-
-        with pytest.raises(ValueError, match=msg):
-            getattr(a, op_name)(pd.array([True, False], dtype=""boolean""))
-
-    def test_logical_nan_raises(self, all_logical_operators):
-        op_name = all_logical_operators
-        a = pd.array([True, False, None], dtype=""boolean"")
-        msg = ""Got float instead""
-
-        with pytest.raises(TypeError, match=msg):
-            getattr(a, op_name)(np.nan)
-
-    @pytest.mark.parametrize(""other"", [""a"", 1])
-    def test_non_bool_or_na_other_raises(self, other, all_logical_operators):
-        a = pd.array([True, False], dtype=""boolean"")
-        with pytest.raises(TypeError, match=str(type(other).__name__)):
-            getattr(a, all_logical_operators)(other)
-
-    def test_kleene_or(self):
-        # A clear test of behavior.
-        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
-        b = pd.array([True, False, None] * 3, dtype=""boolean"")
-        result = a | b
-        expected = pd.array(
-            [True, True, True, True, False, None, True, None, None], dtype=""boolean""
-        )
-        tm.assert_extension_array_equal(result, expected)
+@pytest.fixture
+def data_for_sorting(dtype):
+    return pd.array([True, True, False], dtype=dtype)
 
-        result = b | a
-        tm.assert_extension_array_equal(result, expected)
 
-        # ensure we haven't mutated anything inplace
-        tm.assert_extension_array_equal(
-            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
-        )
-        tm.assert_extension_array_equal(
-            b, pd.array([True, False, None] * 3, dtype=""boolean"")
-        )
+@pytest.fixture
+def data_missing_for_sorting(dtype):
+    return pd.array([True, np.nan, False], dtype=dtype)
 
-    @pytest.mark.parametrize(
-        ""other, expected"",
-        [
-            (pd.NA, [True, None, None]),
-            (True, [True, True, True]),
-            (np.bool_(True), [True, True, True]),
-            (False, [True, False, None]),
-            (np.bool_(False), [True, False, None]),
-        ],
-    )
-    def test_kleene_or_scalar(self, other, expected):
-        # TODO: test True & False
-        a = pd.array([True, False, None], dtype=""boolean"")
-        result = a | other
-        expected = pd.array(expected, dtype=""boolean"")
-        tm.assert_extension_array_equal(result, expected)
-
-        result = other | a
-        tm.assert_extension_array_equal(result, expected)
-
-        # ensure we haven't mutated anything inplace
-        tm.assert_extension_array_equal(
-            a, pd.array([True, False, None], dtype=""boolean"")
-        )
 
-    def test_kleene_and(self):
-        # A clear test of behavior.
-        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
-        b = pd.array([True, False, None] * 3, dtype=""boolean"")
-        result = a & b
-        expected = pd.array(
-            [True, False, None, False, False, False, None, False, None], dtype=""boolean""
-        )
-        tm.assert_extension_array_equal(result, expected)
+@pytest.fixture
+def na_cmp():
+    # we are pd.NA
+    return lambda x, y: x is pd.NA and y is pd.NA
 
-        result = b & a
-        tm.assert_extension_array_equal(result, expected)
 
-        # ensure we haven't mutated anything inplace
-        tm.assert_extension_array_equal(
-            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
-        )
-        tm.assert_extension_array_equal(
-            b, pd.array([True, False, None] * 3, dtype=""boolean"")
-        )
+@pytest.fixture
+def na_value():
+    return pd.NA
 
-    @pytest.mark.parametrize(
-        ""other, expected"",
-        [
-            (pd.NA, [None, False, None]),
-            (True, [True, False, None]),
-            (False, [False, False, False]),
-            (np.bool_(True), [True, False, None]),
-            (np.bool_(False), [False, False, False]),
-        ],
-    )
-    def test_kleene_and_scalar(self, other, expected):
-        a = pd.array([True, False, None], dtype=""boolean"")
-        result = a & other
-        expected = pd.array(expected, dtype=""boolean"")
-        tm.assert_extension_array_equal(result, expected)
-
-        result = other & a
-        tm.assert_extension_array_equal(result, expected)
-
-        # ensure we haven't mutated anything inplace
-        tm.assert_extension_array_equal(
-            a, pd.array([True, False, None], dtype=""boolean"")
-        )
 
-    def test_kleene_xor(self):
-        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
-        b = pd.array([True, False, None] * 3, dtype=""boolean"")
-        result = a ^ b
-        expected = pd.array(
-            [False, True, None, True, False, None, None, None, None], dtype=""boolean""
-        )
-        tm.assert_extension_array_equal(result, expected)
+@pytest.fixture
+def data_for_grouping(dtype):
+    b = True
+    a = False
+    na = np.nan
+    return pd.array([b, b, na, na, a, a, b], dtype=dtype)
 
-        result = b ^ a
-        tm.assert_extension_array_equal(result, expected)
 
-        # ensure we haven't mutated anything inplace
-        tm.assert_extension_array_equal(
-            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
-        )
-        tm.assert_extension_array_equal(
-            b, pd.array([True, False, None] * 3, dtype=""boolean"")
-        )
+class TestDtype(base.BaseDtypeTests):
+    pass
 
-    @pytest.mark.parametrize(
-        ""other, expected"",
-        [
-            (pd.NA, [None, None, None]),
-            (True, [False, True, None]),
-            (np.bool_(True), [False, True, None]),
-            (np.bool_(False), [True, False, None]),
-        ],
-    )
-    def test_kleene_xor_scalar(self, other, expected):
-        a = pd.array([True, False, None], dtype=""boolean"")
-        result = a ^ other
-        expected = pd.array(expected, dtype=""boolean"")
-        tm.assert_extension_array_equal(result, expected)
-
-        result = other ^ a
-        tm.assert_extension_array_equal(result, expected)
-
-        # ensure we haven't mutated anything inplace
-        tm.assert_extension_array_equal(
-            a, pd.array([True, False, None], dtype=""boolean"")
-        )
 
-    @pytest.mark.parametrize(
-        ""other"", [True, False, pd.NA, [True, False, None] * 3],
-    )
-    def test_no_masked_assumptions(self, other, all_logical_operators):
-        # The logical operations should not assume that masked values are False!
-        a = pd.arrays.BooleanArray(
-            np.array([True, True, True, False, False, False, True, False, True]),
-            np.array([False] * 6 + [True, True, True]),
-        )
-        b = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
-        if isinstance(other, list):
-            other = pd.array(other, dtype=""boolean"")
+class TestInterface(base.BaseInterfaceTests):
+    pass
+
+
+class TestConstructors(base.BaseConstructorsTests):
+    pass
+
+
+class TestGetitem(base.BaseGetitemTests):
+    pass
+
+
+class TestSetitem(base.BaseSetitemTests):
+    pass
+
+
+class TestMissing(base.BaseMissingTests):
+    pass
+
 
-        result = getattr(a, all_logical_operators)(other)
-        expected = getattr(b, all_logical_operators)(other)
-        tm.assert_extension_array_equal(result, expected)
+class TestArithmeticOps(base.BaseArithmeticOpsTests):
+    def check_opname(self, s, op_name, other, exc=None):
+        # overwriting to indicate ops don't raise an error
+        super().check_opname(s, op_name, other, exc=None)
 
-        if isinstance(other, BooleanArray):
-            other._data[other._mask] = True
-            a._data[a._mask] = False
+    def _check_op(self, s, op, other, op_name, exc=NotImplementedError):
+        if exc is None:
+            if op_name in (""__sub__"", ""__rsub__""):
+                # subtraction for bools raises TypeError (but not yet in 1.13)
+                if _np_version_under1p14:
+                    pytest.skip(""__sub__ does not yet raise in numpy 1.13"")
+                with pytest.raises(TypeError):
+                    op(s, other)
 
-            result = getattr(a, all_logical_operators)(other)
-            expected = getattr(b, all_logical_operators)(other)
-            tm.assert_extension_array_equal(result, expected)
+                return
 
+            result = op(s, other)
+            expected = s.combine(other, op)
 
-class TestComparisonOps(BaseOpsUtil):
-    def _compare_other(self, data, op_name, other):
-        op = self.get_op_from_name(op_name)
+            if op_name in (
+                ""__floordiv__"",
+                ""__rfloordiv__"",
+                ""__pow__"",
+                ""__rpow__"",
+                ""__mod__"",
+                ""__rmod__"",
+            ):
+                # combine keeps boolean type
+                expected = expected.astype(""Int8"")
+            elif op_name in (""__truediv__"", ""__rtruediv__""):
+                # combine with bools does not generate the correct result
+                #  (numpy behaviour for div is to regard the bools as numeric)
+                expected = s.astype(float).combine(other, op)
+            if op_name == ""__rpow__"":
+                # for rpow, combine does not propagate NaN
+                expected[result.isna()] = np.nan
+            self.assert_series_equal(result, expected)
+        else:
+            with pytest.raises(exc):
+                op(s, other)
 
-        # array
-        result = pd.Series(op(data, other))
-        expected = pd.Series(op(data._data, other), dtype=""boolean"")
-        # propagate NAs
-        expected[data._mask] = pd.NA
+    def _check_divmod_op(self, s, op, other, exc=None):
+        # override to not raise an error
+        super()._check_divmod_op(s, op, other, None)
 
-        tm.assert_series_equal(result, expected)
+    @pytest.mark.skip(reason=""BooleanArray does not error on ops"")
+    def test_error(self, data, all_arithmetic_operators):
+        # other specific errors tested in the boolean array specific tests
+        pass
 
-        # series
-        s = pd.Series(data)
-        result = op(s, other)
 
-        expected = pd.Series(data._data)
-        expected = op(expected, other)
-        expected = expected.astype(""boolean"")
-        # propagate NAs
-        expected[data._mask] = pd.NA
+class TestComparisonOps(base.BaseComparisonOpsTests):
+    def check_opname(self, s, op_name, other, exc=None):
+        # overwriting to indicate ops don't raise an error
+        super().check_opname(s, op_name, other, exc=None)
 
-        tm.assert_series_equal(result, expected)
+    def _compare_other(self, s, data, op_name, other):
+        self.check_opname(s, op_name, other)
 
+    @pytest.mark.skip(reason=""Tested in tests/arrays/test_boolean.py"")
     def test_compare_scalar(self, data, all_compare_operators):
-        op_name = all_compare_operators
-        self._compare_other(data, op_name, True)
+        pass
 
+    @pytest.mark.skip(reason=""Tested in tests/arrays/test_boolean.py"")
     def test_compare_array(self, data, all_compare_operators):
-        op_name = all_compare_operators
-        other = pd.array([True] * len(data), dtype=""boolean"")
-        self._compare_other(data, op_name, other)
-        other = np.array([True] * len(data))
-        self._compare_other(data, op_name, other)
-        other = pd.Series([True] * len(data))
-        self._compare_other(data, op_name, other)
-
-    @pytest.mark.parametrize(""other"", [True, False, pd.NA])
-    def test_scalar(self, other, all_compare_operators):
-        op = self.get_op_from_name(all_compare_operators)
-        a = pd.array([True, False, None], dtype=""boolean"")
-
-        result = op(a, other)
-
-        if other is pd.NA:
-            expected = pd.array([None, None, None], dtype=""boolean"")
-        else:
-            values = op(a._data, other)
-            expected = BooleanArray(values, a._mask, copy=True)
-        tm.assert_extension_array_equal(result, expected)
-
-        # ensure we haven't mutated anything inplace
-        result[0] = None
-        tm.assert_extension_array_equal(
-            a, pd.array([True, False, None], dtype=""boolean"")
+        pass
+
+
+class TestReshaping(base.BaseReshapingTests):
+    pass
+
+
+class TestMethods(base.BaseMethodsTests):
+    @pytest.mark.parametrize(""na_sentinel"", [-1, -2])
+    def test_factorize(self, data_for_grouping, na_sentinel):
+        # override because we only have 2 unique values
+        labels, uniques = pd.factorize(data_for_grouping, na_sentinel=na_sentinel)
+        expected_labels = np.array(
+            [0, 0, na_sentinel, na_sentinel, 1, 1, 0], dtype=np.intp
+        )
+        expected_uniques = data_for_grouping.take([0, 4])
+
+        tm.assert_numpy_array_equal(labels, expected_labels)
+        self.assert_extension_array_equal(uniques, expected_uniques)
+
+    def test_combine_le(self, data_repeated):
+        # override because expected needs to be boolean instead of bool dtype
+        orig_data1, orig_data2 = data_repeated(2)
+        s1 = pd.Series(orig_data1)
+        s2 = pd.Series(orig_data2)
+        result = s1.combine(s2, lambda x1, x2: x1 <= x2)
+        expected = pd.Series(
+            [a <= b for (a, b) in zip(list(orig_data1), list(orig_data2))],
+            dtype=""boolean"",
         )
+        self.assert_series_equal(result, expected)
+
+        val = s1.iloc[0]
+        result = s1.combine(val, lambda x1, x2: x1 <= x2)
+        expected = pd.Series([a <= val for a in list(orig_data1)], dtype=""boolean"")
+        self.assert_series_equal(result, expected)
+
+    def test_searchsorted(self, data_for_sorting, as_series):
+        # override because we only have 2 unique values
+        data_for_sorting = pd.array([True, False], dtype=""boolean"")
+        b, a = data_for_sorting
+        arr = type(data_for_sorting)._from_sequence([a, b])
+
+        if as_series:
+            arr = pd.Series(arr)
+        assert arr.searchsorted(a) == 0
+        assert arr.searchsorted(a, side=""right"") == 1
+
+        assert arr.searchsorted(b) == 1
+        assert arr.searchsorted(b, side=""right"") == 2
+
+        result = arr.searchsorted(arr.take([0, 1]))
+        expected = np.array([0, 1], dtype=np.intp)
+
+        tm.assert_numpy_array_equal(result, expected)
+
+        # sorter
+        sorter = np.array([1, 0])
+        assert data_for_sorting.searchsorted(a, sorter=sorter) == 0
+
+    @pytest.mark.skip(reason=""uses nullable integer"")
+    def test_value_counts(self, all_data, dropna):
+        return super().test_value_counts(all_data, dropna)
 
-    def test_array(self, all_compare_operators):
-        op = self.get_op_from_name(all_compare_operators)
-        a = pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
-        b = pd.array([True, False, None] * 3, dtype=""boolean"")
 
-        result = op(a, b)
+class TestCasting(base.BaseCastingTests):
+    pass
 
-        values = op(a._data, b._data)
-        mask = a._mask | b._mask
-        expected = BooleanArray(values, mask)
-        tm.assert_extension_array_equal(result, expected)
 
-        # ensure we haven't mutated anything inplace
-        result[0] = None
-        tm.assert_extension_array_equal(
-            a, pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype=""boolean"")
+class TestGroupby(base.BaseGroupbyTests):
+    """"""
+    Groupby-specific tests are overridden because boolean only has 2
+    unique values, base tests uses 3 groups.
+    """"""
+
+    def test_grouping_grouper(self, data_for_grouping):
+        df = pd.DataFrame(
+            {""A"": [""B"", ""B"", None, None, ""A"", ""A"", ""B""], ""B"": data_for_grouping}
+        )
+        gr1 = df.groupby(""A"").grouper.groupings[0]
+        gr2 = df.groupby(""B"").grouper.groupings[0]
+
+        tm.assert_numpy_array_equal(gr1.grouper, df.A.values)
+        tm.assert_extension_array_equal(gr2.grouper, data_for_grouping)
+
+    @pytest.mark.parametrize(""as_index"", [True, False])
+    def test_groupby_extension_agg(self, as_index, data_for_grouping):
+        df = pd.DataFrame({""A"": [1, 1, 2, 2, 3, 3, 1], ""B"": data_for_grouping})
+        result = df.groupby(""B"", as_index=as_index).A.mean()
+        _, index = pd.factorize(data_for_grouping, sort=True)
+
+        index = pd.Index(index, name=""B"")
+        expected = pd.Series([3, 1], index=index, name=""A"")
+        if as_index:
+            self.assert_series_equal(result, expected)
+        else:
+            expected = expected.reset_index()
+            self.assert_frame_equal(result, expected)
+
+    def test_groupby_extension_no_sort(self, data_for_grouping):
+        df = pd.DataFrame({""A"": [1, 1, 2, 2, 3, 3, 1], ""B"": data_for_grouping})
+        result = df.groupby(""B"", sort=False).A.mean()
+        _, index = pd.factorize(data_for_grouping, sort=False)
+
+        index = pd.Index(index, name=""B"")
+        expected = pd.Series([1, 3], index=index, name=""A"")
+        self.assert_series_equal(result, expected)
+
+    def test_groupby_extension_transform(self, data_for_grouping):
+        valid = data_for_grouping[~data_for_grouping.isna()]
+        df = pd.DataFrame({""A"": [1, 1, 3, 3, 1], ""B"": valid})
+
+        result = df.groupby(""B"").A.transform(len)
+        expected = pd.Series([3, 3, 2, 2, 3], name=""A"")
+
+        self.assert_series_equal(result, expected)
+
+    def test_groupby_extension_apply(self, data_for_grouping, groupby_apply_op):
+        df = pd.DataFrame({""A"": [1, 1, 2, 2, 3, 3, 1], ""B"": data_for_grouping})
+        df.groupby(""B"").apply(groupby_apply_op)
+        df.groupby(""B"").A.apply(groupby_apply_op)
+        df.groupby(""A"").apply(groupby_apply_op)
+        df.groupby(""A"").B.apply(groupby_apply_op)
+
+    def test_groupby_apply_identity(self, data_for_grouping):
+        df = pd.DataFrame({""A"": [1, 1, 2, 2, 3, 3, 1], ""B"": data_for_grouping})
+        result = df.groupby(""A"").B.apply(lambda x: x.array)
+        expected = pd.Series(
+            [
+                df.B.iloc[[0, 1, 6]].array,
+                df.B.iloc[[2, 3]].array,
+                df.B.iloc[[4, 5]].array,
+            ],
+            index=pd.Index([1, 2, 3], name=""A""),
+            name=""B"",
         )
-        tm.assert_extension_array_equal(
-            b, pd.array([True, False, None] * 3, dtype=""boolean"")
+        self.assert_series_equal(result, expected)
+
+    def test_in_numeric_groupby(self, data_for_grouping):
+        df = pd.DataFrame(
+            {
+                ""A"": [1, 1, 2, 2, 3, 3, 1],
+                ""B"": data_for_grouping,
+                ""C"": [1, 1, 1, 1, 1, 1, 1],
+            }
         )
+        result = df.groupby(""A"").sum().columns
+
+        if data_for_grouping.dtype._is_numeric:
+            expected = pd.Index([""B"", ""C""])
+        else:
+            expected = pd.Index([""C""])
 
+        tm.assert_index_equal(result, expected)
 
-class TestArithmeticOps(BaseOpsUtil):
-    def test_error(self, data, all_arithmetic_operators):
-        # invalid ops
-
-        op = all_arithmetic_operators
-        s = pd.Series(data)
-        ops = getattr(s, op)
-        opa = getattr(data, op)
-
-        # invalid scalars
-        with pytest.raises(TypeError):
-            ops(""foo"")
-        with pytest.raises(TypeError):
-            ops(pd.Timestamp(""20180101""))
-
-        # invalid array-likes
-        if op not in (""__mul__"", ""__rmul__""):
-            # TODO(extension) numpy's mul with object array sees booleans as numbers
-            with pytest.raises(TypeError):
-                ops(pd.Series(""foo"", index=s.index))
-
-        # 2d
-        result = opa(pd.DataFrame({""A"": s}))
-        assert result is NotImplemented
-
-        with pytest.raises(NotImplementedError):
-            opa(np.arange(len(s)).reshape(-1, len(s)))
-
-
-@pytest.mark.parametrize(""dropna"", [True, False])
-def test_reductions_return_types(dropna, data, all_numeric_reductions):
-    op = all_numeric_reductions
-    s = pd.Series(data)
-    if dropna:
-        s = s.dropna()
-
-    if op in (""sum"", ""prod""):
-        assert isinstance(getattr(s, op)(), np.int64)
-    elif op in (""min"", ""max""):
-        assert isinstance(getattr(s, op)(), np.bool_)
-    else:
-        # ""mean"", ""std"", ""var"", ""median"", ""kurt"", ""skew""
-        assert isinstance(getattr(s, op)(), np.float64)
-
-
-@pytest.mark.parametrize(
-    ""values, exp_any, exp_all, exp_any_noskip, exp_all_noskip"",
-    [
-        ([True, pd.NA], True, True, True, pd.NA),
-        ([False, pd.NA], False, False, pd.NA, False),
-        ([pd.NA], False, True, pd.NA, pd.NA),
-        ([], False, True, False, True),
-    ],
-)
-def test_any_all(values, exp_any, exp_all, exp_any_noskip, exp_all_noskip):
-    # the methods return numpy scalars
-    exp_any = pd.NA if exp_any is pd.NA else np.bool_(exp_any)
-    exp_all = pd.NA if exp_all is pd.NA else np.bool_(exp_all)
-    exp_any_noskip = pd.NA if exp_any_noskip is pd.NA else np.bool_(exp_any_noskip)
-    exp_all_noskip = pd.NA if exp_all_noskip is pd.NA else np.bool_(exp_all_noskip)
-
-    for con in [pd.array, pd.Series]:
-        a = con(values, dtype=""boolean"")
-        assert a.any() is exp_any
-        assert a.all() is exp_all
-        assert a.any(skipna=False) is exp_any_noskip
-        assert a.all(skipna=False) is exp_all_noskip
-
-        assert np.any(a.any()) is exp_any
-        assert np.all(a.all()) is exp_all
-
-
-# TODO when BooleanArray coerces to object dtype numpy array, need to do conversion
-# manually in the indexing code
-# def test_indexing_boolean_mask():
-#     arr = pd.array([1, 2, 3, 4], dtype=""Int64"")
-#     mask = pd.array([True, False, True, False], dtype=""boolean"")
-#     result = arr[mask]
-#     expected = pd.array([1, 3], dtype=""Int64"")
-#     tm.assert_extension_array_equal(result, expected)
-
-#     # missing values -> error
-#     mask = pd.array([True, False, True, None], dtype=""boolean"")
-#     with pytest.raises(IndexError):
-#         result = arr[mask]
-
-
-@td.skip_if_no(""pyarrow"", min_version=""0.15.0"")
-def test_arrow_array(data):
-    # protocol added in 0.15.0
-    import pyarrow as pa
-
-    arr = pa.array(data)
-
-    # TODO use to_numpy(na_value=None) here
-    data_object = np.array(data, dtype=object)
-    data_object[data.isna()] = None
-    expected = pa.array(data_object, type=pa.bool_(), from_pandas=True)
-    assert arr.equals(expected)
-
-
-@td.skip_if_no(""pyarrow"", min_version=""0.15.1.dev"")
-def test_arrow_roundtrip():
-    # roundtrip possible from arrow 1.0.0
-    import pyarrow as pa
-
-    data = pd.array([True, False, None], dtype=""boolean"")
-    df = pd.DataFrame({""a"": data})
-    table = pa.table(df)
-    assert table.field(""a"").type == ""bool""
-    result = table.to_pandas()
-    assert isinstance(result[""a""].dtype, pd.BooleanDtype)
-    tm.assert_frame_equal(result, df)
-
-
-def test_value_counts_na():
-    arr = pd.array([True, False, pd.NA], dtype=""boolean"")
-    result = arr.value_counts(dropna=False)
-    expected = pd.Series([1, 1, 1], index=[True, False, pd.NA], dtype=""Int64"")
-    tm.assert_series_equal(result, expected)
-
-    result = arr.value_counts(dropna=True)
-    expected = pd.Series([1, 1], index=[True, False], dtype=""Int64"")
-    tm.assert_series_equal(result, expected)
-
-
-def test_diff():
-    a = pd.array(
-        [True, True, False, False, True, None, True, None, False], dtype=""boolean""
-    )
-    result = pd.core.algorithms.diff(a, 1)
-    expected = pd.array(
-        [None, False, True, False, True, None, None, None, None], dtype=""boolean""
-    )
-    tm.assert_extension_array_equal(result, expected)
-
-    s = pd.Series(a)
-    result = s.diff()
-    expected = pd.Series(expected)
-    tm.assert_series_equal(result, expected)
+
+class TestNumericReduce(base.BaseNumericReduceTests):
+    def check_reduce(self, s, op_name, skipna):
+        result = getattr(s, op_name)(skipna=skipna)
+        expected = getattr(s.astype(""float64""), op_name)(skipna=skipna)
+        # override parent function to cast to bool for min/max
+        if np.isnan(expected):
+            expected = pd.NA
+        elif op_name in (""min"", ""max""):
+            expected = bool(expected)
+        tm.assert_almost_equal(result, expected)
+
+
+class TestBooleanReduce(base.BaseBooleanReduceTests):
+    pass
+
+
+class TestPrinting(base.BasePrintingTests):
+    pass
+
+
+class TestUnaryOps(base.BaseUnaryOpsTests):
+    pass
+
+
+# TODO parsing not yet supported
+# class TestParsing(base.BaseParsingTests):
+#     pass
diff --git a/pandas/tests/extension/base/__init__.py b/pandas/tests/extension/base/__init__.py
index 090df35bd..e2b6ea030 100644
--- a/pandas/tests/extension/base/__init__.py
+++ b/pandas/tests/extension/base/__init__.py
@@ -49,7 +49,12 @@ from .interface import BaseInterfaceTests  # noqa
 from .io import BaseParsingTests  # noqa
 from .methods import BaseMethodsTests  # noqa
 from .missing import BaseMissingTests  # noqa
-from .ops import BaseArithmeticOpsTests, BaseComparisonOpsTests, BaseOpsUtil  # noqa
+from .ops import (  # noqa
+    BaseArithmeticOpsTests,
+    BaseComparisonOpsTests,
+    BaseOpsUtil,
+    BaseUnaryOpsTests,
+)
 from .printing import BasePrintingTests  # noqa
 from .reduce import (  # noqa
     BaseBooleanReduceTests,
diff --git a/pandas/tests/extension/base/ops.py b/pandas/tests/extension/base/ops.py
index 20d06ef2e..0609f19c8 100644
--- a/pandas/tests/extension/base/ops.py
+++ b/pandas/tests/extension/base/ops.py
@@ -168,3 +168,11 @@ class BaseComparisonOpsTests(BaseOpsUtil):
             assert result is NotImplemented
         else:
             raise pytest.skip(f""{type(data).__name__} does not implement __eq__"")
+
+
+class BaseUnaryOpsTests(BaseOpsUtil):
+    def test_invert(self, data):
+        s = pd.Series(data, name=""name"")
+        result = ~s
+        expected = pd.Series(~data, name=""name"")
+        self.assert_series_equal(result, expected)
diff --git a/pandas/tests/frame/test_operators.py b/pandas/tests/frame/test_operators.py
index c727cb398..55f1216a0 100644
--- a/pandas/tests/frame/test_operators.py
+++ b/pandas/tests/frame/test_operators.py
@@ -61,6 +61,27 @@ class TestDataFrameUnaryOperators:
 
         tm.assert_frame_equal(-(df < 0), ~(df < 0))
 
+    def test_invert_mixed(self):
+        shape = (10, 5)
+        df = pd.concat(
+            [
+                pd.DataFrame(np.zeros(shape, dtype=""bool"")),
+                pd.DataFrame(np.zeros(shape, dtype=int)),
+            ],
+            axis=1,
+            ignore_index=True,
+        )
+        result = ~df
+        expected = pd.concat(
+            [
+                pd.DataFrame(np.ones(shape, dtype=""bool"")),
+                pd.DataFrame(-np.ones(shape, dtype=int)),
+            ],
+            axis=1,
+            ignore_index=True,
+        )
+        tm.assert_frame_equal(result, expected)
+
     @pytest.mark.parametrize(
         ""df"",
         [
"
"pandas","144","b106108","ffe6cfdbf82d663c3f77567bde11f1666de1df38","pandas/plotting/_matplotlib/core.py","pandas/plotting/_matplotlib/core.py","diff --git a/pandas/plotting/_matplotlib/core.py b/pandas/plotting/_matplotlib/core.py","pandas/tests/plotting/test_series.py","","diff --git a/pandas/plotting/_matplotlib/core.py b/pandas/plotting/_matplotlib/core.py
index 3c9256e62..82c5ba7f0 100644
--- a/pandas/plotting/_matplotlib/core.py
+++ b/pandas/plotting/_matplotlib/core.py
@@ -1435,8 +1435,13 @@ class BarPlot(MPLPlot):
 
     def _decorate_ticks(self, ax, name, ticklabels, start_edge, end_edge):
         ax.set_xlim((start_edge, end_edge))
-        ax.set_xticks(self.tick_pos)
-        ax.set_xticklabels(ticklabels)
+
+        if self.xticks is not None:
+            ax.set_xticks(np.array(self.xticks))
+        else:
+            ax.set_xticks(self.tick_pos)
+            ax.set_xticklabels(ticklabels)
+
         if name is not None and self.use_index:
             ax.set_xlabel(name)
 
"
"pandas","3","45fee32","d3a6a3a58e1a6eb68b8b8399ff252b8f4501950e","pandas/core/series.py","pandas/core/series.py","diff --git a/pandas/core/series.py b/pandas/core/series.py","pandas/tests/series/methods/test_to_period.py;pandas/tests/series/methods/test_to_timestamp.py","","diff --git a/pandas/core/series.py b/pandas/core/series.py
index 4ba9a0c92..e107b66d3 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -4684,7 +4684,8 @@ Name: Max Speed, dtype: float64
         if copy:
             new_values = new_values.copy()
 
-        assert isinstance(self.index, PeriodIndex)
+        if not isinstance(self.index, PeriodIndex):
+            raise TypeError(f""unsupported Type {type(self.index).__name__}"")
         new_index = self.index.to_timestamp(freq=freq, how=how)  # type: ignore
         return self._constructor(new_values, index=new_index).__finalize__(
             self, method=""to_timestamp""
@@ -4711,7 +4712,8 @@ Name: Max Speed, dtype: float64
         if copy:
             new_values = new_values.copy()
 
-        assert isinstance(self.index, DatetimeIndex)
+        if not isinstance(self.index, DatetimeIndex):
+            raise TypeError(f""unsupported Type {type(self.index).__name__}"")
         new_index = self.index.to_period(freq=freq)  # type: ignore
         return self._constructor(new_values, index=new_index).__finalize__(
             self, method=""to_period""
"
"pandas","84","469b4b7","24d7c06130f9c2aeebedc26971b244ce076f7d0a","pandas/core/reshape/reshape.py;pandas/tests/series/test_analytics.py","pandas/core/reshape/reshape.py;pandas/tests/series/test_analytics.py","diff --git a/pandas/core/reshape/reshape.py b/pandas/core/reshape/reshape.py;diff --git a/pandas/tests/series/test_analytics.py b/pandas/tests/series/test_analytics.py","pandas/tests/frame/test_reshape.py","","diff --git a/pandas/core/reshape/reshape.py b/pandas/core/reshape/reshape.py
index 97f416e32..fab9f41cb 100644
--- a/pandas/core/reshape/reshape.py
+++ b/pandas/core/reshape/reshape.py
@@ -317,6 +317,10 @@ def _unstack_multiple(data, clocs, fill_value=None):
 
     index = data.index
 
+    # GH 19966 Make sure if MultiIndexed index has tuple name, they will be
+    # recognised as a whole
+    if clocs in index.names:
+        clocs = [clocs]
     clocs = [index._get_level_number(i) for i in clocs]
 
     rlocs = [i for i in range(index.nlevels) if i not in clocs]
diff --git a/pandas/tests/series/test_analytics.py b/pandas/tests/series/test_analytics.py
index c29bd3ea0..e6e91b5d4 100644
--- a/pandas/tests/series/test_analytics.py
+++ b/pandas/tests/series/test_analytics.py
@@ -6,7 +6,7 @@ import pytest
 import pandas.util._test_decorators as td
 
 import pandas as pd
-from pandas import DataFrame, MultiIndex, Series
+from pandas import DataFrame, Series
 import pandas._testing as tm
 
 
@@ -160,65 +160,6 @@ class TestSeriesAnalytics:
         assert s.is_monotonic is False
         assert s.is_monotonic_decreasing is True
 
-    def test_unstack(self):
-
-        index = MultiIndex(
-            levels=[[""bar"", ""foo""], [""one"", ""three"", ""two""]],
-            codes=[[1, 1, 0, 0], [0, 1, 0, 2]],
-        )
-
-        s = Series(np.arange(4.0), index=index)
-        unstacked = s.unstack()
-
-        expected = DataFrame(
-            [[2.0, np.nan, 3.0], [0.0, 1.0, np.nan]],
-            index=[""bar"", ""foo""],
-            columns=[""one"", ""three"", ""two""],
-        )
-
-        tm.assert_frame_equal(unstacked, expected)
-
-        unstacked = s.unstack(level=0)
-        tm.assert_frame_equal(unstacked, expected.T)
-
-        index = MultiIndex(
-            levels=[[""bar""], [""one"", ""two"", ""three""], [0, 1]],
-            codes=[[0, 0, 0, 0, 0, 0], [0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],
-        )
-        s = Series(np.random.randn(6), index=index)
-        exp_index = MultiIndex(
-            levels=[[""one"", ""two"", ""three""], [0, 1]],
-            codes=[[0, 1, 2, 0, 1, 2], [0, 1, 0, 1, 0, 1]],
-        )
-        expected = DataFrame({""bar"": s.values}, index=exp_index).sort_index(level=0)
-        unstacked = s.unstack(0).sort_index()
-        tm.assert_frame_equal(unstacked, expected)
-
-        # GH5873
-        idx = pd.MultiIndex.from_arrays([[101, 102], [3.5, np.nan]])
-        ts = pd.Series([1, 2], index=idx)
-        left = ts.unstack()
-        right = DataFrame(
-            [[np.nan, 1], [2, np.nan]], index=[101, 102], columns=[np.nan, 3.5]
-        )
-        tm.assert_frame_equal(left, right)
-
-        idx = pd.MultiIndex.from_arrays(
-            [
-                [""cat"", ""cat"", ""cat"", ""dog"", ""dog""],
-                [""a"", ""a"", ""b"", ""a"", ""b""],
-                [1, 2, 1, 1, np.nan],
-            ]
-        )
-        ts = pd.Series([1.0, 1.1, 1.2, 1.3, 1.4], index=idx)
-        right = DataFrame(
-            [[1.0, 1.3], [1.1, np.nan], [np.nan, 1.4], [1.2, np.nan]],
-            columns=[""cat"", ""dog""],
-        )
-        tpls = [(""a"", 1), (""a"", 2), (""b"", np.nan), (""b"", 1)]
-        right.index = pd.MultiIndex.from_tuples(tpls)
-        tm.assert_frame_equal(ts.unstack(level=0), right)
-
     @pytest.mark.parametrize(""func"", [np.any, np.all])
     @pytest.mark.parametrize(""kwargs"", [dict(keepdims=True), dict(out=object())])
     @td.skip_if_np_lt(""1.15"")
"
"pandas","16","f159734","74e8607cb163b76ccf272ac72ae6b7848fe930c8","pandas/core/indexes/datetimelike.py","pandas/core/indexes/datetimelike.py","diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py","pandas/tests/arithmetic/test_period.py","","diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py
index 4ddac12ae..f41044db2 100644
--- a/pandas/core/indexes/datetimelike.py
+++ b/pandas/core/indexes/datetimelike.py
@@ -88,7 +88,7 @@ def _make_wrapped_arith_op_with_freq(opname: str):
         if result is NotImplemented:
             return NotImplemented
 
-        new_freq = self._get_addsub_freq(other)
+        new_freq = self._get_addsub_freq(other, result)
         result._freq = new_freq
         return result
 
@@ -451,14 +451,16 @@ class DatetimeIndexOpsMixin(ExtensionIndex):
     # --------------------------------------------------------------------
     # Arithmetic Methods
 
-    def _get_addsub_freq(self, other) -> Optional[DateOffset]:
+    def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:
         """"""
         Find the freq we expect the result of an addition/subtraction operation
         to have.
         """"""
         if is_period_dtype(self.dtype):
-            # Only used for ops that stay PeriodDtype
-            return self.freq
+            if is_period_dtype(result.dtype):
+                # Only used for ops that stay PeriodDtype
+                return self.freq
+            return None
         elif self.freq is None:
             return None
         elif lib.is_scalar(other) and isna(other):
"
"pandas","117","fc100fe","f98d2b6587b74c9a640b062d94911b199d962119","pandas/core/dtypes/missing.py","pandas/core/dtypes/missing.py","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py","pandas/tests/series/test_analytics.py","","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py
index cb4199272..205ca1936 100644
--- a/pandas/core/dtypes/missing.py
+++ b/pandas/core/dtypes/missing.py
@@ -176,7 +176,7 @@ def _isna_old(obj):
         raise NotImplementedError(""isna is not defined for MultiIndex"")
     elif isinstance(obj, type):
         return False
-    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass)):
+    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
         return _isna_ndarraylike_old(obj)
     elif isinstance(obj, ABCGeneric):
         return obj._constructor(obj._data.isna(func=_isna_old))
"
"pandas","127","f7d6b58","710d82c0d393c9031e469ec0371660d8187b7dc3","pandas/core/generic.py","pandas/core/generic.py","diff --git a/pandas/core/generic.py b/pandas/core/generic.py","pandas/tests/series/test_timeseries.py","","diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 4e024bba3..17784b623 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -10443,6 +10443,7 @@ class NDFrame(PandasObject, SelectionMixin):
             data = self.fillna(method=fill_method, limit=limit, axis=axis)
 
         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1
+        rs = rs.loc[~rs.index.duplicated()]
         rs = rs.reindex_like(data)
         if freq is None:
             mask = isna(com.values_from_object(data))
"
"pandas","126","29be383","e639af2afd18b90ab9063df9c1927ae1f357a418","pandas/core/frame.py","pandas/core/frame.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py","pandas/tests/frame/test_combine_concat.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 9c9189e5f..442994a04 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -6943,10 +6943,13 @@ class DataFrame(NDFrame):
             other = other._convert(datetime=True, timedelta=True)
             if not self.columns.equals(combined_columns):
                 self = self.reindex(columns=combined_columns)
-        elif isinstance(other, list) and not isinstance(other[0], DataFrame):
-            other = DataFrame(other)
-            if (self.columns.get_indexer(other.columns) >= 0).all():
-                other = other.reindex(columns=self.columns)
+        elif isinstance(other, list):
+            if not other:
+                pass
+            elif not isinstance(other[0], DataFrame):
+                other = DataFrame(other)
+                if (self.columns.get_indexer(other.columns) >= 0).all():
+                    other = other.reindex(columns=self.columns)
 
         from pandas.core.reshape.concat import concat
 
"
"pandas","2","2740fb4","55e8891f6d33be14e0db73ac06513129503f995c","pandas/core/indexing.py","pandas/core/indexing.py","diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py","pandas/tests/indexing/test_scalar.py","","diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index b857a5919..3a146bb04 100644
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -2016,10 +2016,10 @@ class _ScalarAccessIndexer(_NDFrameIndexerBase):
 
         if not isinstance(key, tuple):
             key = _tuplify(self.ndim, key)
+        key = list(self._convert_key(key, is_setter=True))
         if len(key) != self.ndim:
             raise ValueError(""Not enough indexers for scalar access (setting)!"")
 
-        key = list(self._convert_key(key, is_setter=True))
         self.obj._set_value(*key, value=value, takeable=self._takeable)
 
 
@@ -2032,6 +2032,12 @@ class _AtIndexer(_ScalarAccessIndexer):
         Require they keys to be the same type as the index. (so we don't
         fallback)
         """"""
+        # GH 26989
+        # For series, unpacking key needs to result in the label.
+        # This is already the case for len(key) == 1; e.g. (1,)
+        if self.ndim == 1 and len(key) > 1:
+            key = (key,)
+
         # allow arbitrary setting
         if is_setter:
             return list(key)
"
"pandas","38","c81d90f","e7ee418fa7a519225203fef23481c5fa35834dc3","pandas/core/reshape/reshape.py","pandas/core/reshape/reshape.py","diff --git a/pandas/core/reshape/reshape.py b/pandas/core/reshape/reshape.py","pandas/tests/frame/test_reshape.py","","diff --git a/pandas/core/reshape/reshape.py b/pandas/core/reshape/reshape.py
index 14c2a05e5..88e61d239 100644
--- a/pandas/core/reshape/reshape.py
+++ b/pandas/core/reshape/reshape.py
@@ -338,7 +338,7 @@ def _unstack_multiple(data, clocs, fill_value=None):
     comp_ids, obs_ids = compress_group_index(group_index, sort=False)
     recons_codes = decons_obs_group_ids(comp_ids, obs_ids, shape, ccodes, xnull=False)
 
-    if rlocs == []:
+    if not rlocs:
         # Everything is in clocs, so the dummy df has a regular index
         dummy_index = Index(obs_ids, name=""__placeholder__"")
     else:
@@ -363,7 +363,7 @@ def _unstack_multiple(data, clocs, fill_value=None):
             for i in range(len(clocs)):
                 val = clocs[i]
                 result = result.unstack(val, fill_value=fill_value)
-                clocs = [v if i > v else v - 1 for v in clocs]
+                clocs = [v if v < val else v - 1 for v in clocs]
 
             return result
 
"
"pandas","8","ddbeca6","d09f20e29bdfa82f5efc071986e2633001d552f6","pandas/core/internals/blocks.py","pandas/core/internals/blocks.py","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py","pandas/tests/frame/methods/test_replace.py","","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index ff2b63858..bfde82d6f 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -728,11 +728,6 @@ class Block(PandasObject):
 
         mask = missing.mask_missing(values, to_replace)
 
-        if not mask.any():
-            if inplace:
-                return [self]
-            return [self.copy()]
-
         try:
             blocks = self.putmask(mask, value, inplace=inplace)
             # Note: it is _not_ the case that self._can_hold_element(value)
"
"pandas","23","ab9f3c9","38b669a206b151e0a2bb985200d4a493c4ac078f","pandas/core/indexes/datetimelike.py;pandas/tests/indexes/datetimes/test_indexing.py","pandas/core/indexes/datetimelike.py;pandas/tests/indexes/datetimes/test_indexing.py","diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py;diff --git a/pandas/tests/indexes/datetimes/test_indexing.py b/pandas/tests/indexes/datetimes/test_indexing.py","pandas/tests/indexes/datetimes/test_setops.py","","diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py
index b83b64c14..53205d3b4 100644
--- a/pandas/core/indexes/datetimelike.py
+++ b/pandas/core/indexes/datetimelike.py
@@ -724,10 +724,10 @@ class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):
         start = right[0]
 
         if end < start:
-            return type(self)(data=[])
+            return type(self)(data=[], dtype=self.dtype, freq=self.freq)
         else:
             lslice = slice(*left.slice_locs(start, end))
-            left_chunk = left.values[lslice]
+            left_chunk = left._values[lslice]
             return self._shallow_copy(left_chunk)
 
     def _can_fast_union(self, other) -> bool:
diff --git a/pandas/tests/indexes/datetimes/test_indexing.py b/pandas/tests/indexes/datetimes/test_indexing.py
index ff15cded1..08b8e7102 100644
--- a/pandas/tests/indexes/datetimes/test_indexing.py
+++ b/pandas/tests/indexes/datetimes/test_indexing.py
@@ -75,8 +75,9 @@ class TestGetItem:
     def test_dti_business_getitem(self):
         rng = pd.bdate_range(START, END)
         smaller = rng[:5]
-        exp = DatetimeIndex(rng.view(np.ndarray)[:5])
+        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=""B"")
         tm.assert_index_equal(smaller, exp)
+        assert smaller.freq == exp.freq
 
         assert smaller.freq == rng.freq
 
@@ -102,8 +103,9 @@ class TestGetItem:
     def test_dti_custom_getitem(self):
         rng = pd.bdate_range(START, END, freq=""C"")
         smaller = rng[:5]
-        exp = DatetimeIndex(rng.view(np.ndarray)[:5])
+        exp = DatetimeIndex(rng.view(np.ndarray)[:5], freq=""C"")
         tm.assert_index_equal(smaller, exp)
+        assert smaller.freq == exp.freq
         assert smaller.freq == rng.freq
 
         sliced = rng[::5]
"
"pandas","97","f9e524c","6f690b088190581552e04c53288819472fdb2dbe","pandas/core/indexes/timedeltas.py","pandas/core/indexes/timedeltas.py","diff --git a/pandas/core/indexes/timedeltas.py b/pandas/core/indexes/timedeltas.py","pandas/tests/indexes/timedeltas/test_setops.py","","diff --git a/pandas/core/indexes/timedeltas.py b/pandas/core/indexes/timedeltas.py
index 90967ffd0..6a68cb102 100644
--- a/pandas/core/indexes/timedeltas.py
+++ b/pandas/core/indexes/timedeltas.py
@@ -257,7 +257,7 @@ class TimedeltaIndex(
         this, other = self, other
 
         if this._can_fast_union(other):
-            return this._fast_union(other)
+            return this._fast_union(other, sort=sort)
         else:
             result = Index._union(this, other, sort=sort)
             if isinstance(result, TimedeltaIndex):
@@ -265,7 +265,7 @@ class TimedeltaIndex(
                     result._set_freq(""infer"")
             return result
 
-    def _fast_union(self, other):
+    def _fast_union(self, other, sort=None):
         if len(other) == 0:
             return self.view(type(self))
 
@@ -275,6 +275,15 @@ class TimedeltaIndex(
         # to make our life easier, ""sort"" the two ranges
         if self[0] <= other[0]:
             left, right = self, other
+        elif sort is False:
+            # TDIs are not in the ""correct"" order and we don't want
+            #  to sort but want to remove overlaps
+            left, right = self, other
+            left_start = left[0]
+            loc = right.searchsorted(left_start, side=""left"")
+            right_chunk = right.values[:loc]
+            dates = concat_compat((left.values, right_chunk))
+            return self._shallow_copy(dates)
         else:
             left, right = other, self
 
"
"pandas","89","0dc317f","feaa5033b7810f7775fd4806c27b2f9f1e9b5051","pandas/core/reshape/reshape.py","pandas/core/reshape/reshape.py","diff --git a/pandas/core/reshape/reshape.py b/pandas/core/reshape/reshape.py","pandas/tests/frame/test_reshape.py","","diff --git a/pandas/core/reshape/reshape.py b/pandas/core/reshape/reshape.py
index da92e1154..97f416e32 100644
--- a/pandas/core/reshape/reshape.py
+++ b/pandas/core/reshape/reshape.py
@@ -358,7 +358,7 @@ def _unstack_multiple(data, clocs, fill_value=None):
             result = data
             for i in range(len(clocs)):
                 val = clocs[i]
-                result = result.unstack(val)
+                result = result.unstack(val, fill_value=fill_value)
                 clocs = [v if i > v else v - 1 for v in clocs]
 
             return result
"
"pandas","129","5b580fb","82c9547ddcaf2fd70e00f1368731f14a03bbac88","pandas/core/arrays/datetimelike.py","pandas/core/arrays/datetimelike.py","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py","pandas/tests/arithmetic/test_timedelta64.py","","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py
index f93db4695..497f33f0f 100644
--- a/pandas/core/arrays/datetimelike.py
+++ b/pandas/core/arrays/datetimelike.py
@@ -1303,6 +1303,9 @@ class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray)
         if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):
             # ndarray[datetime64] cannot be subtracted from self, so
             # we need to wrap in DatetimeArray/Index and flip the operation
+            if lib.is_scalar(other):
+                # i.e. np.datetime64 object
+                return Timestamp(other) - self
             if not isinstance(other, DatetimeLikeArrayMixin):
                 # Avoid down-casting DatetimeIndex
                 from pandas.core.arrays import DatetimeArray
"
"pandas","1","3fd150c","e41ee47a90bb1d8a1fa28fcefcd45ed8ef5cb946","pandas/core/dtypes/common.py","pandas/core/dtypes/common.py","diff --git a/pandas/core/dtypes/common.py b/pandas/core/dtypes/common.py","pandas/tests/dtypes/test_dtypes.py","","diff --git a/pandas/core/dtypes/common.py b/pandas/core/dtypes/common.py
index 5b20b8e1b..a4a5ae1bf 100644
--- a/pandas/core/dtypes/common.py
+++ b/pandas/core/dtypes/common.py
@@ -599,7 +599,7 @@ def is_string_dtype(arr_or_dtype) -> bool:
         """"""
         These have kind = ""O"" but aren't string dtypes so need to be explicitly excluded
         """"""
-        is_excluded_checks = (is_period_dtype, is_interval_dtype)
+        is_excluded_checks = (is_period_dtype, is_interval_dtype, is_categorical_dtype)
         return any(is_excluded(dtype) for is_excluded in is_excluded_checks)
 
     return _is_dtype(arr_or_dtype, condition)
"
"pandas","61","74dad82","f7e2b74f1bcc1d1cbebbc42481e33f0abb2843dc","pandas/core/series.py","pandas/core/series.py","diff --git a/pandas/core/series.py b/pandas/core/series.py","pandas/tests/indexing/test_indexing.py","","diff --git a/pandas/core/series.py b/pandas/core/series.py
index c54331f86..94818ab8d 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -919,7 +919,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
                 indexer = self.index.get_indexer_for(key)
                 return self.iloc[indexer]
             else:
-                return self._get_values(key)
+                return self.iloc[key]
 
         if isinstance(key, (list, tuple)):
             # TODO: de-dup with tuple case handled above?
"
"pandas","128","794a1c2","112e6b8d054f9adc1303138533ed6506975f94db","pandas/io/json/_json.py","pandas/io/json/_json.py","diff --git a/pandas/io/json/_json.py b/pandas/io/json/_json.py","pandas/tests/io/json/test_readlines.py","","diff --git a/pandas/io/json/_json.py b/pandas/io/json/_json.py
index 861535599..0a8f275cf 100644
--- a/pandas/io/json/_json.py
+++ b/pandas/io/json/_json.py
@@ -577,6 +577,8 @@ def read_json(
         dtype = True
     if convert_axes is None and orient != ""table"":
         convert_axes = True
+    if encoding is None:
+        encoding = ""utf-8""
 
     compression = _infer_compression(path_or_buf, compression)
     filepath_or_buffer, _, compression, should_close = get_filepath_or_buffer(
"
"pandas","149","0d69d91","fa1364d1299a53093bc704f9c34c595b602a568b","pandas/io/parquet.py","pandas/io/parquet.py","diff --git a/pandas/io/parquet.py b/pandas/io/parquet.py","pandas/tests/io/test_gcs.py","","diff --git a/pandas/io/parquet.py b/pandas/io/parquet.py
index 3612099d9..69ee6583d 100644
--- a/pandas/io/parquet.py
+++ b/pandas/io/parquet.py
@@ -7,7 +7,7 @@ from pandas.errors import AbstractMethodError
 
 from pandas import DataFrame, get_option
 
-from pandas.io.common import get_filepath_or_buffer, is_s3_url
+from pandas.io.common import get_filepath_or_buffer, is_gcs_url, is_s3_url
 
 
 def get_engine(engine):
@@ -159,12 +159,12 @@ class FastParquetImpl(BaseImpl):
         if partition_cols is not None:
             kwargs[""file_scheme""] = ""hive""
 
-        if is_s3_url(path):
-            # path is s3:// so we need to open the s3file in 'wb' mode.
+        if is_s3_url(path) or is_gcs_url(path):
+            # if path is s3:// or gs:// we need to open the file in 'wb' mode.
             # TODO: Support 'ab'
 
             path, _, _, _ = get_filepath_or_buffer(path, mode=""wb"")
-            # And pass the opened s3file to the fastparquet internal impl.
+            # And pass the opened file to the fastparquet internal impl.
             kwargs[""open_with""] = lambda path, _: path
         else:
             path, _, _, _ = get_filepath_or_buffer(path)
"
"pandas","68","01582c4","d28db65bdba16e9400a16469ba2707f94ae63483","pandas/core/arrays/interval.py;pandas/tests/extension/base/methods.py","pandas/core/arrays/interval.py;pandas/tests/extension/base/methods.py","diff --git a/pandas/core/arrays/interval.py b/pandas/core/arrays/interval.py;diff --git a/pandas/tests/extension/base/methods.py b/pandas/tests/extension/base/methods.py","pandas/tests/arrays/interval/test_interval.py","","diff --git a/pandas/core/arrays/interval.py b/pandas/core/arrays/interval.py
index 398ed75c0..0b35a031b 100644
--- a/pandas/core/arrays/interval.py
+++ b/pandas/core/arrays/interval.py
@@ -27,6 +27,7 @@ from pandas.core.dtypes.common import (
 from pandas.core.dtypes.dtypes import IntervalDtype
 from pandas.core.dtypes.generic import (
     ABCDatetimeIndex,
+    ABCExtensionArray,
     ABCIndexClass,
     ABCInterval,
     ABCIntervalIndex,
@@ -789,6 +790,33 @@ class IntervalArray(IntervalMixin, ExtensionArray):
         # Avoid materializing self.values
         return self.left.size
 
+    def shift(self, periods: int = 1, fill_value: object = None) -> ABCExtensionArray:
+        if not len(self) or periods == 0:
+            return self.copy()
+
+        if isna(fill_value):
+            fill_value = self.dtype.na_value
+
+        # ExtensionArray.shift doesn't work for two reasons
+        # 1. IntervalArray.dtype.na_value may not be correct for the dtype.
+        # 2. IntervalArray._from_sequence only accepts NaN for missing values,
+        #    not other values like NaT
+
+        empty_len = min(abs(periods), len(self))
+        if isna(fill_value):
+            fill_value = self.left._na_value
+            empty = IntervalArray.from_breaks([fill_value] * (empty_len + 1))
+        else:
+            empty = self._from_sequence([fill_value] * empty_len)
+
+        if periods > 0:
+            a = empty
+            b = self[:-periods]
+        else:
+            a = self[abs(periods) :]
+            b = empty
+        return self._concat_same_type([a, b])
+
     def take(self, indices, allow_fill=False, fill_value=None, axis=None, **kwargs):
         """"""
         Take elements from the IntervalArray.
diff --git a/pandas/tests/extension/base/methods.py b/pandas/tests/extension/base/methods.py
index 4a84a2108..22e53dbc8 100644
--- a/pandas/tests/extension/base/methods.py
+++ b/pandas/tests/extension/base/methods.py
@@ -280,6 +280,13 @@ class BaseMethodsTests(BaseExtensionTests):
         expected = empty
         self.assert_extension_array_equal(result, expected)
 
+    def test_shift_zero_copies(self, data):
+        result = data.shift(0)
+        assert result is not data
+
+        result = data[:0].shift(2)
+        assert result is not data
+
     def test_shift_fill_value(self, data):
         arr = data[:4]
         fill_value = data[0]
"
"pandas","93","425c2fb","bde25278ccf4fb2d751c5e99e24b2270e0d62ef7","pandas/core/arrays/datetimelike.py","pandas/core/arrays/datetimelike.py","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py","pandas/tests/indexes/period/test_indexing.py;","","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py
index 0fadf3a05..a10e0f63b 100644
--- a/pandas/core/arrays/datetimelike.py
+++ b/pandas/core/arrays/datetimelike.py
@@ -1,1724 +1,961 @@
-from datetime import datetime, timedelta
+""""""
+Base and utility classes for tseries type pandas objects.
+""""""
 import operator
-from typing import Any, Sequence, Type, Union, cast
-import warnings
+from typing import List, Optional, Set
 
 import numpy as np
 
-from pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib
-from pandas._libs.tslibs.c_timestamp import integer_op_not_supported
-from pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period
-from pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds
-from pandas._libs.tslibs.timestamps import RoundTo, round_nsint64
-from pandas._typing import DatetimeLikeScalar
-from pandas.compat import set_function_name
+from pandas._libs import NaT, iNaT, join as libjoin, lib
+from pandas._libs.algos import unique_deltas
+from pandas._libs.tslibs import timezones
 from pandas.compat.numpy import function as nv
-from pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning
-from pandas.util._decorators import Appender, Substitution
-from pandas.util._validators import validate_fillna_kwargs
+from pandas.errors import AbstractMethodError
+from pandas.util._decorators import Appender, cache_readonly
 
 from pandas.core.dtypes.common import (
+    ensure_int64,
+    is_bool_dtype,
     is_categorical_dtype,
-    is_datetime64_any_dtype,
-    is_datetime64_dtype,
-    is_datetime64tz_dtype,
-    is_datetime_or_timedelta_dtype,
     is_dtype_equal,
-    is_float_dtype,
-    is_integer_dtype,
+    is_float,
+    is_integer,
     is_list_like,
-    is_object_dtype,
     is_period_dtype,
-    is_string_dtype,
-    is_timedelta64_dtype,
-    is_unsigned_integer_dtype,
-    pandas_dtype,
+    is_scalar,
+    needs_i8_conversion,
+)
+from pandas.core.dtypes.concat import concat_compat
+from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
+from pandas.core.dtypes.missing import isna
+
+from pandas.core import algorithms
+from pandas.core.accessor import PandasDelegate
+from pandas.core.arrays import (
+    DatetimeArray,
+    ExtensionArray,
+    ExtensionOpsMixin,
+    TimedeltaArray,
+)
+from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin
+import pandas.core.indexes.base as ibase
+from pandas.core.indexes.base import Index, _index_shared_docs
+from pandas.core.indexes.numeric import Int64Index
+from pandas.core.ops import get_op_result_name
+from pandas.core.tools.timedeltas import to_timedelta
+
+from pandas.tseries.frequencies import DateOffset, to_offset
+
+from .extension import (
+    ExtensionIndex,
+    inherit_names,
+    make_wrapped_arith_op,
+    make_wrapped_comparison_op,
 )
-from pandas.core.dtypes.generic import ABCIndexClass, ABCPeriodArray, ABCSeries
-from pandas.core.dtypes.inference import is_array_like
-from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna
-
-from pandas.core import missing, nanops, ops
-from pandas.core.algorithms import checked_add_with_arr, take, unique1d, value_counts
-import pandas.core.common as com
-from pandas.core.indexers import check_bool_array_indexer
-from pandas.core.ops.common import unpack_zerodim_and_defer
-from pandas.core.ops.invalid import invalid_comparison, make_invalid_op
-
-from pandas.tseries import frequencies
-from pandas.tseries.offsets import DateOffset, Tick
 
-from .base import ExtensionArray, ExtensionOpsMixin
+_index_doc_kwargs = dict(ibase._index_doc_kwargs)
 
 
-def _datetimelike_array_cmp(cls, op):
+def _join_i8_wrapper(joinf, with_indexers: bool = True):
     """"""
-    Wrap comparison operations to convert Timestamp/Timedelta/Period-like to
-    boxed scalars/arrays.
+    Create the join wrapper methods.
     """"""
-    opname = f""__{op.__name__}__""
-    nat_result = opname == ""__ne__""
 
-    @unpack_zerodim_and_defer(opname)
-    def wrapper(self, other):
-
-        if isinstance(other, str):
-            try:
-                # GH#18435 strings get a pass from tzawareness compat
-                other = self._scalar_from_string(other)
-            except ValueError:
-                # failed to parse as Timestamp/Timedelta/Period
-                return invalid_comparison(self, other, op)
+    @staticmethod  # type: ignore
+    def wrapper(left, right):
+        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
+            left = left.view(""i8"")
+        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
+            right = right.view(""i8"")
 
-        if isinstance(other, self._recognized_scalars) or other is NaT:
-            other = self._scalar_type(other)
-            self._check_compatible_with(other)
+        results = joinf(left, right)
+        if with_indexers:
+            # dtype should be timedelta64[ns] for TimedeltaIndex
+            #  and datetime64[ns] for DatetimeIndex
+            dtype = left.dtype.base
 
-            other_i8 = self._unbox_scalar(other)
+            join_index, left_indexer, right_indexer = results
+            join_index = join_index.view(dtype)
+            return join_index, left_indexer, right_indexer
+        return results
 
-            result = op(self.view(""i8""), other_i8)
-            if isna(other):
-                result.fill(nat_result)
-
-        elif not is_list_like(other):
-            return invalid_comparison(self, other, op)
-
-        elif len(other) != len(self):
-            raise ValueError(""Lengths must match"")
-
-        else:
-            if isinstance(other, list):
-                # TODO: could use pd.Index to do inference?
-                other = np.array(other)
-
-            if not isinstance(other, (np.ndarray, type(self))):
-                return invalid_comparison(self, other, op)
-
-            if is_object_dtype(other):
-                # We have to use comp_method_OBJECT_ARRAY instead of numpy
-                #  comparison otherwise it would fail to raise when
-                #  comparing tz-aware and tz-naive
-                with np.errstate(all=""ignore""):
-                    result = ops.comp_method_OBJECT_ARRAY(
-                        op, self.astype(object), other
-                    )
-                o_mask = isna(other)
-
-            elif not type(self)._is_recognized_dtype(other.dtype):
-                return invalid_comparison(self, other, op)
-
-            else:
-                # For PeriodDType this casting is unnecessary
-                other = type(self)._from_sequence(other)
-                self._check_compatible_with(other)
+    return wrapper
 
-                result = op(self.view(""i8""), other.view(""i8""))
-                o_mask = other._isnan
-
-            if o_mask.any():
-                result[o_mask] = nat_result
-
-        if self._hasnans:
-            result[self._isnan] = nat_result
-
-        return result
-
-    return set_function_name(wrapper, opname, cls)
 
+@inherit_names(
+    [""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
+    DatetimeLikeArrayMixin,
+    cache=True,
+)
+@inherit_names(
+    [""__iter__"", ""mean"", ""freq"", ""freqstr"", ""_ndarray_values"", ""asi8"", ""_box_values""],
+    DatetimeLikeArrayMixin,
+)
+class DatetimeIndexOpsMixin(ExtensionIndex, ExtensionOpsMixin):
+    """"""
+    Common ops mixin to support a unified interface datetimelike Index.
+    """"""
 
-class AttributesMixin:
-    _data: np.ndarray
+    _data: ExtensionArray
+    freq: Optional[DateOffset]
+    freqstr: Optional[str]
+    _resolution: int
+    _bool_ops: List[str] = []
+    _field_ops: List[str] = []
 
-    @classmethod
-    def _simple_new(cls, values, **kwargs):
-        raise AbstractMethodError(cls)
+    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
+    _hasnans = hasnans  # for index / array -agnostic code
 
     @property
-    def _scalar_type(self) -> Type[DatetimeLikeScalar]:
-        """"""The scalar associated with this datelike
+    def is_all_dates(self) -> bool:
+        return True
 
-        * PeriodArray : Period
-        * DatetimeArray : Timestamp
-        * TimedeltaArray : Timedelta
+    @classmethod
+    def _create_comparison_method(cls, op):
         """"""
-        raise AbstractMethodError(self)
-
-    def _scalar_from_string(
-        self, value: str
-    ) -> Union[Period, Timestamp, Timedelta, NaTType]:
+        Create a comparison method that dispatches to ``cls.values``.
         """"""
-        Construct a scalar type from a string.
+        return make_wrapped_comparison_op(f""__{op.__name__}__"")
 
-        Parameters
-        ----------
-        value : str
+    # ------------------------------------------------------------------------
+    # Abstract data attributes
 
-        Returns
-        -------
-        Period, Timestamp, or Timedelta, or NaT
-            Whatever the type of ``self._scalar_type`` is.
+    @property
+    def values(self):
+        # Note: PeriodArray overrides this to return an ndarray of objects.
+        return self._data._data
 
-        Notes
-        -----
-        This should call ``self._check_compatible_with`` before
-        unboxing the result.
+    def __array_wrap__(self, result, context=None):
         """"""
-        raise AbstractMethodError(self)
-
-    def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:
+        Gets called after a ufunc.
         """"""
-        Unbox the integer value of a scalar `value`.
+        result = lib.item_from_zerodim(result)
+        if is_bool_dtype(result) or lib.is_scalar(result):
+            return result
 
-        Parameters
-        ----------
-        value : Union[Period, Timestamp, Timedelta]
+        attrs = self._get_attributes_dict()
+        if not is_period_dtype(self) and attrs[""freq""]:
+            # no need to infer if freq is None
+            attrs[""freq""] = ""infer""
+        return Index(result, **attrs)
 
-        Returns
-        -------
-        int
+    # ------------------------------------------------------------------------
 
-        Examples
-        --------
-        >>> self._unbox_scalar(Timedelta('10s'))  # DOCTEST: +SKIP
-        10000000000
+    def equals(self, other) -> bool:
         """"""
-        raise AbstractMethodError(self)
-
-    def _check_compatible_with(
-        self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False
-    ) -> None:
+        Determines if two Index objects contain the same elements.
         """"""
-        Verify that `self` and `other` are compatible.
+        if self.is_(other):
+            return True
 
-        * DatetimeArray verifies that the timezones (if any) match
-        * PeriodArray verifies that the freq matches
-        * Timedelta has no verification
-
-        In each case, NaT is considered compatible.
+        if not isinstance(other, ABCIndexClass):
+            return False
+        elif not isinstance(other, type(self)):
+            try:
+                other = type(self)(other)
+            except (ValueError, TypeError, OverflowError):
+                # e.g.
+                #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime
+                #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex
+                #  OverflowError -> Index([very_large_timedeltas])
+                return False
+
+        if not is_dtype_equal(self.dtype, other.dtype):
+            # have different timezone
+            return False
+
+        return np.array_equal(self.asi8, other.asi8)
+
+    @Appender(_index_shared_docs[""contains""] % _index_doc_kwargs)
+    def __contains__(self, key):
+        try:
+            res = self.get_loc(key)
+            return (
+                is_scalar(res)
+                or isinstance(res, slice)
+                or (is_list_like(res) and len(res))
+            )
+        except (KeyError, TypeError, ValueError):
+            return False
 
-        Parameters
-        ----------
-        other
-        setitem : bool, default False
-            For __setitem__ we may have stricter compatiblity resrictions than
-            for comparisons.
-
-        Raises
-        ------
-        Exception
-        """"""
-        raise AbstractMethodError(self)
+    # Try to run function on index first, and then on elements of index
+    # Especially important for group-by functionality
+    def map(self, mapper, na_action=None):
+        try:
+            result = mapper(self)
 
+            # Try to use this result if we can
+            if isinstance(result, np.ndarray):
+                result = Index(result)
 
-class DatelikeOps:
-    """"""
-    Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.
-    """"""
+            if not isinstance(result, Index):
+                raise TypeError(""The map function must return an Index object"")
+            return result
+        except Exception:
+            return self.astype(object).map(mapper)
 
-    @Substitution(
-        URL=""https://docs.python.org/3/library/datetime.html""
-        ""#strftime-and-strptime-behavior""
-    )
-    def strftime(self, date_format):
+    def sort_values(self, return_indexer=False, ascending=True):
         """"""
-        Convert to Index using specified date_format.
-
-        Return an Index of formatted strings specified by date_format, which
-        supports the same string format as the python standard library. Details
-        of the string format can be found in `python string format
-        doc <%(URL)s>`__.
-
-        Parameters
-        ----------
-        date_format : str
-            Date format string (e.g. ""%%Y-%%m-%%d"").
-
-        Returns
-        -------
-        ndarray
-            NumPy ndarray of formatted strings.
-
-        See Also
-        --------
-        to_datetime : Convert the given argument to datetime.
-        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.
-        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.
-        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.
-
-        Examples
-        --------
-        >>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),
-        ...                     periods=3, freq='s')
-        >>> rng.strftime('%%B %%d, %%Y, %%r')
-        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',
-               'March 10, 2018, 09:00:02 AM'],
-              dtype='object')
+        Return sorted copy of Index.
         """"""
-        result = self._format_native_types(date_format=date_format, na_rep=np.nan)
-        return result.astype(object)
-
-
-class TimelikeOps:
-    """"""
-    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.
-    """"""
-
-    _round_doc = """"""
-        Perform {op} operation on the data to the specified `freq`.
-
-        Parameters
-        ----------
-        freq : str or Offset
-            The frequency level to {op} the index to. Must be a fixed
-            frequency like 'S' (second) not 'ME' (month end). See
-            :ref:`frequency aliases <timeseries.offset_aliases>` for
-            a list of possible `freq` values.
-        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
-            Only relevant for DatetimeIndex:
-
-            - 'infer' will attempt to infer fall dst-transition hours based on
-              order
-            - bool-ndarray where True signifies a DST time, False designates
-              a non-DST time (note that this flag is only applicable for
-              ambiguous times)
-            - 'NaT' will return NaT where there are ambiguous times
-            - 'raise' will raise an AmbiguousTimeError if there are ambiguous
-              times.
+        if return_indexer:
+            _as = self.argsort()
+            if not ascending:
+                _as = _as[::-1]
+            sorted_index = self.take(_as)
+            return sorted_index, _as
+        else:
+            # NB: using asi8 instead of _ndarray_values matters in numpy 1.18
+            #  because the treatment of NaT has been changed to put NaT last
+            #  instead of first.
+            sorted_values = np.sort(self.asi8)
+            attribs = self._get_attributes_dict()
+            freq = attribs[""freq""]
+
+            if freq is not None and not is_period_dtype(self):
+                if freq.n > 0 and not ascending:
+                    freq = freq * -1
+                elif freq.n < 0 and ascending:
+                    freq = freq * -1
+            attribs[""freq""] = freq
+
+            if not ascending:
+                sorted_values = sorted_values[::-1]
+
+            return self._simple_new(sorted_values, **attribs)
+
+    @Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
+    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
+        nv.validate_take(tuple(), kwargs)
+        indices = ensure_int64(indices)
+
+        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
+        if isinstance(maybe_slice, slice):
+            return self[maybe_slice]
+
+        return ExtensionIndex.take(
+            self, indices, axis, allow_fill, fill_value, **kwargs
+        )
 
-            .. versionadded:: 0.24.0
+    _can_hold_na = True
 
-        nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \
-default 'raise'
-            A nonexistent time does not exist in a particular timezone
-            where clocks moved forward due to DST.
+    _na_value = NaT
+    """"""The expected NA value to use with this index.""""""
 
-            - 'shift_forward' will shift the nonexistent time forward to the
-              closest existing time
-            - 'shift_backward' will shift the nonexistent time backward to the
-              closest existing time
-            - 'NaT' will return NaT where there are nonexistent times
-            - timedelta objects will shift nonexistent times by the timedelta
-            - 'raise' will raise an NonExistentTimeError if there are
-              nonexistent times.
+    def _convert_tolerance(self, tolerance, target):
+        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())
 
-            .. versionadded:: 0.24.0
+        if target.size != tolerance.size and tolerance.size > 1:
+            raise ValueError(""list-like tolerance size must match target index size"")
+        return tolerance
 
-        Returns
-        -------
-        DatetimeIndex, TimedeltaIndex, or Series
-            Index of the same type for a DatetimeIndex or TimedeltaIndex,
-            or a Series with the same index for a Series.
+    def tolist(self) -> List:
+        """"""
+        Return a list of the underlying data.
+        """"""
+        return list(self.astype(object))
 
-        Raises
-        ------
-        ValueError if the `freq` cannot be converted.
+    def min(self, axis=None, skipna=True, *args, **kwargs):
+        """"""
+        Return the minimum value of the Index or minimum along
+        an axis.
 
-        Examples
+        See Also
         --------
-        **DatetimeIndex**
-
-        >>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')
-        >>> rng
-        DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:01:00'],
-                      dtype='datetime64[ns]', freq='T')
+        numpy.ndarray.min
+        Series.min : Return the minimum value in a Series.
         """"""
+        nv.validate_min(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    _round_example = """""">>> rng.round('H')
-        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:00:00'],
-                      dtype='datetime64[ns]', freq=None)
+        if not len(self):
+            return self._na_value
 
-        **Series**
+        i8 = self.asi8
+        try:
+            # quick check
+            if len(i8) and self.is_monotonic:
+                if i8[0] != iNaT:
+                    return self._box_func(i8[0])
+
+            if self.hasnans:
+                if skipna:
+                    min_stamp = self[~self._isnan].asi8.min()
+                else:
+                    return self._na_value
+            else:
+                min_stamp = i8.min()
+            return self._box_func(min_stamp)
+        except ValueError:
+            return self._na_value
 
-        >>> pd.Series(rng).dt.round(""H"")
-        0   2018-01-01 12:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 12:00:00
-        dtype: datetime64[ns]
+    def argmin(self, axis=None, skipna=True, *args, **kwargs):
         """"""
+        Returns the indices of the minimum values along an axis.
 
-    _floor_example = """""">>> rng.floor('H')
-        DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:00:00'],
-                      dtype='datetime64[ns]', freq=None)
-
-        **Series**
+        See `numpy.ndarray.argmin` for more information on the
+        `axis` parameter.
 
-        >>> pd.Series(rng).dt.floor(""H"")
-        0   2018-01-01 11:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 12:00:00
-        dtype: datetime64[ns]
+        See Also
+        --------
+        numpy.ndarray.argmin
         """"""
+        nv.validate_argmin(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    _ceil_example = """""">>> rng.ceil('H')
-        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 13:00:00'],
-                      dtype='datetime64[ns]', freq=None)
-
-        **Series**
+        i8 = self.asi8
+        if self.hasnans:
+            mask = self._isnan
+            if mask.all() or not skipna:
+                return -1
+            i8 = i8.copy()
+            i8[mask] = np.iinfo(""int64"").max
+        return i8.argmin()
 
-        >>> pd.Series(rng).dt.ceil(""H"")
-        0   2018-01-01 12:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 13:00:00
-        dtype: datetime64[ns]
+    def max(self, axis=None, skipna=True, *args, **kwargs):
         """"""
+        Return the maximum value of the Index or maximum along
+        an axis.
 
-    def _round(self, freq, mode, ambiguous, nonexistent):
-        # round the local times
-        values = _ensure_datetimelike_to_i8(self)
-        result = round_nsint64(values, mode, freq)
-        result = self._maybe_mask_results(result, fill_value=NaT)
-
-        dtype = self.dtype
-        if is_datetime64tz_dtype(self):
-            dtype = None
-        return self._ensure_localized(
-            self._simple_new(result, dtype=dtype), ambiguous, nonexistent
-        )
-
-    @Appender((_round_doc + _round_example).format(op=""round""))
-    def round(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)
-
-    @Appender((_round_doc + _floor_example).format(op=""floor""))
-    def floor(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)
-
-    @Appender((_round_doc + _ceil_example).format(op=""ceil""))
-    def ceil(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)
+        See Also
+        --------
+        numpy.ndarray.max
+        Series.max : Return the maximum value in a Series.
+        """"""
+        nv.validate_max(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
+        if not len(self):
+            return self._na_value
 
-class DatetimeLikeArrayMixin(ExtensionOpsMixin, AttributesMixin, ExtensionArray):
-    """"""
-    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray
+        i8 = self.asi8
+        try:
+            # quick check
+            if len(i8) and self.is_monotonic:
+                if i8[-1] != iNaT:
+                    return self._box_func(i8[-1])
+
+            if self.hasnans:
+                if skipna:
+                    max_stamp = self[~self._isnan].asi8.max()
+                else:
+                    return self._na_value
+            else:
+                max_stamp = i8.max()
+            return self._box_func(max_stamp)
+        except ValueError:
+            return self._na_value
 
-    Assumes that __new__/__init__ defines:
-        _data
-        _freq
+    def argmax(self, axis=None, skipna=True, *args, **kwargs):
+        """"""
+        Returns the indices of the maximum values along an axis.
 
-    and that the inheriting class has methods:
-        _generate_range
-    """"""
+        See `numpy.ndarray.argmax` for more information on the
+        `axis` parameter.
 
-    @property
-    def ndim(self) -> int:
-        return self._data.ndim
+        See Also
+        --------
+        numpy.ndarray.argmax
+        """"""
+        nv.validate_argmax(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    @property
-    def shape(self):
-        return self._data.shape
+        i8 = self.asi8
+        if self.hasnans:
+            mask = self._isnan
+            if mask.all() or not skipna:
+                return -1
+            i8 = i8.copy()
+            i8[mask] = 0
+        return i8.argmax()
 
-    def reshape(self, *args, **kwargs):
-        # Note: we drop any freq
-        data = self._data.reshape(*args, **kwargs)
-        return type(self)(data, dtype=self.dtype)
+    # --------------------------------------------------------------------
+    # Rendering Methods
 
-    def ravel(self, *args, **kwargs):
-        # Note: we drop any freq
-        data = self._data.ravel(*args, **kwargs)
-        return type(self)(data, dtype=self.dtype)
+    def _format_with_header(self, header, na_rep=""NaT"", **kwargs):
+        return header + list(self._format_native_types(na_rep, **kwargs))
 
     @property
-    def _box_func(self):
-        """"""
-        box function to get object from internal representation
-        """"""
+    def _formatter_func(self):
         raise AbstractMethodError(self)
 
-    def _box_values(self, values):
+    def _format_attrs(self):
         """"""
-        apply box func to passed values
+        Return a list of tuples of the (attr,formatted_value).
         """"""
-        return lib.map_infer(values, self._box_func)
+        attrs = super()._format_attrs()
+        for attrib in self._attributes:
+            if attrib == ""freq"":
+                freq = self.freqstr
+                if freq is not None:
+                    freq = repr(freq)
+                attrs.append((""freq"", freq))
+        return attrs
 
-    def __iter__(self):
-        return (self._box_func(v) for v in self.asi8)
+    # --------------------------------------------------------------------
 
-    @property
-    def asi8(self) -> np.ndarray:
+    def _convert_scalar_indexer(self, key, kind=None):
         """"""
-        Integer representation of the values.
+        We don't allow integer or float indexing on datetime-like when using
+        loc.
 
-        Returns
-        -------
-        ndarray
-            An ndarray with int64 dtype.
-        """"""
-        # do not cache or you'll create a memory leak
-        return self._data.view(""i8"")
-
-    @property
-    def _ndarray_values(self):
-        return self._data
-
-    # ----------------------------------------------------------------
-    # Rendering Methods
+        Parameters
+        ----------
+        key : label of the slice bound
+        kind : {'ix', 'loc', 'getitem', 'iloc'} or None
+        """"""
+
+        assert kind in [""ix"", ""loc"", ""getitem"", ""iloc"", None]
+
+        # we don't allow integer/float indexing for loc
+        # we don't allow float indexing for ix/getitem
+        if is_scalar(key):
+            is_int = is_integer(key)
+            is_flt = is_float(key)
+            if kind in [""loc""] and (is_int or is_flt):
+                self._invalid_indexer(""index"", key)
+            elif kind in [""ix"", ""getitem""] and is_flt:
+                self._invalid_indexer(""index"", key)
+
+        return super()._convert_scalar_indexer(key, kind=kind)
+
+    __add__ = make_wrapped_arith_op(""__add__"")
+    __radd__ = make_wrapped_arith_op(""__radd__"")
+    __sub__ = make_wrapped_arith_op(""__sub__"")
+    __rsub__ = make_wrapped_arith_op(""__rsub__"")
+    __pow__ = make_wrapped_arith_op(""__pow__"")
+    __rpow__ = make_wrapped_arith_op(""__rpow__"")
+    __mul__ = make_wrapped_arith_op(""__mul__"")
+    __rmul__ = make_wrapped_arith_op(""__rmul__"")
+    __floordiv__ = make_wrapped_arith_op(""__floordiv__"")
+    __rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
+    __mod__ = make_wrapped_arith_op(""__mod__"")
+    __rmod__ = make_wrapped_arith_op(""__rmod__"")
+    __divmod__ = make_wrapped_arith_op(""__divmod__"")
+    __rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
+    __truediv__ = make_wrapped_arith_op(""__truediv__"")
+    __rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
+
+    def isin(self, values, level=None):
+        """"""
+        Compute boolean array of whether each index value is found in the
+        passed set of values.
 
-    def _format_native_types(self, na_rep=""NaT"", date_format=None):
-        """"""
-        Helper method for astype when converting to strings.
+        Parameters
+        ----------
+        values : set or sequence of values
 
         Returns
         -------
-        ndarray[str]
+        is_contained : ndarray (boolean dtype)
         """"""
-        raise AbstractMethodError(self)
-
-    def _formatter(self, boxed=False):
-        # TODO: Remove Datetime & DatetimeTZ formatters.
-        return ""'{}'"".format
-
-    # ----------------------------------------------------------------
-    # Array-Like / EA-Interface Methods
-
-    @property
-    def nbytes(self):
-        return self._data.nbytes
-
-    def __array__(self, dtype=None):
-        # used for Timedelta/DatetimeArray, overwritten by PeriodArray
-        if is_object_dtype(dtype):
-            return np.array(list(self), dtype=object)
-        return self._data
-
-    @property
-    def size(self) -> int:
-        """"""The number of elements in this array.""""""
-        return np.prod(self.shape)
+        if level is not None:
+            self._validate_index_level(level)
 
-    def __len__(self) -> int:
-        return len(self._data)
+        if not isinstance(values, type(self)):
+            try:
+                values = type(self)(values)
+            except ValueError:
+                return self.astype(object).isin(values)
 
-    def __getitem__(self, key):
-        """"""
-        This getitem defers to the underlying array, which by-definition can
-        only handle list-likes, slices, and integer scalars
-        """"""
+        return algorithms.isin(self.asi8, values.asi8)
 
-        is_int = lib.is_integer(key)
-        if lib.is_scalar(key) and not is_int:
-            raise IndexError(
-                ""only integers, slices (`:`), ellipsis (`...`), ""
-                ""numpy.newaxis (`None`) and integer or boolean ""
-                ""arrays are valid indices""
-            )
+    @Appender(_index_shared_docs[""repeat""] % _index_doc_kwargs)
+    def repeat(self, repeats, axis=None):
+        nv.validate_repeat(tuple(), dict(axis=axis))
+        result = type(self._data)(self.asi8.repeat(repeats), dtype=self.dtype)
+        return self._shallow_copy(result)
 
-        getitem = self._data.__getitem__
-        if is_int:
-            val = getitem(key)
-            if lib.is_scalar(val):
-                # i.e. self.ndim == 1
-                return self._box_func(val)
-            return type(self)(val, dtype=self.dtype)
-
-        if com.is_bool_indexer(key):
-            key = check_bool_array_indexer(self, key)
-            if key.all():
-                key = slice(0, None, None)
-            else:
-                key = lib.maybe_booleans_to_slice(key.view(np.uint8))
+    @Appender(_index_shared_docs[""where""] % _index_doc_kwargs)
+    def where(self, cond, other=None):
+        values = self.view(""i8"")
 
-        is_period = is_period_dtype(self)
-        if is_period:
-            freq = self.freq
-        else:
-            freq = None
-            if isinstance(key, slice):
-                if self.freq is not None and key.step is not None:
-                    freq = key.step * self.freq
-                else:
-                    freq = self.freq
-            elif key is Ellipsis:
-                # GH#21282 indexing with Ellipsis is similar to a full slice,
-                #  should preserve `freq` attribute
-                freq = self.freq
-
-        result = getitem(key)
-        if result.ndim > 1:
-            # To support MPL which performs slicing with 2 dim
-            # even though it only has 1 dim by definition
-            if is_period:
-                return self._simple_new(result, dtype=self.dtype, freq=freq)
-            return result
+        if is_scalar(other) and isna(other):
+            other = NaT.value
 
-        return self._simple_new(result, dtype=self.dtype, freq=freq)
-
-    def __setitem__(
-        self,
-        key: Union[int, Sequence[int], Sequence[bool], slice],
-        value: Union[NaTType, Any, Sequence[Any]],
-    ) -> None:
-        # I'm fudging the types a bit here. ""Any"" above really depends
-        # on type(self). For PeriodArray, it's Period (or stuff coercible
-        # to a period in from_sequence). For DatetimeArray, it's Timestamp...
-        # I don't know if mypy can do that, possibly with Generics.
-        # https://mypy.readthedocs.io/en/latest/generics.html
-        if lib.is_scalar(value) and not isna(value):
-            value = com.maybe_box_datetimelike(value)
-
-        if is_list_like(value):
-            is_slice = isinstance(key, slice)
-
-            if lib.is_scalar(key):
-                raise ValueError(""setting an array element with a sequence."")
-
-            if not is_slice:
-                key = cast(Sequence, key)
-                if len(key) != len(value) and not com.is_bool_indexer(key):
-                    msg = (
-                        f""shape mismatch: value array of length '{len(key)}' ""
-                        ""does not match indexing result of length ""
-                        f""'{len(value)}'.""
-                    )
-                    raise ValueError(msg)
-                elif not len(key):
-                    return
-
-            value = type(self)._from_sequence(value, dtype=self.dtype)
-            self._check_compatible_with(value, setitem=True)
-            value = value.asi8
-        elif isinstance(value, self._scalar_type):
-            self._check_compatible_with(value, setitem=True)
-            value = self._unbox_scalar(value)
-        elif is_valid_nat_for_dtype(value, self.dtype):
-            value = iNaT
         else:
-            msg = (
-                f""'value' should be a '{self._scalar_type.__name__}', 'NaT', ""
-                f""or array of those. Got '{type(value).__name__}' instead.""
-            )
-            raise TypeError(msg)
-        self._data[key] = value
-        self._maybe_clear_freq()
-
-    def _maybe_clear_freq(self):
-        # inplace operations like __setitem__ may invalidate the freq of
-        # DatetimeArray and TimedeltaArray
-        pass
+            # Do type inference if necessary up front
+            # e.g. we passed PeriodIndex.values and got an ndarray of Periods
+            other = Index(other)
 
-    def astype(self, dtype, copy=True):
-        # Some notes on cases we don't have to handle here in the base class:
-        #   1. PeriodArray.astype handles period -> period
-        #   2. DatetimeArray.astype handles conversion between tz.
-        #   3. DatetimeArray.astype handles datetime -> period
-        from pandas import Categorical
-
-        dtype = pandas_dtype(dtype)
-
-        if is_object_dtype(dtype):
-            return self._box_values(self.asi8)
-        elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):
-            return self._format_native_types()
-        elif is_integer_dtype(dtype):
-            # we deliberately ignore int32 vs. int64 here.
-            # See https://github.com/pandas-dev/pandas/issues/24381 for more.
-            values = self.asi8
-
-            if is_unsigned_integer_dtype(dtype):
-                # Again, we ignore int32 vs. int64
-                values = values.view(""uint64"")
-
-            if copy:
-                values = values.copy()
-            return values
-        elif (
-            is_datetime_or_timedelta_dtype(dtype)
-            and not is_dtype_equal(self.dtype, dtype)
-        ) or is_float_dtype(dtype):
-            # disallow conversion between datetime/timedelta,
-            # and conversions for any datetimelike to float
-            msg = f""Cannot cast {type(self).__name__} to dtype {dtype}""
-            raise TypeError(msg)
-        elif is_categorical_dtype(dtype):
-            return Categorical(self, dtype=dtype)
-        else:
-            return np.asarray(self, dtype=dtype)
+            if is_categorical_dtype(other):
+                # e.g. we have a Categorical holding self.dtype
+                if needs_i8_conversion(other.categories):
+                    other = other._internal_get_values()
 
-    def view(self, dtype=None):
-        if dtype is None or dtype is self.dtype:
-            return type(self)(self._data, dtype=self.dtype)
-        return self._data.view(dtype=dtype)
+            if not is_dtype_equal(self.dtype, other.dtype):
+                raise TypeError(f""Where requires matching dtype, not {other.dtype}"")
 
-    # ------------------------------------------------------------------
-    # ExtensionArray Interface
+            other = other.view(""i8"")
 
-    def unique(self):
-        result = unique1d(self.asi8)
-        return type(self)(result, dtype=self.dtype)
+        result = np.where(cond, values, other).astype(""i8"")
+        return self._shallow_copy(result)
 
-    def _validate_fill_value(self, fill_value):
+    def _summary(self, name=None):
         """"""
-        If a fill_value is passed to `take` convert it to an i8 representation,
-        raising ValueError if this is not possible.
+        Return a summarized representation.
 
         Parameters
         ----------
-        fill_value : object
+        name : str
+            Name to use in the summary representation.
 
         Returns
         -------
-        fill_value : np.int64
-
-        Raises
-        ------
-        ValueError
+        str
+            Summarized representation of the index.
         """"""
-        if isna(fill_value):
-            fill_value = iNaT
-        elif isinstance(fill_value, self._recognized_scalars):
-            self._check_compatible_with(fill_value)
-            fill_value = self._scalar_type(fill_value)
-            fill_value = self._unbox_scalar(fill_value)
+        formatter = self._formatter_func
+        if len(self) > 0:
+            index_summary = f"", {formatter(self[0])} to {formatter(self[-1])}""
         else:
-            raise ValueError(
-                f""'fill_value' should be a {self._scalar_type}. Got '{fill_value}'.""
-            )
-        return fill_value
-
-    def take(self, indices, allow_fill=False, fill_value=None):
-        if allow_fill:
-            fill_value = self._validate_fill_value(fill_value)
+            index_summary = """"
 
-        new_values = take(
-            self.asi8, indices, allow_fill=allow_fill, fill_value=fill_value
-        )
+        if name is None:
+            name = type(self).__name__
+        result = f""{name}: {len(self)} entries{index_summary}""
+        if self.freq:
+            result += f""\nFreq: {self.freqstr}""
 
-        return type(self)(new_values, dtype=self.dtype)
+        # display as values, not quoted
+        result = result.replace(""'"", """")
+        return result
 
-    @classmethod
-    def _concat_same_type(cls, to_concat):
-        dtypes = {x.dtype for x in to_concat}
-        assert len(dtypes) == 1
-        dtype = list(dtypes)[0]
+    def _concat_same_dtype(self, to_concat, name):
+        """"""
+        Concatenate to_concat which has the same class.
+        """"""
+        attribs = self._get_attributes_dict()
+        attribs[""name""] = name
+        # do not pass tz to set because tzlocal cannot be hashed
+        if len({str(x.dtype) for x in to_concat}) != 1:
+            raise ValueError(""to_concat must have the same tz"")
 
-        values = np.concatenate([x.asi8 for x in to_concat])
-        return cls(values, dtype=dtype)
+        new_data = type(self._values)._concat_same_type(to_concat).asi8
 
-    def copy(self):
-        values = self.asi8.copy()
-        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)
+        # GH 3232: If the concat result is evenly spaced, we can retain the
+        # original frequency
+        is_diff_evenly_spaced = len(unique_deltas(new_data)) == 1
+        if not is_period_dtype(self) and not is_diff_evenly_spaced:
+            # reset freq
+            attribs[""freq""] = None
 
-    def _values_for_factorize(self):
-        return self.asi8, iNaT
+        return self._simple_new(new_data, **attribs)
 
-    @classmethod
-    def _from_factorized(cls, values, original):
-        return cls(values, dtype=original.dtype)
+    @Appender(_index_shared_docs[""astype""])
+    def astype(self, dtype, copy=True):
+        if is_dtype_equal(self.dtype, dtype) and copy is False:
+            # Ensure that self.astype(self.dtype) is self
+            return self
 
-    def _values_for_argsort(self):
-        return self._data
+        new_values = self._data.astype(dtype, copy=copy)
 
-    # ------------------------------------------------------------------
-    # Additional array methods
-    #  These are not part of the EA API, but we implement them because
-    #  pandas assumes they're there.
+        # pass copy=False because any copying will be done in the
+        #  _data.astype call above
+        return Index(new_values, dtype=new_values.dtype, name=self.name, copy=False)
 
-    def searchsorted(self, value, side=""left"", sorter=None):
+    def shift(self, periods=1, freq=None):
         """"""
-        Find indices where elements should be inserted to maintain order.
+        Shift index by desired number of time frequency increments.
 
-        Find the indices into a sorted array `self` such that, if the
-        corresponding elements in `value` were inserted before the indices,
-        the order of `self` would be preserved.
+        This method is for shifting the values of datetime-like indexes
+        by a specified time increment a given number of times.
 
         Parameters
         ----------
-        value : array_like
-            Values to insert into `self`.
-        side : {'left', 'right'}, optional
-            If 'left', the index of the first suitable location found is given.
-            If 'right', return the last such index.  If there is no suitable
-            index, return either 0 or N (where N is the length of `self`).
-        sorter : 1-D array_like, optional
-            Optional array of integer indices that sort `self` into ascending
-            order. They are typically the result of ``np.argsort``.
+        periods : int, default 1
+            Number of periods (or increments) to shift by,
+            can be positive or negative.
 
-        Returns
-        -------
-        indices : array of ints
-            Array of insertion points with the same shape as `value`.
-        """"""
-        if isinstance(value, str):
-            value = self._scalar_from_string(value)
+            .. versionchanged:: 0.24.0
 
-        if not (isinstance(value, (self._scalar_type, type(self))) or isna(value)):
-            raise ValueError(f""Unexpected type for 'value': {type(value)}"")
-
-        self._check_compatible_with(value)
-        if isinstance(value, type(self)):
-            value = value.asi8
-        else:
-            value = self._unbox_scalar(value)
-
-        return self.asi8.searchsorted(value, side=side, sorter=sorter)
+        freq : pandas.DateOffset, pandas.Timedelta or string, optional
+            Frequency increment to shift by.
+            If None, the index is shifted by its own `freq` attribute.
+            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.
 
-    def repeat(self, repeats, *args, **kwargs):
-        """"""
-        Repeat elements of an array.
+        Returns
+        -------
+        pandas.DatetimeIndex
+            Shifted index.
 
         See Also
         --------
-        numpy.ndarray.repeat
-        """"""
-        nv.validate_repeat(args, kwargs)
-        values = self._data.repeat(repeats)
-        return type(self)(values.view(""i8""), dtype=self.dtype)
-
-    def value_counts(self, dropna=False):
+        Index.shift : Shift values of Index.
+        PeriodIndex.shift : Shift values of PeriodIndex.
         """"""
-        Return a Series containing counts of unique values.
+        result = self._data._time_shift(periods, freq=freq)
+        return type(self)(result, name=self.name)
 
-        Parameters
-        ----------
-        dropna : bool, default True
-            Don't include counts of NaT values.
+    # --------------------------------------------------------------------
+    # List-like Methods
 
-        Returns
-        -------
-        Series
-        """"""
-        from pandas import Series, Index
+    def delete(self, loc):
+        new_i8s = np.delete(self.asi8, loc)
 
-        if dropna:
-            values = self[~self.isna()]._data
+        freq = None
+        if is_period_dtype(self):
+            freq = self.freq
+        elif is_integer(loc):
+            if loc in (0, -len(self), -1, len(self) - 1):
+                freq = self.freq
         else:
-            values = self._data
-
-        cls = type(self)
-
-        result = value_counts(values, sort=False, dropna=dropna)
-        index = Index(
-            cls(result.index.view(""i8""), dtype=self.dtype), name=result.index.name
-        )
-        return Series(result.values, index=index, name=result.name)
+            if is_list_like(loc):
+                loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))
+            if isinstance(loc, slice) and loc.step in (1, None):
+                if loc.start in (0, None) or loc.stop in (len(self), None):
+                    freq = self.freq
 
-    def map(self, mapper):
-        # TODO(GH-23179): Add ExtensionArray.map
-        # Need to figure out if we want ExtensionArray.map first.
-        # If so, then we can refactor IndexOpsMixin._map_values to
-        # a standalone function and call from here..
-        # Else, just rewrite _map_infer_values to do the right thing.
-        from pandas import Index
+        return self._shallow_copy(new_i8s, freq=freq)
 
-        return Index(self).map(mapper).array
 
-    # ------------------------------------------------------------------
-    # Null Handling
+class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):
+    """"""
+    Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,
+    but not PeriodIndex
+    """"""
 
-    def isna(self):
-        return self._isnan
+    # Compat for frequency inference, see GH#23789
+    _is_monotonic_increasing = Index.is_monotonic_increasing
+    _is_monotonic_decreasing = Index.is_monotonic_decreasing
+    _is_unique = Index.is_unique
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _isnan(self):
-        """"""
-        return if each value is nan
+    def _set_freq(self, freq):
         """"""
-        return self.asi8 == iNaT
+        Set the _freq attribute on our underlying DatetimeArray.
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _hasnans(self):
-        """"""
-        return if I have any nans; enables various perf speedups
-        """"""
-        return bool(self._isnan.any())
-
-    def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):
-        """"""
         Parameters
         ----------
-        result : a ndarray
-        fill_value : object, default iNaT
-        convert : str, dtype or None
-
-        Returns
-        -------
-        result : ndarray with values replace by the fill_value
-
-        mask the result if needed, convert to the provided dtype if its not
-        None
-
-        This is an internal routine.
-        """"""
-
-        if self._hasnans:
-            if convert:
-                result = result.astype(convert)
-            if fill_value is None:
-                fill_value = np.nan
-            result[self._isnan] = fill_value
-        return result
-
-    def fillna(self, value=None, method=None, limit=None):
-        # TODO(GH-20300): remove this
-        # Just overriding to ensure that we avoid an astype(object).
-        # Either 20300 or a `_values_for_fillna` would avoid this duplication.
-        if isinstance(value, ABCSeries):
-            value = value.array
-
-        value, method = validate_fillna_kwargs(value, method)
-
-        mask = self.isna()
-
-        if is_array_like(value):
-            if len(value) != len(self):
-                raise ValueError(
-                    f""Length of 'value' does not match. Got ({len(value)}) ""
-                    f"" expected {len(self)}""
-                )
-            value = value[mask]
-
-        if mask.any():
-            if method is not None:
-                if method == ""pad"":
-                    func = missing.pad_1d
-                else:
-                    func = missing.backfill_1d
-
-                values = self._data
-                if not is_period_dtype(self):
-                    # For PeriodArray self._data is i8, which gets copied
-                    #  by `func`.  Otherwise we need to make a copy manually
-                    # to avoid modifying `self` in-place.
-                    values = values.copy()
-
-                new_values = func(values, limit=limit, mask=mask)
-                if is_datetime64tz_dtype(self):
-                    # we need to pass int64 values to the constructor to avoid
-                    #  re-localizing incorrectly
-                    new_values = new_values.view(""i8"")
-                new_values = type(self)(new_values, dtype=self.dtype)
-            else:
-                # fill with value
-                new_values = self.copy()
-                new_values[mask] = value
+        freq : DateOffset, None, or ""infer""
+        """"""
+        # GH#29843
+        if freq is None:
+            # Always valid
+            pass
+        elif len(self) == 0 and isinstance(freq, DateOffset):
+            # Always valid.  In the TimedeltaIndex case, we assume this
+            #  is a Tick offset.
+            pass
         else:
-            new_values = self.copy()
-        return new_values
+            # As an internal method, we can ensure this assertion always holds
+            assert freq == ""infer""
+            freq = to_offset(self.inferred_freq)
 
-    # ------------------------------------------------------------------
-    # Frequency Properties/Methods
+        self._data._freq = freq
 
-    @property
-    def freq(self):
-        """"""
-        Return the frequency object if it is set, otherwise None.
-        """"""
-        return self._freq
+    def _shallow_copy(self, values=None, **kwargs):
+        if values is None:
+            values = self._data
+        if isinstance(values, type(self)):
+            values = values._data
 
-    @freq.setter
-    def freq(self, value):
-        if value is not None:
-            value = frequencies.to_offset(value)
-            self._validate_frequency(self, value)
+        attributes = self._get_attributes_dict()
 
-        self._freq = value
+        if ""freq"" not in kwargs and self.freq is not None:
+            if isinstance(values, (DatetimeArray, TimedeltaArray)):
+                if values.freq is None:
+                    del attributes[""freq""]
 
-    @property
-    def freqstr(self):
-        """"""
-        Return the frequency object as a string if its set, otherwise None
-        """"""
-        if self.freq is None:
-            return None
-        return self.freq.freqstr
+        attributes.update(kwargs)
+        return self._simple_new(values, **attributes)
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def inferred_freq(self):
-        """"""
-        Tryies to return a string representing a frequency guess,
-        generated by infer_freq.  Returns None if it can't autodetect the
-        frequency.
-        """"""
-        if self.ndim != 1:
-            return None
-        try:
-            return frequencies.infer_freq(self)
-        except ValueError:
-            return None
+    # --------------------------------------------------------------------
+    # Set Operation Methods
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _resolution(self):
-        return frequencies.Resolution.get_reso_from_freq(self.freqstr)
+    @Appender(Index.difference.__doc__)
+    def difference(self, other, sort=None):
+        new_idx = super().difference(other, sort=sort)
+        new_idx._set_freq(None)
+        return new_idx
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def resolution(self):
-        """"""
-        Returns day, hour, minute, second, millisecond or microsecond
+    def intersection(self, other, sort=False):
         """"""
-        return frequencies.Resolution.get_str(self._resolution)
+        Specialized intersection for DatetimeIndex/TimedeltaIndex.
 
-    @classmethod
-    def _validate_frequency(cls, index, freq, **kwargs):
-        """"""
-        Validate that a frequency is compatible with the values of a given
-        Datetime Array/Index or Timedelta Array/Index
+        May be much faster than Index.intersection
 
         Parameters
         ----------
-        index : DatetimeIndex or TimedeltaIndex
-            The index on which to determine if the given frequency is valid
-        freq : DateOffset
-            The frequency to validate
-        """"""
-        if is_period_dtype(cls):
-            # Frequency validation is not meaningful for Period Array/Index
-            return None
-
-        inferred = index.inferred_freq
-        if index.size == 0 or inferred == freq.freqstr:
-            return None
+        other : Same type as self or array-like
+        sort : False or None, default False
+            Sort the resulting index if possible.
 
-        try:
-            on_freq = cls._generate_range(
-                start=index[0], end=None, periods=len(index), freq=freq, **kwargs
-            )
-            if not np.array_equal(index.asi8, on_freq.asi8):
-                raise ValueError
-        except ValueError as e:
-            if ""non-fixed"" in str(e):
-                # non-fixed frequencies are not meaningful for timedelta64;
-                #  we retain that error message
-                raise e
-            # GH#11587 the main way this is reached is if the `np.array_equal`
-            #  check above is False.  This can also be reached if index[0]
-            #  is `NaT`, in which case the call to `cls._generate_range` will
-            #  raise a ValueError, which we re-raise with a more targeted
-            #  message.
-            raise ValueError(
-                f""Inferred frequency {inferred} from passed values ""
-                f""does not conform to passed frequency {freq.freqstr}""
-            )
-
-    # monotonicity/uniqueness properties are called via frequencies.infer_freq,
-    #  see GH#23789
-
-    @property
-    def _is_monotonic_increasing(self):
-        return algos.is_monotonic(self.asi8, timelike=True)[0]
+            .. versionadded:: 0.24.0
 
-    @property
-    def _is_monotonic_decreasing(self):
-        return algos.is_monotonic(self.asi8, timelike=True)[1]
+            .. versionchanged:: 0.24.1
 
-    @property
-    def _is_unique(self):
-        return len(unique1d(self.asi8)) == len(self)
-
-    # ------------------------------------------------------------------
-    # Arithmetic Methods
-    _create_comparison_method = classmethod(_datetimelike_array_cmp)
-
-    # pow is invalid for all three subclasses; TimedeltaArray will override
-    #  the multiplication and division ops
-    __pow__ = make_invalid_op(""__pow__"")
-    __rpow__ = make_invalid_op(""__rpow__"")
-    __mul__ = make_invalid_op(""__mul__"")
-    __rmul__ = make_invalid_op(""__rmul__"")
-    __truediv__ = make_invalid_op(""__truediv__"")
-    __rtruediv__ = make_invalid_op(""__rtruediv__"")
-    __floordiv__ = make_invalid_op(""__floordiv__"")
-    __rfloordiv__ = make_invalid_op(""__rfloordiv__"")
-    __mod__ = make_invalid_op(""__mod__"")
-    __rmod__ = make_invalid_op(""__rmod__"")
-    __divmod__ = make_invalid_op(""__divmod__"")
-    __rdivmod__ = make_invalid_op(""__rdivmod__"")
-
-    def _add_datetimelike_scalar(self, other):
-        # Overridden by TimedeltaArray
-        raise TypeError(f""cannot add {type(self).__name__} and {type(other).__name__}"")
-
-    _add_datetime_arraylike = _add_datetimelike_scalar
-
-    def _sub_datetimelike_scalar(self, other):
-        # Overridden by DatetimeArray
-        assert other is not NaT
-        raise TypeError(f""cannot subtract a datelike from a {type(self).__name__}"")
-
-    _sub_datetime_arraylike = _sub_datetimelike_scalar
-
-    def _sub_period(self, other):
-        # Overridden by PeriodArray
-        raise TypeError(f""cannot subtract Period from a {type(self).__name__}"")
-
-    def _add_offset(self, offset):
-        raise AbstractMethodError(self)
+               Changed the default to ``False`` to match the behaviour
+               from before 0.24.0.
 
-    def _add_delta(self, other):
-        """"""
-        Add a timedelta-like, Tick or TimedeltaIndex-like object
-        to self, yielding an int64 numpy array
+            .. versionchanged:: 0.25.0
 
-        Parameters
-        ----------
-        delta : {timedelta, np.timedelta64, Tick,
-                 TimedeltaIndex, ndarray[timedelta64]}
+               The `sort` keyword is added
 
         Returns
         -------
-        result : ndarray[int64]
-
-        Notes
-        -----
-        The result's name is set outside of _add_delta by the calling
-        method (__add__ or __sub__), if necessary (i.e. for Indexes).
+        y : Index or same type as self
         """"""
-        if isinstance(other, (Tick, timedelta, np.timedelta64)):
-            new_values = self._add_timedeltalike_scalar(other)
-        elif is_timedelta64_dtype(other):
-            # ndarray[timedelta64] or TimedeltaArray/index
-            new_values = self._add_delta_tdi(other)
+        self._validate_sort_keyword(sort)
+        self._assert_can_do_setop(other)
 
-        return new_values
+        if self.equals(other):
+            return self._get_reconciled_name_object(other)
 
-    def _add_timedeltalike_scalar(self, other):
-        """"""
-        Add a delta of a timedeltalike
-        return the i8 result view
-        """"""
-        if isna(other):
-            # i.e np.timedelta64(""NaT""), not recognized by delta_to_nanoseconds
-            new_values = np.empty(self.shape, dtype=""i8"")
-            new_values[:] = iNaT
-            return new_values
-
-        inc = delta_to_nanoseconds(other)
-        new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(
-            ""i8""
-        )
-        new_values = self._maybe_mask_results(new_values)
-        return new_values.view(""i8"")
-
-    def _add_delta_tdi(self, other):
-        """"""
-        Add a delta of a TimedeltaIndex
-        return the i8 result view
-        """"""
-        if len(self) != len(other):
-            raise ValueError(""cannot add indices of unequal length"")
+        if len(self) == 0:
+            return self.copy()
+        if len(other) == 0:
+            return other.copy()
+
+        if not isinstance(other, type(self)):
+            result = Index.intersection(self, other, sort=sort)
+            if isinstance(result, type(self)):
+                if result.freq is None:
+                    result._set_freq(""infer"")
+            return result
 
-        if isinstance(other, np.ndarray):
-            # ndarray[timedelta64]; wrap in TimedeltaIndex for op
-            from pandas.core.arrays import TimedeltaArray
+        elif (
+            other.freq is None
+            or self.freq is None
+            or other.freq != self.freq
+            or not other.freq.is_anchored()
+            or (not self.is_monotonic or not other.is_monotonic)
+        ):
+            result = Index.intersection(self, other, sort=sort)
 
-            other = TimedeltaArray._from_sequence(other)
+            # Invalidate the freq of `result`, which may not be correct at
+            # this point, depending on the values.
 
-        self_i8 = self.asi8
-        other_i8 = other.asi8
-        new_values = checked_add_with_arr(
-            self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan
-        )
-        if self._hasnans or other._hasnans:
-            mask = (self._isnan) | (other._isnan)
-            new_values[mask] = iNaT
-        return new_values.view(""i8"")
-
-    def _add_nat(self):
-        """"""
-        Add pd.NaT to self
-        """"""
-        if is_period_dtype(self):
-            raise TypeError(
-                f""Cannot add {type(self).__name__} and {type(NaT).__name__}""
+            result._set_freq(None)
+            result = self._shallow_copy(
+                result._data, name=result.name, dtype=result.dtype, freq=None
             )
+            if result.freq is None:
+                result._set_freq(""infer"")
+            return result
 
-        # GH#19124 pd.NaT is treated like a timedelta for both timedelta
-        # and datetime dtypes
-        result = np.zeros(self.shape, dtype=np.int64)
-        result.fill(iNaT)
-        return type(self)(result, dtype=self.dtype, freq=None)
-
-    def _sub_nat(self):
-        """"""
-        Subtract pd.NaT from self
-        """"""
-        # GH#19124 Timedelta - datetime is not in general well-defined.
-        # We make an exception for pd.NaT, which in this case quacks
-        # like a timedelta.
-        # For datetime64 dtypes by convention we treat NaT as a datetime, so
-        # this subtraction returns a timedelta64 dtype.
-        # For period dtype, timedelta64 is a close-enough return dtype.
-        result = np.zeros(self.shape, dtype=np.int64)
-        result.fill(iNaT)
-        return result.view(""timedelta64[ns]"")
-
-    def _sub_period_array(self, other):
-        """"""
-        Subtract a Period Array/Index from self.  This is only valid if self
-        is itself a Period Array/Index, raises otherwise.  Both objects must
-        have the same frequency.
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        else:
+            left, right = other, self
 
-        Parameters
-        ----------
-        other : PeriodIndex or PeriodArray
+        # after sorting, the intersection always starts with the right index
+        # and ends with the index of which the last elements is smallest
+        end = min(left[-1], right[-1])
+        start = right[0]
 
-        Returns
-        -------
-        result : np.ndarray[object]
-            Array of DateOffset objects; nulls represented by NaT.
-        """"""
-        if not is_period_dtype(self):
-            raise TypeError(
-                f""cannot subtract {other.dtype}-dtype from {type(self).__name__}""
-            )
+        if end < start:
+            return type(self)(data=[])
+        else:
+            lslice = slice(*left.slice_locs(start, end))
+            left_chunk = left.values[lslice]
+            return self._shallow_copy(left_chunk)
 
-        if self.freq != other.freq:
-            msg = DIFFERENT_FREQ.format(
-                cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr
-            )
-            raise IncompatibleFrequency(msg)
+    def _can_fast_union(self, other) -> bool:
+        if not isinstance(other, type(self)):
+            return False
 
-        new_values = checked_add_with_arr(
-            self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan
-        )
+        freq = self.freq
 
-        new_values = np.array([self.freq.base * x for x in new_values])
-        if self._hasnans or other._hasnans:
-            mask = (self._isnan) | (other._isnan)
-            new_values[mask] = NaT
-        return new_values
+        if freq is None or freq != other.freq:
+            return False
 
-    def _addsub_object_array(self, other: np.ndarray, op):
-        """"""
-        Add or subtract array-like of DateOffset objects
+        if not self.is_monotonic or not other.is_monotonic:
+            return False
 
-        Parameters
-        ----------
-        other : np.ndarray[object]
-        op : {operator.add, operator.sub}
+        if len(self) == 0 or len(other) == 0:
+            return True
 
-        Returns
-        -------
-        result : same class as self
-        """"""
-        assert op in [operator.add, operator.sub]
-        if len(other) == 1:
-            return op(self, other[0])
-
-        warnings.warn(
-            ""Adding/subtracting array of DateOffsets to ""
-            f""{type(self).__name__} not vectorized"",
-            PerformanceWarning,
-        )
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        else:
+            left, right = other, self
 
-        # For EA self.astype('O') returns a numpy array, not an Index
-        left = self.astype(""O"")
+        right_start = right[0]
+        left_end = left[-1]
 
-        res_values = op(left, np.array(other))
-        kwargs = {}
-        if not is_period_dtype(self):
-            kwargs[""freq""] = ""infer""
+        # Only need to ""adjoin"", not overlap
         try:
-            res = type(self)._from_sequence(res_values, **kwargs)
+            return (right_start == left_end + freq) or right_start in left
         except ValueError:
-            # e.g. we've passed a Timestamp to TimedeltaArray
-            res = res_values
-        return res
-
-    def _time_shift(self, periods, freq=None):
-        """"""
-        Shift each value by `periods`.
-
-        Note this is different from ExtensionArray.shift, which
-        shifts the *position* of each element, padding the end with
-        missing values.
-
-        Parameters
-        ----------
-        periods : int
-            Number of periods to shift by.
-        freq : pandas.DateOffset, pandas.Timedelta, or str
-            Frequency increment to shift by.
-        """"""
-        if freq is not None and freq != self.freq:
-            if isinstance(freq, str):
-                freq = frequencies.to_offset(freq)
-            offset = periods * freq
-            result = self + offset
-            return result
-
-        if periods == 0:
-            # immutable so OK
-            return self.copy()
-
-        if self.freq is None:
-            raise NullFrequencyError(""Cannot shift with no freq"")
-
-        start = self[0] + periods * self.freq
-        end = self[-1] + periods * self.freq
-
-        # Note: in the DatetimeTZ case, _generate_range will infer the
-        #  appropriate timezone from `start` and `end`, so tz does not need
-        #  to be passed explicitly.
-        return self._generate_range(start=start, end=end, periods=None, freq=self.freq)
-
-    @unpack_zerodim_and_defer(""__add__"")
-    def __add__(self, other):
-
-        # scalar others
-        if other is NaT:
-            result = self._add_nat()
-        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
-            result = self._add_delta(other)
-        elif isinstance(other, DateOffset):
-            # specifically _not_ a Tick
-            result = self._add_offset(other)
-        elif isinstance(other, (datetime, np.datetime64)):
-            result = self._add_datetimelike_scalar(other)
-        elif lib.is_integer(other):
-            # This check must come after the check for np.timedelta64
-            # as is_integer returns True for these
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._time_shift(other)
-
-        # array-like others
-        elif is_timedelta64_dtype(other):
-            # TimedeltaIndex, ndarray[timedelta64]
-            result = self._add_delta(other)
-        elif is_object_dtype(other):
-            # e.g. Array/Index of DateOffset objects
-            result = self._addsub_object_array(other, operator.add)
-        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
-            # DatetimeIndex, ndarray[datetime64]
-            return self._add_datetime_arraylike(other)
-        elif is_integer_dtype(other):
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._addsub_int_array(other, operator.add)
+            # if we are comparing a freq that does not propagate timezones
+            # this will raise
+            return False
+
+    def _fast_union(self, other, sort=None):
+        if len(other) == 0:
+            return self.view(type(self))
+
+        if len(self) == 0:
+            return other.view(type(self))
+
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        elif sort is False:
+            # TDIs are not in the ""correct"" order and we don't want
+            #  to sort but want to remove overlaps
+            left, right = self, other
+            left_start = left[0]
+            loc = right.searchsorted(left_start, side=""left"")
+            right_chunk = right.values[:loc]
+            dates = concat_compat((left.values, right_chunk))
+            return self._shallow_copy(dates)
         else:
-            # Includes Categorical, other ExtensionArrays
-            # For PeriodDtype, if self is a TimedeltaArray and other is a
-            #  PeriodArray with  a timedelta-like (i.e. Tick) freq, this
-            #  operation is valid.  Defer to the PeriodArray implementation.
-            #  In remaining cases, this will end up raising TypeError.
-            return NotImplemented
+            left, right = other, self
 
-        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
-            from pandas.core.arrays import TimedeltaArray
-
-            return TimedeltaArray(result)
-        return result
+        left_end = left[-1]
+        right_end = right[-1]
 
-    def __radd__(self, other):
-        # alias for __add__
-        return self.__add__(other)
-
-    @unpack_zerodim_and_defer(""__sub__"")
-    def __sub__(self, other):
-
-        # scalar others
-        if other is NaT:
-            result = self._sub_nat()
-        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
-            result = self._add_delta(-other)
-        elif isinstance(other, DateOffset):
-            # specifically _not_ a Tick
-            result = self._add_offset(-other)
-        elif isinstance(other, (datetime, np.datetime64)):
-            result = self._sub_datetimelike_scalar(other)
-        elif lib.is_integer(other):
-            # This check must come after the check for np.timedelta64
-            # as is_integer returns True for these
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._time_shift(-other)
-
-        elif isinstance(other, Period):
-            result = self._sub_period(other)
-
-        # array-like others
-        elif is_timedelta64_dtype(other):
-            # TimedeltaIndex, ndarray[timedelta64]
-            result = self._add_delta(-other)
-        elif is_object_dtype(other):
-            # e.g. Array/Index of DateOffset objects
-            result = self._addsub_object_array(other, operator.sub)
-        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
-            # DatetimeIndex, ndarray[datetime64]
-            result = self._sub_datetime_arraylike(other)
-        elif is_period_dtype(other):
-            # PeriodIndex
-            result = self._sub_period_array(other)
-        elif is_integer_dtype(other):
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._addsub_int_array(other, operator.sub)
+        # concatenate
+        if left_end < right_end:
+            loc = right.searchsorted(left_end, side=""right"")
+            right_chunk = right.values[loc:]
+            dates = concat_compat((left.values, right_chunk))
+            return self._shallow_copy(dates)
         else:
-            # Includes ExtensionArrays, float_dtype
-            return NotImplemented
+            return left
 
-        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
-            from pandas.core.arrays import TimedeltaArray
+    def _union(self, other, sort):
+        if not len(other) or self.equals(other) or not len(self):
+            return super()._union(other, sort=sort)
 
-            return TimedeltaArray(result)
-        return result
+        # We are called by `union`, which is responsible for this validation
+        assert isinstance(other, type(self))
 
-    def __rsub__(self, other):
-        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):
-            # ndarray[datetime64] cannot be subtracted from self, so
-            # we need to wrap in DatetimeArray/Index and flip the operation
-            if lib.is_scalar(other):
-                # i.e. np.datetime64 object
-                return Timestamp(other) - self
-            if not isinstance(other, DatetimeLikeArrayMixin):
-                # Avoid down-casting DatetimeIndex
-                from pandas.core.arrays import DatetimeArray
-
-                other = DatetimeArray(other)
-            return other - self
-        elif (
-            is_datetime64_any_dtype(self.dtype)
-            and hasattr(other, ""dtype"")
-            and not is_datetime64_any_dtype(other.dtype)
-        ):
-            # GH#19959 datetime - datetime is well-defined as timedelta,
-            # but any other type - datetime is not well-defined.
-            raise TypeError(
-                f""cannot subtract {type(self).__name__} from {type(other).__name__}""
-            )
-        elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):
-            # TODO: Can we simplify/generalize these cases at all?
-            raise TypeError(f""cannot subtract {type(self).__name__} from {other.dtype}"")
-        elif is_timedelta64_dtype(self.dtype):
-            if lib.is_integer(other) or is_integer_dtype(other):
-                # need to subtract before negating, since that flips freq
-                # -self flips self.freq, messing up results
-                return -(self - other)
-
-            return (-self) + other
-
-        return -(self - other)
-
-    def __iadd__(self, other):  # type: ignore
-        result = self + other
-        self[:] = result[:]
-
-        if not is_period_dtype(self):
-            # restore freq, which is invalidated by setitem
-            self._freq = result._freq
-        return self
-
-    def __isub__(self, other):  # type: ignore
-        result = self - other
-        self[:] = result[:]
-
-        if not is_period_dtype(self):
-            # restore freq, which is invalidated by setitem
-            self._freq = result._freq
-        return self
-
-    # --------------------------------------------------------------
-    # Comparison Methods
-
-    def _ensure_localized(
-        self, arg, ambiguous=""raise"", nonexistent=""raise"", from_utc=False
-    ):
-        """"""
-        Ensure that we are re-localized.
+        this, other = self._maybe_utc_convert(other)
 
-        This is for compat as we can then call this on all datetimelike
-        arrays generally (ignored for Period/Timedelta)
-
-        Parameters
-        ----------
-        arg : Union[DatetimeLikeArray, DatetimeIndexOpsMixin, ndarray]
-        ambiguous : str, bool, or bool-ndarray, default 'raise'
-        nonexistent : str, default 'raise'
-        from_utc : bool, default False
-            If True, localize the i8 ndarray to UTC first before converting to
-            the appropriate tz. If False, localize directly to the tz.
-
-        Returns
-        -------
-        localized array
-        """"""
-
-        # reconvert to local tz
-        tz = getattr(self, ""tz"", None)
-        if tz is not None:
-            if not isinstance(arg, type(self)):
-                arg = self._simple_new(arg)
-            if from_utc:
-                arg = arg.tz_localize(""UTC"").tz_convert(self.tz)
-            else:
-                arg = arg.tz_localize(
-                    self.tz, ambiguous=ambiguous, nonexistent=nonexistent
-                )
-        return arg
-
-    # --------------------------------------------------------------
-    # Reductions
-
-    def _reduce(self, name, axis=0, skipna=True, **kwargs):
-        op = getattr(self, name, None)
-        if op:
-            return op(skipna=skipna, **kwargs)
+        if this._can_fast_union(other):
+            return this._fast_union(other, sort=sort)
         else:
-            return super()._reduce(name, skipna, **kwargs)
-
-    def min(self, axis=None, skipna=True, *args, **kwargs):
-        """"""
-        Return the minimum value of the Array or minimum along
-        an axis.
-
-        See Also
-        --------
-        numpy.ndarray.min
-        Index.min : Return the minimum value in an Index.
-        Series.min : Return the minimum value in a Series.
-        """"""
-        nv.validate_min(args, kwargs)
-        nv.validate_minmax_axis(axis)
+            result = Index._union(this, other, sort=sort)
+            if isinstance(result, type(self)):
+                assert result._data.dtype == this.dtype
+                if result.freq is None:
+                    result._set_freq(""infer"")
+            return result
 
-        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())
-        if isna(result):
-            # Period._from_ordinal does not handle np.nan gracefully
-            return NaT
-        return self._box_func(result)
+    # --------------------------------------------------------------------
+    # Join Methods
+    _join_precedence = 10
 
-    def max(self, axis=None, skipna=True, *args, **kwargs):
-        """"""
-        Return the maximum value of the Array or maximum along
-        an axis.
+    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)
+    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)
+    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)
+    _left_indexer_unique = _join_i8_wrapper(
+        libjoin.left_join_indexer_unique, with_indexers=False
+    )
 
-        See Also
-        --------
-        numpy.ndarray.max
-        Index.max : Return the maximum value in an Index.
-        Series.max : Return the maximum value in a Series.
+    def join(
+        self, other, how: str = ""left"", level=None, return_indexers=False, sort=False
+    ):
         """"""
-        # TODO: skipna is broken with max.
-        # See https://github.com/pandas-dev/pandas/issues/24265
-        nv.validate_max(args, kwargs)
-        nv.validate_minmax_axis(axis)
-
-        mask = self.isna()
-        if skipna:
-            values = self[~mask].asi8
-        elif mask.any():
-            return NaT
-        else:
-            values = self.asi8
-
-        if not len(values):
-            # short-circuit for empty max / min
-            return NaT
-
-        result = nanops.nanmax(values, skipna=skipna)
-        # Don't have to worry about NA `result`, since no NA went in.
-        return self._box_func(result)
-
-    def mean(self, skipna=True):
+        See Index.join
         """"""
-        Return the mean value of the Array.
-
-        .. versionadded:: 0.25.0
-
-        Parameters
-        ----------
-        skipna : bool, default True
-            Whether to ignore any NaT elements.
+        if self._is_convertible_to_index_for_join(other):
+            try:
+                other = type(self)(other)
+            except (TypeError, ValueError):
+                pass
+
+        this, other = self._maybe_utc_convert(other)
+        return Index.join(
+            this,
+            other,
+            how=how,
+            level=level,
+            return_indexers=return_indexers,
+            sort=sort,
+        )
 
-        Returns
-        -------
-        scalar
-            Timestamp or Timedelta.
+    def _maybe_utc_convert(self, other):
+        this = self
+        if not hasattr(self, ""tz""):
+            return this, other
 
-        See Also
-        --------
-        numpy.ndarray.mean : Returns the average of array elements along a given axis.
-        Series.mean : Return the mean value in a Series.
+        if isinstance(other, type(self)):
+            if self.tz is not None:
+                if other.tz is None:
+                    raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
+            elif other.tz is not None:
+                raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
 
-        Notes
-        -----
-        mean is only defined for Datetime and Timedelta dtypes, not for Period.
-        """"""
-        if is_period_dtype(self):
-            # See discussion in GH#24757
-            raise TypeError(
-                f""mean is not implemented for {type(self).__name__} since the ""
-                ""meaning is ambiguous.  An alternative is ""
-                ""obj.to_timestamp(how='start').mean()""
-            )
+            if not timezones.tz_compare(self.tz, other.tz):
+                this = self.tz_convert(""UTC"")
+                other = other.tz_convert(""UTC"")
+        return this, other
 
-        mask = self.isna()
-        if skipna:
-            values = self[~mask]
-        elif mask.any():
-            return NaT
+    @classmethod
+    def _is_convertible_to_index_for_join(cls, other: Index) -> bool:
+        """"""
+        return a boolean whether I can attempt conversion to a
+        DatetimeIndex/TimedeltaIndex
+        """"""
+        if isinstance(other, cls):
+            return False
+        elif len(other) > 0 and other.inferred_type not in (
+            ""floating"",
+            ""mixed-integer"",
+            ""integer"",
+            ""integer-na"",
+            ""mixed-integer-float"",
+            ""mixed"",
+        ):
+            return True
+        return False
+
+    def _wrap_joined_index(self, joined, other):
+        name = get_op_result_name(self, other)
+        if (
+            isinstance(other, type(self))
+            and self.freq == other.freq
+            and self._can_fast_union(other)
+        ):
+            joined = self._shallow_copy(joined)
+            joined.name = name
+            return joined
         else:
-            values = self
-
-        if not len(values):
-            # short-circuit for empty max / min
-            return NaT
-
-        result = nanops.nanmean(values.view(""i8""), skipna=skipna)
-        # Don't have to worry about NA `result`, since no NA went in.
-        return self._box_func(result)
-
-
-DatetimeLikeArrayMixin._add_comparison_ops()
+            kwargs = {}
+            if hasattr(self, ""tz""):
+                kwargs[""tz""] = getattr(other, ""tz"", None)
+            return self._simple_new(joined, name, **kwargs)
 
-# -------------------------------------------------------------------
-# Shared Constructor Helpers
 
-
-def validate_periods(periods):
+class DatetimelikeDelegateMixin(PandasDelegate):
     """"""
-    If a `periods` argument is passed to the Datetime/Timedelta Array/Index
-    constructor, cast it to an integer.
-
-    Parameters
-    ----------
-    periods : None, float, int
-
-    Returns
-    -------
-    periods : None or int
-
-    Raises
-    ------
-    TypeError
-        if periods is None, float, or int
+    Delegation mechanism, specific for Datetime, Timedelta, and Period types.
+
+    Functionality is delegated from the Index class to an Array class. A
+    few things can be customized
+
+    * _delegated_methods, delegated_properties : List
+        The list of property / method names being delagated.
+    * raw_methods : Set
+        The set of methods whose results should should *not* be
+        boxed in an index, after being returned from the array
+    * raw_properties : Set
+        The set of properties whose results should should *not* be
+        boxed in an index, after being returned from the array
     """"""
-    if periods is not None:
-        if lib.is_float(periods):
-            periods = int(periods)
-        elif not lib.is_integer(periods):
-            raise TypeError(f""periods must be a number, got {periods}"")
-    return periods
-
 
-def validate_endpoints(closed):
-    """"""
-    Check that the `closed` argument is among [None, ""left"", ""right""]
+    # raw_methods : dispatch methods that shouldn't be boxed in an Index
+    _raw_methods: Set[str] = set()
+    # raw_properties : dispatch properties that shouldn't be boxed in an Index
+    _raw_properties: Set[str] = set()
+    _data: ExtensionArray
 
-    Parameters
-    ----------
-    closed : {None, ""left"", ""right""}
-
-    Returns
-    -------
-    left_closed : bool
-    right_closed : bool
-
-    Raises
-    ------
-    ValueError : if argument is not among valid values
-    """"""
-    left_closed = False
-    right_closed = False
-
-    if closed is None:
-        left_closed = True
-        right_closed = True
-    elif closed == ""left"":
-        left_closed = True
-    elif closed == ""right"":
-        right_closed = True
-    else:
-        raise ValueError(""Closed has to be either 'left', 'right' or None"")
-
-    return left_closed, right_closed
-
-
-def validate_inferred_freq(freq, inferred_freq, freq_infer):
-    """"""
-    If the user passes a freq and another freq is inferred from passed data,
-    require that they match.
-
-    Parameters
-    ----------
-    freq : DateOffset or None
-    inferred_freq : DateOffset or None
-    freq_infer : bool
-
-    Returns
-    -------
-    freq : DateOffset or None
-    freq_infer : bool
-
-    Notes
-    -----
-    We assume at this point that `maybe_infer_freq` has been called, so
-    `freq` is either a DateOffset object or None.
-    """"""
-    if inferred_freq is not None:
-        if freq is not None and freq != inferred_freq:
-            raise ValueError(
-                f""Inferred frequency {inferred_freq} from passed ""
-                ""values does not conform to passed frequency ""
-                f""{freq.freqstr}""
-            )
-        elif freq is None:
-            freq = inferred_freq
-        freq_infer = False
-
-    return freq, freq_infer
-
-
-def maybe_infer_freq(freq):
-    """"""
-    Comparing a DateOffset to the string ""infer"" raises, so we need to
-    be careful about comparisons.  Make a dummy variable `freq_infer` to
-    signify the case where the given freq is ""infer"" and set freq to None
-    to avoid comparison trouble later on.
-
-    Parameters
-    ----------
-    freq : {DateOffset, None, str}
-
-    Returns
-    -------
-    freq : {DateOffset, None}
-    freq_infer : bool
-    """"""
-    freq_infer = False
-    if not isinstance(freq, DateOffset):
-        # if a passed freq is None, don't infer automatically
-        if freq != ""infer"":
-            freq = frequencies.to_offset(freq)
-        else:
-            freq_infer = True
-            freq = None
-    return freq, freq_infer
+    def _delegate_property_get(self, name, *args, **kwargs):
+        result = getattr(self._data, name)
+        if name not in self._raw_properties:
+            result = Index(result, name=self.name)
+        return result
 
+    def _delegate_property_set(self, name, value, *args, **kwargs):
+        setattr(self._data, name, value)
 
-def _ensure_datetimelike_to_i8(other, to_utc=False):
-    """"""
-    Helper for coercing an input scalar or array to i8.
-
-    Parameters
-    ----------
-    other : 1d array
-    to_utc : bool, default False
-        If True, convert the values to UTC before extracting the i8 values
-        If False, extract the i8 values directly.
-
-    Returns
-    -------
-    i8 1d array
-    """"""
-    from pandas import Index
-
-    if lib.is_scalar(other) and isna(other):
-        return iNaT
-    elif isinstance(other, (ABCPeriodArray, ABCIndexClass, DatetimeLikeArrayMixin)):
-        # convert tz if needed
-        if getattr(other, ""tz"", None) is not None:
-            if to_utc:
-                other = other.tz_convert(""UTC"")
-            else:
-                other = other.tz_localize(None)
-    else:
-        try:
-            return np.array(other, copy=False).view(""i8"")
-        except TypeError:
-            # period array cannot be coerced to int
-            other = Index(other)
-    return other.asi8
+    def _delegate_method(self, name, *args, **kwargs):
+        result = operator.methodcaller(name, *args, **kwargs)(self._data)
+        if name not in self._raw_methods:
+            result = Index(result, name=self.name)
+        return result
"
"pandas","53","f9b49c8","020dcce17e3bd0983fca5b02556bd431140ab371","pandas/core/indexes/base.py;pandas/core/series.py;pandas/tests/indexing/test_scalar.py","pandas/core/indexes/base.py;pandas/core/series.py;pandas/tests/indexing/test_scalar.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py;diff --git a/pandas/core/series.py b/pandas/core/series.py;diff --git a/pandas/tests/indexing/test_scalar.py b/pandas/tests/indexing/test_scalar.py","pandas/tests/indexing/test_categorical.py;pandas/tests/indexing/test_loc.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index b1ac17361..c896e68f7 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -3111,7 +3111,7 @@ class Index(IndexOpsMixin, PandasObject):
                     self._invalid_indexer(""label"", key)
 
             elif kind == ""loc"" and is_integer(key):
-                if not self.holds_integer():
+                if not (is_integer_dtype(self.dtype) or is_object_dtype(self.dtype)):
                     self._invalid_indexer(""label"", key)
 
         return key
diff --git a/pandas/core/series.py b/pandas/core/series.py
index 218237433..77f9df0f4 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -969,9 +969,11 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         if takeable:
             return self._values[label]
 
+        # Similar to Index.get_value, but we do not fall back to positional
+        loc = self.index.get_loc(label)
         # We assume that _convert_scalar_indexer has already been called,
         #  with kind=""loc"", if necessary, by the time we get here
-        return self.index.get_value(self, label)
+        return self.index._get_values_for_loc(self, loc, label)
 
     def __setitem__(self, key, value):
         key = com.apply_if_callable(key, self)
diff --git a/pandas/tests/indexing/test_scalar.py b/pandas/tests/indexing/test_scalar.py
index c4750778e..25939e63c 100644
--- a/pandas/tests/indexing/test_scalar.py
+++ b/pandas/tests/indexing/test_scalar.py
@@ -138,16 +138,12 @@ class TestScalar2:
         result = ser.loc[""a""]
         assert result == 1
 
-        msg = (
-            ""cannot do label indexing on Index ""
-            r""with these indexers \[0\] of type int""
-        )
-        with pytest.raises(TypeError, match=msg):
+        with pytest.raises(KeyError, match=""^0$""):
             ser.at[0]
-        with pytest.raises(TypeError, match=msg):
+        with pytest.raises(KeyError, match=""^0$""):
             ser.loc[0]
 
-    def test_frame_raises_type_error(self):
+    def test_frame_raises_key_error(self):
         # GH#31724 .at should match .loc
         df = DataFrame({""A"": [1, 2, 3]}, index=list(""abc""))
         result = df.at[""a"", ""A""]
@@ -155,13 +151,9 @@ class TestScalar2:
         result = df.loc[""a"", ""A""]
         assert result == 1
 
-        msg = (
-            ""cannot do label indexing on Index ""
-            r""with these indexers \[0\] of type int""
-        )
-        with pytest.raises(TypeError, match=msg):
+        with pytest.raises(KeyError, match=""^0$""):
             df.at[""a"", 0]
-        with pytest.raises(TypeError, match=msg):
+        with pytest.raises(KeyError, match=""^0$""):
             df.loc[""a"", 0]
 
     def test_series_at_raises_key_error(self):
"
"pandas","31","45c13a9","8267427bfe567eec9a098aa8c071dddcc1d289f9","pandas/core/groupby/groupby.py","pandas/core/groupby/groupby.py","diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py","pandas/tests/groupby/test_function.py","","diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py
index 1474e173b..7a7ac58b9 100644
--- a/pandas/core/groupby/groupby.py
+++ b/pandas/core/groupby/groupby.py
@@ -44,7 +44,9 @@ from pandas.util._decorators import Appender, Substitution, cache_readonly, doc
 from pandas.core.dtypes.cast import maybe_cast_result
 from pandas.core.dtypes.common import (
     ensure_float,
+    is_bool_dtype,
     is_datetime64_dtype,
+    is_extension_array_dtype,
     is_integer_dtype,
     is_numeric_dtype,
     is_object_dtype,
@@ -1867,9 +1869,13 @@ class GroupBy(_GroupBy[FrameOrSeries]):
                 )
 
             inference = None
-            if is_integer_dtype(vals):
+            if is_integer_dtype(vals.dtype):
+                if is_extension_array_dtype(vals.dtype):
+                    vals = vals.to_numpy(dtype=float, na_value=np.nan)
                 inference = np.int64
-            elif is_datetime64_dtype(vals):
+            elif is_bool_dtype(vals.dtype) and is_extension_array_dtype(vals.dtype):
+                vals = vals.to_numpy(dtype=float, na_value=np.nan)
+            elif is_datetime64_dtype(vals.dtype):
                 inference = ""datetime64[ns]""
                 vals = np.asarray(vals).astype(np.float)
 
"
"pandas","43","81149fb","be7bfe6ab7ae2cba056f61dea6c3b0226bf80082","pandas/core/ops/__init__.py","pandas/core/ops/__init__.py","diff --git a/pandas/core/ops/__init__.py b/pandas/core/ops/__init__.py","pandas/tests/frame/test_arithmetic.py","","diff --git a/pandas/core/ops/__init__.py b/pandas/core/ops/__init__.py
index ed779c5da..3153a9ac2 100644
--- a/pandas/core/ops/__init__.py
+++ b/pandas/core/ops/__init__.py
@@ -711,13 +711,17 @@ def _align_method_FRAME(
 
 
 def _should_reindex_frame_op(
-    left: ""DataFrame"", right, axis, default_axis: int, fill_value, level
+    left: ""DataFrame"", right, op, axis, default_axis: int, fill_value, level
 ) -> bool:
     """"""
     Check if this is an operation between DataFrames that will need to reindex.
     """"""
     assert isinstance(left, ABCDataFrame)
 
+    if op is operator.pow or op is rpow:
+        # GH#32685 pow has special semantics for operating with null values
+        return False
+
     if not isinstance(right, ABCDataFrame):
         return False
 
@@ -779,7 +783,9 @@ def _arith_method_FRAME(cls, op, special):
     @Appender(doc)
     def f(self, other, axis=default_axis, level=None, fill_value=None):
 
-        if _should_reindex_frame_op(self, other, axis, default_axis, fill_value, level):
+        if _should_reindex_frame_op(
+            self, other, op, axis, default_axis, fill_value, level
+        ):
             return _frame_arith_method_with_reindex(self, other, op)
 
         self, other = _align_method_FRAME(self, other, axis, flex=True, level=level)
"
"pandas","118","6f1accd","76e39ebcf584042fab4f224a6bd2c903bb0c8aff","pandas/core/reshape/melt.py","pandas/core/reshape/melt.py","diff --git a/pandas/core/reshape/melt.py b/pandas/core/reshape/melt.py","pandas/tests/reshape/test_melt.py","","diff --git a/pandas/core/reshape/melt.py b/pandas/core/reshape/melt.py
index 4cba52c5c..8e9edfa5f 100644
--- a/pandas/core/reshape/melt.py
+++ b/pandas/core/reshape/melt.py
@@ -11,6 +11,7 @@ from pandas.core.dtypes.generic import ABCMultiIndex
 from pandas.core.dtypes.missing import notna
 
 from pandas.core.arrays import Categorical
+import pandas.core.common as com
 from pandas.core.frame import DataFrame, _shared_docs
 from pandas.core.indexes.base import Index
 from pandas.core.reshape.concat import concat
@@ -47,7 +48,7 @@ def melt(
         else:
             # Check that `id_vars` are in frame
             id_vars = list(id_vars)
-            missing = Index(np.ravel(id_vars)).difference(cols)
+            missing = Index(com.flatten(id_vars)).difference(cols)
             if not missing.empty:
                 raise KeyError(
                     ""The following 'id_vars' are not present""
@@ -69,7 +70,7 @@ def melt(
         else:
             value_vars = list(value_vars)
             # Check that `value_vars` are in frame
-            missing = Index(np.ravel(value_vars)).difference(cols)
+            missing = Index(com.flatten(value_vars)).difference(cols)
             if not missing.empty:
                 raise KeyError(
                     ""The following 'value_vars' are not present in""
"
"pandas","39","8a5f291","a3097b5bd172e76dd3524eb5dbe18b6b4c22df50","pandas/core/ops/methods.py","pandas/core/ops/methods.py","diff --git a/pandas/core/ops/methods.py b/pandas/core/ops/methods.py","pandas/tests/frame//test_axis_select_reindex.py","","diff --git a/pandas/core/ops/methods.py b/pandas/core/ops/methods.py
index c04658565..0cf1ac4d1 100644
--- a/pandas/core/ops/methods.py
+++ b/pandas/core/ops/methods.py
@@ -93,7 +93,8 @@ def add_special_arithmetic_methods(cls):
 
         def f(self, other):
             result = method(self, other)
-
+            # Delete cacher
+            self._reset_cacher()
             # this makes sure that we are aligned like the input
             # we are updating inplace so we want to ignore is_copy
             self._update_inplace(
"
"pandas","44","96d22d4","50817487ce5b1a2c4896495509e2b53e22fa3212","pandas/core/indexes/base.py;pandas/core/indexes/datetimelike.py;pandas/core/indexes/datetimes.py;pandas/core/indexes/period.py;pandas/core/indexes/timedeltas.py","pandas/core/indexes/base.py;pandas/core/indexes/datetimelike.py;pandas/core/indexes/datetimes.py;pandas/core/indexes/period.py;pandas/core/indexes/timedeltas.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py;diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py;diff --git a/pandas/core/indexes/datetimes.py b/pandas/core/indexes/datetimes.py;diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py;diff --git a/pandas/core/indexes/timedeltas.py b/pandas/core/indexes/timedeltas.py","pandas/tests/indexes/test_base.py;pandas/tests/indexing/test_loc.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index 319664894..51c9cd881 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -29,7 +29,6 @@ from pandas.core.dtypes.common import (
     ensure_platform_int,
     is_bool,
     is_bool_dtype,
-    is_categorical,
     is_categorical_dtype,
     is_datetime64_any_dtype,
     is_dtype_equal,
@@ -532,6 +531,9 @@ class Index(IndexOpsMixin, PandasObject):
                 return self._constructor(values, **attributes)
             except (TypeError, ValueError):
                 pass
+
+        # Remove tz so Index will try non-DatetimeIndex inference
+        attributes.pop(""tz"", None)
         return Index(values, **attributes)
 
     def _update_inplace(self, result, **kwargs):
@@ -4657,10 +4659,8 @@ class Index(IndexOpsMixin, PandasObject):
         if pself is not self or ptarget is not target:
             return pself.get_indexer_non_unique(ptarget)
 
-        if is_categorical(target):
+        if is_categorical_dtype(target.dtype):
             tgt_values = np.asarray(target)
-        elif self.is_all_dates and target.is_all_dates:  # GH 30399
-            tgt_values = target.asi8
         else:
             tgt_values = target._get_engine_target()
 
diff --git a/pandas/core/indexes/datetimelike.py b/pandas/core/indexes/datetimelike.py
index 087fe64ca..9c55d2de9 100644
--- a/pandas/core/indexes/datetimelike.py
+++ b/pandas/core/indexes/datetimelike.py
@@ -8,13 +8,14 @@ import numpy as np
 
 from pandas._libs import NaT, iNaT, join as libjoin, lib
 from pandas._libs.tslibs import timezones
-from pandas._typing import Label
+from pandas._typing import DtypeObj, Label
 from pandas.compat.numpy import function as nv
 from pandas.errors import AbstractMethodError
 from pandas.util._decorators import Appender, cache_readonly, doc
 
 from pandas.core.dtypes.common import (
     ensure_int64,
+    ensure_platform_int,
     is_bool_dtype,
     is_categorical_dtype,
     is_dtype_equal,
@@ -32,7 +33,7 @@ from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray
 from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin
 from pandas.core.base import IndexOpsMixin
 import pandas.core.indexes.base as ibase
-from pandas.core.indexes.base import Index, _index_shared_docs
+from pandas.core.indexes.base import Index, _index_shared_docs, ensure_index
 from pandas.core.indexes.extension import (
     ExtensionIndex,
     inherit_names,
@@ -101,6 +102,12 @@ class DatetimeIndexOpsMixin(ExtensionIndex):
     def is_all_dates(self) -> bool:
         return True
 
+    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
+        """"""
+        Can we compare values of the given dtype to our own?
+        """"""
+        raise AbstractMethodError(self)
+
     # ------------------------------------------------------------------------
     # Abstract data attributes
 
@@ -426,6 +433,21 @@ class DatetimeIndexOpsMixin(ExtensionIndex):
             # try to find the dates
             return (lhs_mask & rhs_mask).nonzero()[0]
 
+    @Appender(Index.get_indexer_non_unique.__doc__)
+    def get_indexer_non_unique(self, target):
+        target = ensure_index(target)
+        pself, ptarget = self._maybe_promote(target)
+        if pself is not self or ptarget is not target:
+            return pself.get_indexer_non_unique(ptarget)
+
+        if not self._is_comparable_dtype(target.dtype):
+            no_matches = -1 * np.ones(self.shape, dtype=np.intp)
+            return no_matches, no_matches
+
+        tgt_values = target.asi8
+        indexer, missing = self._engine.get_indexer_non_unique(tgt_values)
+        return ensure_platform_int(indexer), missing
+
     # --------------------------------------------------------------------
 
     __add__ = make_wrapped_arith_op(""__add__"")
diff --git a/pandas/core/indexes/datetimes.py b/pandas/core/indexes/datetimes.py
index e79113322..ca1995adc 100644
--- a/pandas/core/indexes/datetimes.py
+++ b/pandas/core/indexes/datetimes.py
@@ -7,10 +7,18 @@ import numpy as np
 
 from pandas._libs import NaT, Period, Timestamp, index as libindex, lib, tslib as libts
 from pandas._libs.tslibs import fields, parsing, timezones
-from pandas._typing import Label
+from pandas._typing import DtypeObj, Label
 from pandas.util._decorators import cache_readonly
 
-from pandas.core.dtypes.common import _NS_DTYPE, is_float, is_integer, is_scalar
+from pandas.core.dtypes.common import (
+    _NS_DTYPE,
+    is_datetime64_any_dtype,
+    is_datetime64_dtype,
+    is_datetime64tz_dtype,
+    is_float,
+    is_integer,
+    is_scalar,
+)
 from pandas.core.dtypes.missing import is_valid_nat_for_dtype
 
 from pandas.core.arrays.datetimes import DatetimeArray, tz_to_dtype
@@ -298,6 +306,18 @@ class DatetimeIndex(DatetimeTimedeltaMixin):
             return Timestamp(value).asm8
         raise ValueError(""Passed item and index have different timezone"")
 
+    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
+        """"""
+        Can we compare values of the given dtype to our own?
+        """"""
+        if not is_datetime64_any_dtype(dtype):
+            return False
+        if self.tz is not None:
+            # If we have tz, we can compare to tzaware
+            return is_datetime64tz_dtype(dtype)
+        # if we dont have tz, we can only compare to tznaive
+        return is_datetime64_dtype(dtype)
+
     # --------------------------------------------------------------------
     # Rendering Methods
 
diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py
index f83234f1a..f6bf02b6d 100644
--- a/pandas/core/indexes/period.py
+++ b/pandas/core/indexes/period.py
@@ -9,7 +9,7 @@ from pandas._libs.lib import no_default
 from pandas._libs.tslibs import frequencies as libfrequencies, resolution
 from pandas._libs.tslibs.parsing import parse_time_string
 from pandas._libs.tslibs.period import Period
-from pandas._typing import Label
+from pandas._typing import DtypeObj, Label
 from pandas.util._decorators import Appender, cache_readonly
 
 from pandas.core.dtypes.common import (
@@ -23,6 +23,7 @@ from pandas.core.dtypes.common import (
     is_scalar,
     pandas_dtype,
 )
+from pandas.core.dtypes.dtypes import PeriodDtype
 
 from pandas.core.arrays.period import (
     PeriodArray,
@@ -298,6 +299,14 @@ class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):
         # raise when input doesn't have freq
         raise raise_on_incompatible(self, None)
 
+    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
+        """"""
+        Can we compare values of the given dtype to our own?
+        """"""
+        if not isinstance(dtype, PeriodDtype):
+            return False
+        return dtype.freq == self.freq
+
     # ------------------------------------------------------------------------
     # Rendering Methods
 
@@ -454,12 +463,11 @@ class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):
     def get_indexer_non_unique(self, target):
         target = ensure_index(target)
 
-        if isinstance(target, PeriodIndex):
-            if target.freq != self.freq:
-                no_matches = -1 * np.ones(self.shape, dtype=np.intp)
-                return no_matches, no_matches
+        if not self._is_comparable_dtype(target.dtype):
+            no_matches = -1 * np.ones(self.shape, dtype=np.intp)
+            return no_matches, no_matches
 
-            target = target.asi8
+        target = target.asi8
 
         indexer, missing = self._int64index.get_indexer_non_unique(target)
         return ensure_platform_int(indexer), missing
diff --git a/pandas/core/indexes/timedeltas.py b/pandas/core/indexes/timedeltas.py
index 7a7670b0e..588cb3e37 100644
--- a/pandas/core/indexes/timedeltas.py
+++ b/pandas/core/indexes/timedeltas.py
@@ -1,7 +1,7 @@
 """""" implement the TimedeltaIndex """"""
 
 from pandas._libs import NaT, Timedelta, index as libindex
-from pandas._typing import Label
+from pandas._typing import DtypeObj, Label
 from pandas.util._decorators import Appender
 
 from pandas.core.dtypes.common import (
@@ -213,6 +213,12 @@ class TimedeltaIndex(DatetimeTimedeltaMixin, dtl.TimelikeOps):
             other = TimedeltaIndex(other)
         return self, other
 
+    def _is_comparable_dtype(self, dtype: DtypeObj) -> bool:
+        """"""
+        Can we compare values of the given dtype to our own?
+        """"""
+        return is_timedelta64_dtype(dtype)
+
     def get_loc(self, key, method=None, tolerance=None):
         """"""
         Get integer location for requested label
"
"pandas","162","341043d","640d9e1f5fe8ab64d1f6496b8216c28185e53225","pandas/core/reshape/pivot.py","pandas/core/reshape/pivot.py","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py","pandas/tests/reshape/test_pivot.py","","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py
index 79716520f..d653dd873 100644
--- a/pandas/core/reshape/pivot.py
+++ b/pandas/core/reshape/pivot.py
@@ -611,13 +611,21 @@ def _normalize(table, normalize, margins, margins_name=""All""):
         table = table.fillna(0)
 
     elif margins is True:
-
-        column_margin = table.loc[:, margins_name].drop(margins_name)
-        index_margin = table.loc[margins_name, :].drop(margins_name)
-        table = table.drop(margins_name, axis=1).drop(margins_name)
-        # to keep index and columns names
-        table_index_names = table.index.names
-        table_columns_names = table.columns.names
+        # keep index and column of pivoted table
+        table_index = table.index
+        table_columns = table.columns
+
+        # check if margin name is in (for MI cases) or equal to last
+        # index/column and save the column and index margin
+        if (margins_name not in table.iloc[-1, :].name) | (
+            margins_name != table.iloc[:, -1].name
+        ):
+            raise ValueError(""{} not in pivoted DataFrame"".format(margins_name))
+        column_margin = table.iloc[:-1, -1]
+        index_margin = table.iloc[-1, :-1]
+
+        # keep the core table
+        table = table.iloc[:-1, :-1]
 
         # Normalize core
         table = _normalize(table, normalize=normalize, margins=False)
@@ -627,11 +635,13 @@ def _normalize(table, normalize, margins, margins_name=""All""):
             column_margin = column_margin / column_margin.sum()
             table = concat([table, column_margin], axis=1)
             table = table.fillna(0)
+            table.columns = table_columns
 
         elif normalize == ""index"":
             index_margin = index_margin / index_margin.sum()
             table = table.append(index_margin)
             table = table.fillna(0)
+            table.index = table_index
 
         elif normalize == ""all"" or normalize is True:
             column_margin = column_margin / column_margin.sum()
@@ -641,13 +651,12 @@ def _normalize(table, normalize, margins, margins_name=""All""):
             table = table.append(index_margin)
 
             table = table.fillna(0)
+            table.index = table_index
+            table.columns = table_columns
 
         else:
             raise ValueError(""Not a valid normalize argument"")
 
-        table.index.names = table_index_names
-        table.columns.names = table_columns_names
-
     else:
         raise ValueError(""Not a valid margins argument"")
 
"
"pandas","7","27f365d","64336ff8414f8977ff94adb9a5bc000a3a4ef454","pandas/core/indexes/base.py","pandas/core/indexes/base.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py","pandas/tests/frame/indexing/test_indexing.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index d8b645746..6a7551391 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -3073,9 +3073,8 @@ class Index(IndexOpsMixin, PandasObject):
         left_indexer = self.get_indexer(target, ""pad"", limit=limit)
         right_indexer = self.get_indexer(target, ""backfill"", limit=limit)
 
-        target = np.asarray(target)
-        left_distances = abs(self.values[left_indexer] - target)
-        right_distances = abs(self.values[right_indexer] - target)
+        left_distances = np.abs(self[left_indexer] - target)
+        right_distances = np.abs(self[right_indexer] - target)
 
         op = operator.lt if self.is_monotonic_increasing else operator.le
         indexer = np.where(
"
"pandas","114","8f0310a","9a222ea0300053ff46da984e3b3f68622ccba9c3","pandas/core/indexes/base.py","pandas/core/indexes/base.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py","pandas/tests/extension/decimal/test_decimal.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index b99e60f8c..87c110f95 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -69,6 +69,7 @@ import pandas.core.algorithms as algos
 from pandas.core.arrays import ExtensionArray
 from pandas.core.base import IndexOpsMixin, PandasObject
 import pandas.core.common as com
+from pandas.core.construction import extract_array
 from pandas.core.indexers import maybe_convert_indices
 from pandas.core.indexes.frozen import FrozenList
 import pandas.core.missing as missing
@@ -4489,22 +4490,26 @@ class Index(IndexOpsMixin, PandasObject):
         # if we have something that is Index-like, then
         # use this, e.g. DatetimeIndex
         # Things like `Series._get_value` (via .at) pass the EA directly here.
-        s = getattr(series, ""_values"", series)
-        if isinstance(s, (ExtensionArray, Index)) and is_scalar(key):
-            # GH 20882, 21257
-            # Unify Index and ExtensionArray treatment
-            # First try to convert the key to a location
-            # If that fails, raise a KeyError if an integer
-            # index, otherwise, see if key is an integer, and
-            # try that
-            try:
-                iloc = self.get_loc(key)
-                return s[iloc]
-            except KeyError:
-                if len(self) > 0 and (self.holds_integer() or self.is_boolean()):
-                    raise
-                elif is_integer(key):
-                    return s[key]
+        s = extract_array(series, extract_numpy=True)
+        if isinstance(s, ExtensionArray):
+            if is_scalar(key):
+                # GH 20882, 21257
+                # First try to convert the key to a location
+                # If that fails, raise a KeyError if an integer
+                # index, otherwise, see if key is an integer, and
+                # try that
+                try:
+                    iloc = self.get_loc(key)
+                    return s[iloc]
+                except KeyError:
+                    if len(self) > 0 and (self.holds_integer() or self.is_boolean()):
+                        raise
+                    elif is_integer(key):
+                        return s[key]
+            else:
+                # if key is not a scalar, directly raise an error (the code below
+                # would convert to numpy arrays and raise later any way) - GH29926
+                raise InvalidIndexError(key)
 
         s = com.values_from_object(series)
         k = com.values_from_object(key)
"
"pandas","25","ecc3b2e","73d614403759831814ef7ab83ef1e4aaa645b33a","pandas/core/arrays/datetimes.py","pandas/core/arrays/datetimes.py","diff --git a/pandas/core/arrays/datetimes.py b/pandas/core/arrays/datetimes.py","pandas/tests/indexes/datetimes/test_misc.py","","diff --git a/pandas/core/arrays/datetimes.py b/pandas/core/arrays/datetimes.py
index f5cc0817e..dd553011c 100644
--- a/pandas/core/arrays/datetimes.py
+++ b/pandas/core/arrays/datetimes.py
@@ -1277,7 +1277,11 @@ default 'raise'
         """"""
         from pandas import DataFrame
 
-        sarray = fields.build_isocalendar_sarray(self.asi8)
+        if self.tz is not None and not timezones.is_utc(self.tz):
+            values = self._local_timestamps()
+        else:
+            values = self.asi8
+        sarray = fields.build_isocalendar_sarray(values)
         iso_calendar_df = DataFrame(
             sarray, columns=[""year"", ""week"", ""day""], dtype=""UInt32""
         )
"
"pandas","63","e5c65bf","e1ca66bae38b8026079dfcbe0edad5f278546608","pandas/core/indexing.py","pandas/core/indexing.py","diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py","pandas/tests/indexing/test_scalar.py","","diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index 3e468fb56..f55a54a54 100755
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -2087,21 +2087,11 @@ class _AtIndexer(_ScalarAccessIndexer):
         if is_setter:
             return list(key)
 
-        for ax, i in zip(self.obj.axes, key):
-            if ax.is_integer():
-                if not is_integer(i):
-                    raise ValueError(
-                        ""At based indexing on an integer index ""
-                        ""can only have integer indexers""
-                    )
-            else:
-                if is_integer(i) and not (ax.holds_integer() or ax.is_floating()):
-                    raise ValueError(
-                        ""At based indexing on an non-integer ""
-                        ""index can only have non-integer ""
-                        ""indexers""
-                    )
-        return key
+        lkey = list(key)
+        for n, (ax, i) in enumerate(zip(self.obj.axes, key)):
+            lkey[n] = ax._convert_scalar_indexer(i, kind=""loc"")
+
+        return tuple(lkey)
 
 
 @Appender(IndexingMixin.iat.__doc__)
"
"pandas","74","9a211aa","839e7f1416148caff518a5b75327a2480a2bbbb4","pandas/core/indexes/timedeltas.py","pandas/core/indexes/timedeltas.py","diff --git a/pandas/core/indexes/timedeltas.py b/pandas/core/indexes/timedeltas.py","pandas/tests/indexes/timedeltas/test_constructors.py","","diff --git a/pandas/core/indexes/timedeltas.py b/pandas/core/indexes/timedeltas.py
index d0a31b682..e78714487 100644
--- a/pandas/core/indexes/timedeltas.py
+++ b/pandas/core/indexes/timedeltas.py
@@ -163,7 +163,7 @@ class TimedeltaIndex(
                 ""represent unambiguous timedelta values durations.""
             )
 
-        if isinstance(data, TimedeltaArray):
+        if isinstance(data, TimedeltaArray) and freq is None:
             if copy:
                 data = data.copy()
             return cls._simple_new(data, name=name, freq=freq)
"
"pandas","140","3b19e1d","4375daffeed16531bae3fdaf85324b590d1dcb59","pandas/core/groupby/generic.py","pandas/core/groupby/generic.py","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py","pandas/tests/groupby/test_apply.py","","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py
index b5aec1897..e556708dc 100644
--- a/pandas/core/groupby/generic.py
+++ b/pandas/core/groupby/generic.py
@@ -1913,7 +1913,9 @@ def _recast_datetimelike_result(result: DataFrame) -> DataFrame:
     result = result.copy()
 
     obj_cols = [
-        idx for idx in range(len(result.columns)) if is_object_dtype(result.dtypes[idx])
+        idx
+        for idx in range(len(result.columns))
+        if is_object_dtype(result.dtypes.iloc[idx])
     ]
 
     # See GH#26285
"
"pandas","113","b164624","8705aad961dd227d38ff93a39697547b98109c9d","pandas/conftest.py;pandas/core/arrays/integer.py","pandas/conftest.py;pandas/core/arrays/integer.py","diff --git a/pandas/conftest.py b/pandas/conftest.py;diff --git a/pandas/core/arrays/integer.py b/pandas/core/arrays/integer.py","pandas/tests/extension/test_integer.py","","diff --git a/pandas/conftest.py b/pandas/conftest.py
index 3553a411a..6b43bf58b 100644
--- a/pandas/conftest.py
+++ b/pandas/conftest.py
@@ -654,6 +654,24 @@ def any_int_dtype(request):
     return request.param
 
 
+@pytest.fixture(params=ALL_EA_INT_DTYPES)
+def any_nullable_int_dtype(request):
+    """"""
+    Parameterized fixture for any nullable integer dtype.
+
+    * 'UInt8'
+    * 'Int8'
+    * 'UInt16'
+    * 'Int16'
+    * 'UInt32'
+    * 'Int32'
+    * 'UInt64'
+    * 'Int64'
+    """"""
+
+    return request.param
+
+
 @pytest.fixture(params=ALL_REAL_DTYPES)
 def any_real_dtype(request):
     """"""
diff --git a/pandas/core/arrays/integer.py b/pandas/core/arrays/integer.py
index 2bfb53aa1..08a3eca1e 100644
--- a/pandas/core/arrays/integer.py
+++ b/pandas/core/arrays/integer.py
@@ -26,6 +26,7 @@ from pandas.core.dtypes.missing import isna, notna
 from pandas.core import nanops, ops
 from pandas.core.algorithms import take
 from pandas.core.arrays import ExtensionArray, ExtensionOpsMixin
+from pandas.core.ops import invalid_comparison
 from pandas.core.ops.common import unpack_zerodim_and_defer
 from pandas.core.tools.numeric import to_numeric
 
@@ -646,7 +647,11 @@ class IntegerArray(ExtensionArray, ExtensionOpsMixin):
             with warnings.catch_warnings():
                 warnings.filterwarnings(""ignore"", ""elementwise"", FutureWarning)
                 with np.errstate(all=""ignore""):
-                    result = op(self._data, other)
+                    method = getattr(self._data, f""__{op_name}__"")
+                    result = method(other)
+
+                    if result is NotImplemented:
+                        result = invalid_comparison(self._data, other, op)
 
             # nans propagate
             if mask is None:
"
"pandas","62","46a77f6","74dad82827e9b13552df2d6d3fbbeb901821b53f","pandas/core/internals/blocks.py;pandas/io/stata.py","pandas/core/internals/blocks.py;pandas/io/stata.py","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py;diff --git a/pandas/io/stata.py b/pandas/io/stata.py","pandas/tests/indexing/test_indexing.py","","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index 85a261792..536aa53c9 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -830,6 +830,9 @@ class Block(PandasObject):
         """"""
         transpose = self.ndim == 2
 
+        if isinstance(indexer, np.ndarray) and indexer.ndim > self.ndim:
+            raise ValueError(f""Cannot set values with ndim > {self.ndim}"")
+
         # coerce None values, if appropriate
         if value is None:
             if self.is_numeric:
diff --git a/pandas/io/stata.py b/pandas/io/stata.py
index 06bf906be..d651fe9f6 100644
--- a/pandas/io/stata.py
+++ b/pandas/io/stata.py
@@ -1678,6 +1678,10 @@ the string values returned are correct.""""""
                     missing_value = StataMissingValue(um)
 
                     loc = missing_loc[umissing_loc == j]
+                    if loc.ndim == 2 and loc.shape[1] == 1:
+                        # GH#31813 avoid trying to set Series values with wrong
+                        #  dimension
+                        loc = loc[:, 0]
                     replacement.iloc[loc] = missing_value
             else:  # All replacements are identical
                 dtype = series.dtype
"
"pandas","10","de8ca78","e1ee2b0679e5999c993a787606d30e75faaba7a2","pandas/core/internals/blocks.py","pandas/core/internals/blocks.py","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py","pandas/tests/series/methods/test_update.py","","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index d028d8404..e4dcffae4 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -1599,7 +1599,7 @@ class ExtensionBlock(Block):
 
         new_values = self.values if inplace else self.values.copy()
 
-        if isinstance(new, np.ndarray) and len(new) == len(mask):
+        if isinstance(new, (np.ndarray, ExtensionArray)) and len(new) == len(mask):
             new = new[mask]
 
         mask = _safe_reshape(mask, new_values.shape)
"
"pandas","110","cceef8e","96bb151fe1a5b812ecab400adcd297d14fd0e0e4","pandas/core/indexes/base.py;pandas/core/indexes/category.py","pandas/core/indexes/base.py;pandas/core/indexes/category.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py;diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py","pandas/tests/indexing/test_categorical.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index ba0f771e1..fc2412cea 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -2982,7 +2982,9 @@ class Index(IndexOpsMixin, PandasObject):
 
         is_null_slicer = start is None and stop is None
         is_index_slice = is_int(start) and is_int(stop)
-        is_positional = is_index_slice and not self.is_integer()
+        is_positional = is_index_slice and not (
+            self.is_integer() or self.is_categorical()
+        )
 
         if kind == ""getitem"":
             """"""
diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py
index 2cc853ecf..dc1cbb601 100644
--- a/pandas/core/indexes/category.py
+++ b/pandas/core/indexes/category.py
@@ -753,6 +753,13 @@ class CategoricalIndex(Index, accessor.PandasDelegate):
 
     take_nd = take
 
+    @Appender(_index_shared_docs[""_maybe_cast_slice_bound""])
+    def _maybe_cast_slice_bound(self, label, side, kind):
+        if kind == ""loc"":
+            return label
+
+        return super()._maybe_cast_slice_bound(label, side, kind)
+
     def map(self, mapper):
         """"""
         Map values using input correspondence (a dict, Series, or function).
"
"pandas","13","08f9bd2","91150d976ac41bd93a0e6516b2090c534f91aff2","pandas/core/dtypes/missing.py","pandas/core/dtypes/missing.py","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py","pandas/tests/arrays/categorical/test_missing.py","","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py
index d329f4337..92e1b17c4 100644
--- a/pandas/core/dtypes/missing.py
+++ b/pandas/core/dtypes/missing.py
@@ -134,13 +134,13 @@ def _isna_new(obj):
     elif isinstance(obj, type):
         return False
     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
-        return _isna_ndarraylike(obj)
+        return _isna_ndarraylike(obj, old=False)
     elif isinstance(obj, ABCDataFrame):
         return obj.isna()
     elif isinstance(obj, list):
-        return _isna_ndarraylike(np.asarray(obj, dtype=object))
+        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=False)
     elif hasattr(obj, ""__array__""):
-        return _isna_ndarraylike(np.asarray(obj))
+        return _isna_ndarraylike(np.asarray(obj), old=False)
     else:
         return False
 
@@ -165,13 +165,13 @@ def _isna_old(obj):
     elif isinstance(obj, type):
         return False
     elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
-        return _isna_ndarraylike_old(obj)
+        return _isna_ndarraylike(obj, old=True)
     elif isinstance(obj, ABCDataFrame):
         return obj.isna()
     elif isinstance(obj, list):
-        return _isna_ndarraylike_old(np.asarray(obj, dtype=object))
+        return _isna_ndarraylike(np.asarray(obj, dtype=object), old=True)
     elif hasattr(obj, ""__array__""):
-        return _isna_ndarraylike_old(np.asarray(obj))
+        return _isna_ndarraylike(np.asarray(obj), old=True)
     else:
         return False
 
@@ -207,40 +207,40 @@ def _use_inf_as_na(key):
         globals()[""_isna""] = _isna_new
 
 
-def _isna_ndarraylike(obj):
-    values = getattr(obj, ""_values"", obj)
-    dtype = values.dtype
-
-    if is_extension_array_dtype(dtype):
-        result = values.isna()
-    elif is_string_dtype(dtype):
-        result = _isna_string_dtype(values, dtype, old=False)
-
-    elif needs_i8_conversion(dtype):
-        # this is the NaT pattern
-        result = values.view(""i8"") == iNaT
-    else:
-        result = np.isnan(values)
-
-    # box
-    if isinstance(obj, ABCSeries):
-        result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)
-
-    return result
+def _isna_ndarraylike(obj, old: bool = False):
+    """"""
+    Return an array indicating which values of the input array are NaN / NA.
 
+    Parameters
+    ----------
+    obj: array-like
+        The input array whose elements are to be checked.
+    old: bool
+        Whether or not to treat infinite values as NA.
 
-def _isna_ndarraylike_old(obj):
+    Returns
+    -------
+    array-like
+        Array of boolean values denoting the NA status of each element.
+    """"""
     values = getattr(obj, ""_values"", obj)
     dtype = values.dtype
 
-    if is_string_dtype(dtype):
-        result = _isna_string_dtype(values, dtype, old=True)
-
+    if is_extension_array_dtype(dtype):
+        if old:
+            result = values.isna() | (values == -np.inf) | (values == np.inf)
+        else:
+            result = values.isna()
+    elif is_string_dtype(dtype):
+        result = _isna_string_dtype(values, dtype, old=old)
     elif needs_i8_conversion(dtype):
         # this is the NaT pattern
         result = values.view(""i8"") == iNaT
     else:
-        result = ~np.isfinite(values)
+        if old:
+            result = ~np.isfinite(values)
+        else:
+            result = np.isnan(values)
 
     # box
     if isinstance(obj, ABCSeries):
"
"pandas","47","1a5b11d","810a4e5b19616efb503767b4518083c9a59c11e6","pandas/core/frame.py;pandas/core/indexing.py","pandas/core/frame.py;pandas/core/indexing.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py;diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py","pandas/tests/frame/indexing/test_indexing.py;pandas/tests/indexing/test_loc.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index b0909e23b..179857da4 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -2687,6 +2687,7 @@ class DataFrame(NDFrame):
                 for k1, k2 in zip(key, value.columns):
                     self[k1] = value[k2]
             else:
+                self.loc._ensure_listlike_indexer(key, axis=1)
                 indexer = self.loc._get_listlike_indexer(
                     key, axis=1, raise_missing=False
                 )[1]
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index c9362a052..7bd25814a 100755
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -8,6 +8,7 @@ from pandas.errors import AbstractMethodError
 from pandas.util._decorators import Appender
 
 from pandas.core.dtypes.common import (
+    is_hashable,
     is_integer,
     is_iterator,
     is_list_like,
@@ -581,6 +582,9 @@ class _LocationIndexer(_NDFrameIndexerBase):
         """"""
         Convert a potentially-label-based key into a positional indexer.
         """"""
+        if self.name == ""loc"":
+            self._ensure_listlike_indexer(key)
+
         if self.axis is not None:
             return self._convert_tuple(key, is_setter=True)
 
@@ -611,6 +615,42 @@ class _LocationIndexer(_NDFrameIndexerBase):
                 raise
             raise IndexingError(key) from e
 
+    def _ensure_listlike_indexer(self, key, axis=None):
+        """"""
+        Ensure that a list-like of column labels are all present by adding them if
+        they do not already exist.
+
+        Parameters
+        ----------
+        key : _LocIndexer key or list-like of column labels
+            Target labels.
+        axis : key axis if known
+        """"""
+        column_axis = 1
+
+        # column only exists in 2-dimensional DataFrame
+        if self.ndim != 2:
+            return
+
+        if isinstance(key, tuple):
+            # key may be a tuple if key is a _LocIndexer key
+            # in that case, set key to the column part of key
+            key = key[column_axis]
+            axis = column_axis
+
+        if (
+            axis == column_axis
+            and not isinstance(self.obj.columns, ABCMultiIndex)
+            and is_list_like_indexer(key)
+            and not com.is_bool_indexer(key)
+            and all(is_hashable(k) for k in key)
+        ):
+            for k in key:
+                try:
+                    self.obj[k]
+                except KeyError:
+                    self.obj[k] = np.nan
+
     def __setitem__(self, key, value):
         if isinstance(key, tuple):
             key = tuple(com.apply_if_callable(x, self.obj) for x in key)
"
"pandas","82","6f395ad","e83a6bddac8c89b144dfe0783594dd332c5b3030","pandas/core/internals/concat.py","pandas/core/internals/concat.py","diff --git a/pandas/core/internals/concat.py b/pandas/core/internals/concat.py","pandas/tests/reshape/merge/test_merge.py","","diff --git a/pandas/core/internals/concat.py b/pandas/core/internals/concat.py
index c6f30ef65..c75373b82 100644
--- a/pandas/core/internals/concat.py
+++ b/pandas/core/internals/concat.py
@@ -350,7 +350,7 @@ def _get_empty_dtype_and_na(join_units):
         dtype = upcast_classes[""datetimetz""]
         return dtype[0], tslibs.NaT
     elif ""datetime"" in upcast_classes:
-        return np.dtype(""M8[ns]""), tslibs.iNaT
+        return np.dtype(""M8[ns]""), np.datetime64(""NaT"", ""ns"")
     elif ""timedelta"" in upcast_classes:
         return np.dtype(""m8[ns]""), np.timedelta64(""NaT"", ""ns"")
     else:  # pragma
"
"pandas","103","d1f82f7","19578e364fb47ce10dd14174cffc3ecfea1a58cd","pandas/core/groupby/generic.py;pandas/core/groupby/groupby.py","pandas/core/groupby/generic.py;pandas/core/groupby/groupby.py","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py;diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py","pandas/tests/groupby/test_transform.py","","diff --git a/pandas/core/groupby/generic.py b/pandas/core/groupby/generic.py
index 6b110a0c8..be94fa548 100644
--- a/pandas/core/groupby/generic.py
+++ b/pandas/core/groupby/generic.py
@@ -809,6 +809,9 @@ class SeriesGroupBy(GroupBy):
                     periods=periods, fill_method=fill_method, limit=limit, freq=freq
                 )
             )
+        if fill_method is None:  # GH30463
+            fill_method = ""pad""
+            limit = 0
         filled = getattr(self, fill_method)(limit=limit)
         fill_grp = filled.groupby(self.grouper.codes)
         shifted = fill_grp.shift(periods=periods, freq=freq)
diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py
index 0edf1c957..81a914531 100644
--- a/pandas/core/groupby/groupby.py
+++ b/pandas/core/groupby/groupby.py
@@ -2362,6 +2362,9 @@ class GroupBy(_GroupBy):
                     axis=axis,
                 )
             )
+        if fill_method is None:  # GH30463
+            fill_method = ""pad""
+            limit = 0
         filled = getattr(self, fill_method)(limit=limit)
         fill_grp = filled.groupby(self.grouper.codes)
         shifted = fill_grp.shift(periods=periods, freq=freq)
"
"pandas","70","a05e6c9","06ef193a5c1957c0a76e3e88bc7b834b38972c39","pandas/core/groupby/groupby.py;pandas/core/groupby/ops.py;pandas/tests/groupby/aggregate/test_aggregate.py;pandas/tests/resample/test_datetime_index.py;pandas/tests/resample/test_timedelta.py","pandas/core/groupby/groupby.py;pandas/core/groupby/ops.py;pandas/tests/groupby/aggregate/test_aggregate.py;pandas/tests/resample/test_datetime_index.py;pandas/tests/resample/test_timedelta.py","diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py;diff --git a/pandas/core/groupby/ops.py b/pandas/core/groupby/ops.py;diff --git a/pandas/tests/groupby/aggregate/test_aggregate.py b/pandas/tests/groupby/aggregate/test_aggregate.py;diff --git a/pandas/tests/resample/test_datetime_index.py b/pandas/tests/resample/test_datetime_index.py;diff --git a/pandas/tests/resample/test_timedelta.py b/pandas/tests/resample/test_timedelta.py","pandas/tests/groupby/test_categorical.py","","diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py
index aa21aa452..02e938331 100644
--- a/pandas/core/groupby/groupby.py
+++ b/pandas/core/groupby/groupby.py
@@ -813,9 +813,10 @@ b  2"""""",
                 # datetime64tz is handled correctly in agg_series,
                 #  so is excluded here.
 
-                # return the same type (Series) as our caller
-                cls = dtype.construct_array_type()
-                result = try_cast_to_ea(cls, result, dtype=dtype)
+                if len(result) and isinstance(result[0], dtype.type):
+                    cls = dtype.construct_array_type()
+                    result = try_cast_to_ea(cls, result, dtype=dtype)
+
             elif numeric_only and is_numeric_dtype(dtype) or not numeric_only:
                 result = maybe_downcast_to_dtype(result, dtype)
 
diff --git a/pandas/core/groupby/ops.py b/pandas/core/groupby/ops.py
index 679d36685..2e95daa39 100644
--- a/pandas/core/groupby/ops.py
+++ b/pandas/core/groupby/ops.py
@@ -543,6 +543,17 @@ class BaseGrouper:
             if mask.any():
                 result = result.astype(""float64"")
                 result[mask] = np.nan
+        elif (
+            how == ""add""
+            and is_integer_dtype(orig_values.dtype)
+            and is_extension_array_dtype(orig_values.dtype)
+        ):
+            # We need this to ensure that Series[Int64Dtype].resample().sum()
+            # remains int64 dtype.
+            # Two options for avoiding this special case
+            # 1. mask-aware ops and avoid casting to float with NaN above
+            # 2. specify the result dtype when calling this method
+            result = result.astype(""int64"")
 
         if kind == ""aggregate"" and self._filter_empty_groups and not counts.all():
             assert result.ndim != 2
diff --git a/pandas/tests/groupby/aggregate/test_aggregate.py b/pandas/tests/groupby/aggregate/test_aggregate.py
index 67bdcc246..2d31996a8 100644
--- a/pandas/tests/groupby/aggregate/test_aggregate.py
+++ b/pandas/tests/groupby/aggregate/test_aggregate.py
@@ -663,6 +663,27 @@ def test_aggregate_mixed_types():
     tm.assert_frame_equal(result, expected)
 
 
+@pytest.mark.xfail(reason=""Not implemented."")
+def test_aggregate_udf_na_extension_type():
+    # https://github.com/pandas-dev/pandas/pull/31359
+    # This is currently failing to cast back to Int64Dtype.
+    # The presence of the NA causes two problems
+    # 1. NA is not an instance of Int64Dtype.type (numpy.int64)
+    # 2. The presence of an NA forces object type, so the non-NA values is
+    #    a Python int rather than a NumPy int64. Python ints aren't
+    #    instances of numpy.int64.
+    def aggfunc(x):
+        if all(x > 2):
+            return 1
+        else:
+            return pd.NA
+
+    df = pd.DataFrame({""A"": pd.array([1, 2, 3])})
+    result = df.groupby([1, 1, 2]).agg(aggfunc)
+    expected = pd.DataFrame({""A"": pd.array([1, pd.NA], dtype=""Int64"")}, index=[1, 2])
+    tm.assert_frame_equal(result, expected)
+
+
 class TestLambdaMangling:
     def test_basic(self):
         df = pd.DataFrame({""A"": [0, 0, 1, 1], ""B"": [1, 2, 3, 4]})
diff --git a/pandas/tests/resample/test_datetime_index.py b/pandas/tests/resample/test_datetime_index.py
index 486032971..3ad82b9e0 100644
--- a/pandas/tests/resample/test_datetime_index.py
+++ b/pandas/tests/resample/test_datetime_index.py
@@ -122,7 +122,9 @@ def test_resample_integerarray():
 
     result = ts.resample(""3T"").mean()
     expected = Series(
-        [1, 4, 7], index=pd.date_range(""1/1/2000"", periods=3, freq=""3T""), dtype=""Int64""
+        [1, 4, 7],
+        index=pd.date_range(""1/1/2000"", periods=3, freq=""3T""),
+        dtype=""float64"",
     )
     tm.assert_series_equal(result, expected)
 
diff --git a/pandas/tests/resample/test_timedelta.py b/pandas/tests/resample/test_timedelta.py
index d1bcdc55c..a4d14f127 100644
--- a/pandas/tests/resample/test_timedelta.py
+++ b/pandas/tests/resample/test_timedelta.py
@@ -105,7 +105,7 @@ def test_resample_categorical_data_with_timedeltaindex():
         index=pd.to_timedelta([0, 10], unit=""s""),
     )
     expected = expected.reindex([""Group_obj"", ""Group""], axis=1)
-    expected[""Group""] = expected[""Group_obj""].astype(""category"")
+    expected[""Group""] = expected[""Group_obj""]
     tm.assert_frame_equal(result, expected)
 
 
"
"pandas","151","6110608","5a227a410c520ceec2d94369a44e2ab774a40dc3","pandas/core/arrays/numpy_.py","pandas/core/arrays/numpy_.py","diff --git a/pandas/core/arrays/numpy_.py b/pandas/core/arrays/numpy_.py","pandas/tests/arrays/test_numpy.py","","diff --git a/pandas/core/arrays/numpy_.py b/pandas/core/arrays/numpy_.py
index 4e2e37d88..32da0199e 100644
--- a/pandas/core/arrays/numpy_.py
+++ b/pandas/core/arrays/numpy_.py
@@ -235,15 +235,8 @@ class PandasArray(ExtensionArray, ExtensionOpsMixin, NDArrayOperatorsMixin):
         if not lib.is_scalar(value):
             value = np.asarray(value)
 
-        values = self._ndarray
-        t = np.result_type(value, values)
-        if t != self._ndarray.dtype:
-            values = values.astype(t, casting=""safe"")
-            values[key] = value
-            self._dtype = PandasDtype(t)
-            self._ndarray = values
-        else:
-            self._ndarray[key] = value
+        value = np.asarray(value, dtype=self._ndarray.dtype)
+        self._ndarray[key] = value
 
     def __len__(self) -> int:
         return len(self._ndarray)
"
"pandas","111","28715a7","27836e93dc3c9d55c60282ccb15c88c42a340d87","pandas/core/indexes/base.py;pandas/core/indexes/category.py","pandas/core/indexes/base.py;pandas/core/indexes/category.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py;diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py","pandas/tests/indexing/test_categorical.py;pandas/tests/indexing/test_floats.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index 803c29c32..ba0f771e1 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -2939,11 +2939,11 @@ class Index(IndexOpsMixin, PandasObject):
                     ""unicode"",
                     ""mixed"",
                 ]:
-                    return self._invalid_indexer(""label"", key)
+                    self._invalid_indexer(""label"", key)
 
             elif kind in [""loc""] and is_integer(key):
                 if not self.holds_integer():
-                    return self._invalid_indexer(""label"", key)
+                    self._invalid_indexer(""label"", key)
 
         return key
 
diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py
index d061f61ef..2cc853ecf 100644
--- a/pandas/core/indexes/category.py
+++ b/pandas/core/indexes/category.py
@@ -696,9 +696,11 @@ class CategoricalIndex(Index, accessor.PandasDelegate):
 
     @Appender(_index_shared_docs[""_convert_scalar_indexer""])
     def _convert_scalar_indexer(self, key, kind=None):
-        if self.categories._defer_to_indexing:
-            return self.categories._convert_scalar_indexer(key, kind=kind)
-
+        if kind == ""loc"":
+            try:
+                return self.categories._convert_scalar_indexer(key, kind=kind)
+            except TypeError:
+                self._invalid_indexer(""label"", key)
         return super()._convert_scalar_indexer(key, kind=kind)
 
     @Appender(_index_shared_docs[""_convert_list_indexer""])
"
"pandas","123","b6d64d2","17fe9a467581ca39f44c89876ebd0d38b9ca77ea","pandas/core/indexes/base.py;pandas/core/indexes/numeric.py;pandas/core/indexes/range.py","pandas/core/indexes/base.py;pandas/core/indexes/numeric.py;pandas/core/indexes/range.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py;diff --git a/pandas/core/indexes/numeric.py b/pandas/core/indexes/numeric.py;diff --git a/pandas/core/indexes/range.py b/pandas/core/indexes/range.py","pandas/tests/indexes/test_numeric.py;pandas/tests/indexes/test_range.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index 86664a14e..8978a0982 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -381,7 +381,7 @@ class Index(IndexOpsMixin, PandasObject):
                             pass
 
                         # Return an actual float index.
-                        return Float64Index(data, copy=copy, dtype=dtype, name=name)
+                        return Float64Index(data, copy=copy, name=name)
 
                     elif inferred == ""string"":
                         pass
diff --git a/pandas/core/indexes/numeric.py b/pandas/core/indexes/numeric.py
index 074cce085..29f56259d 100644
--- a/pandas/core/indexes/numeric.py
+++ b/pandas/core/indexes/numeric.py
@@ -15,6 +15,8 @@ from pandas.core.dtypes.common import (
     is_float_dtype,
     is_integer_dtype,
     is_scalar,
+    is_signed_integer_dtype,
+    is_unsigned_integer_dtype,
     needs_i8_conversion,
     pandas_dtype,
 )
@@ -27,6 +29,7 @@ from pandas.core.dtypes.generic import (
 )
 from pandas.core.dtypes.missing import isna
 
+from pandas._typing import Dtype
 from pandas.core import algorithms
 import pandas.core.common as com
 from pandas.core.indexes.base import Index, InvalidIndexError, _index_shared_docs
@@ -45,7 +48,7 @@ class NumericIndex(Index):
     _is_numeric_dtype = True
 
     def __new__(cls, data=None, dtype=None, copy=False, name=None, fastpath=None):
-
+        cls._validate_dtype(dtype)
         if fastpath is not None:
             warnings.warn(
                 ""The 'fastpath' keyword is deprecated, and will be ""
@@ -80,6 +83,22 @@ class NumericIndex(Index):
             name = data.name
         return cls._simple_new(subarr, name=name)
 
+    @classmethod
+    def _validate_dtype(cls, dtype: Dtype) -> None:
+        if dtype is None:
+            return
+        validation_metadata = {
+            ""int64index"": (is_signed_integer_dtype, ""signed integer""),
+            ""uint64index"": (is_unsigned_integer_dtype, ""unsigned integer""),
+            ""float64index"": (is_float_dtype, ""float""),
+            ""rangeindex"": (is_signed_integer_dtype, ""signed integer""),
+        }
+
+        validation_func, expected = validation_metadata[cls._typ]
+        if not validation_func(dtype):
+            msg = f""Incorrect `dtype` passed: expected {expected}, received {dtype}""
+            raise ValueError(msg)
+
     @Appender(_index_shared_docs[""_maybe_cast_slice_bound""])
     def _maybe_cast_slice_bound(self, label, side, kind):
         assert kind in [""ix"", ""loc"", ""getitem"", None]
diff --git a/pandas/core/indexes/range.py b/pandas/core/indexes/range.py
index d200ff6a7..6f677848b 100644
--- a/pandas/core/indexes/range.py
+++ b/pandas/core/indexes/range.py
@@ -14,7 +14,6 @@ from pandas.util._decorators import Appender, cache_readonly
 from pandas.core.dtypes.common import (
     ensure_platform_int,
     ensure_python_int,
-    is_int64_dtype,
     is_integer,
     is_integer_dtype,
     is_list_like,
@@ -165,12 +164,6 @@ class RangeIndex(Int64Index):
 
     # --------------------------------------------------------------------
 
-    @staticmethod
-    def _validate_dtype(dtype):
-        """""" require dtype to be None or int64 """"""
-        if not (dtype is None or is_int64_dtype(dtype)):
-            raise TypeError(""Invalid to pass a non-int64 dtype to RangeIndex"")
-
     @cache_readonly
     def _constructor(self):
         """""" return the class to use for construction """"""
"
"pandas","41","1b49f69","d4273353bc512e3b4e79c361b879633f33ec7289","pandas/core/internals/blocks.py","pandas/core/internals/blocks.py","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py","pandas/tests/indexing/test_iloc.py;pandas/tests/internals/test_internals.py","","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index ce3c34d33..f2f8b6067 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -11,6 +11,7 @@ from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers
 import pandas._libs.internals as libinternals
 from pandas._libs.tslibs import Timedelta, conversion
 from pandas._libs.tslibs.timezones import tz_compare
+from pandas._typing import ArrayLike
 from pandas.util._validators import validate_bool_kwarg
 
 from pandas.core.dtypes.cast import (
@@ -340,11 +341,12 @@ class Block(PandasObject):
 
     def set(self, locs, values):
         """"""
-        Modify Block in-place with new item value
+        Modify block values in-place with new item value.
 
-        Returns
-        -------
-        None
+        Notes
+        -----
+        `set` never creates a new array or new Block, whereas `setitem` _may_
+        create a new array and always creates a new Block.
         """"""
         self.values[locs] = values
 
@@ -793,7 +795,7 @@ class Block(PandasObject):
 
     def setitem(self, indexer, value):
         """"""
-        Set the value inplace, returning a a maybe different typed block.
+        Attempt self.values[indexer] = value, possibly creating a new array.
 
         Parameters
         ----------
@@ -1633,12 +1635,15 @@ class ExtensionBlock(Block):
                 raise IndexError(f""{self} only contains one item"")
             return self.values
 
-    def should_store(self, value):
+    def should_store(self, value: ArrayLike) -> bool:
+        """"""
+        Can we set the given array-like value inplace?
+        """"""
         return isinstance(value, self._holder)
 
-    def set(self, locs, values, check=False):
+    def set(self, locs, values):
         assert locs.tolist() == [0]
-        self.values = values
+        self.values[:] = values
 
     def putmask(
         self, mask, new, align=True, inplace=False, axis=0, transpose=False,
@@ -1749,7 +1754,7 @@ class ExtensionBlock(Block):
 
     def setitem(self, indexer, value):
         """"""
-        Set the value inplace, returning a same-typed block.
+        Attempt self.values[indexer] = value, possibly creating a new array.
 
         This differs from Block.setitem by not allowing setitem to change
         the dtype of the Block.
@@ -2055,7 +2060,7 @@ class FloatBlock(FloatOrComplexBlock):
         )
         return formatter.get_result_as_array()
 
-    def should_store(self, value) -> bool:
+    def should_store(self, value: ArrayLike) -> bool:
         # when inserting a column should not coerce integers to floats
         # unnecessarily
         return issubclass(value.dtype.type, np.floating) and value.dtype == self.dtype
@@ -2073,7 +2078,7 @@ class ComplexBlock(FloatOrComplexBlock):
             element, (float, int, complex, np.float_, np.int_)
         ) and not isinstance(element, (bool, np.bool_))
 
-    def should_store(self, value) -> bool:
+    def should_store(self, value: ArrayLike) -> bool:
         return issubclass(value.dtype.type, np.complexfloating)
 
 
@@ -2092,7 +2097,7 @@ class IntBlock(NumericBlock):
             )
         return is_integer(element)
 
-    def should_store(self, value) -> bool:
+    def should_store(self, value: ArrayLike) -> bool:
         return is_integer_dtype(value) and value.dtype == self.dtype
 
 
@@ -2103,6 +2108,9 @@ class DatetimeLikeBlockMixin:
     def _holder(self):
         return DatetimeArray
 
+    def should_store(self, value):
+        return is_dtype_equal(self.dtype, value.dtype)
+
     @property
     def fill_value(self):
         return np.datetime64(""NaT"", ""ns"")
@@ -2239,16 +2247,9 @@ class DatetimeBlock(DatetimeLikeBlockMixin, Block):
         ).reshape(i8values.shape)
         return np.atleast_2d(result)
 
-    def should_store(self, value) -> bool:
-        return is_datetime64_dtype(value.dtype)
-
     def set(self, locs, values):
         """"""
-        Modify Block in-place with new item value
-
-        Returns
-        -------
-        None
+        See Block.set.__doc__
         """"""
         values = conversion.ensure_datetime64ns(values, copy=False)
 
@@ -2272,6 +2273,7 @@ class DatetimeTZBlock(ExtensionBlock, DatetimeBlock):
     _can_hold_element = DatetimeBlock._can_hold_element
     to_native_types = DatetimeBlock.to_native_types
     fill_value = np.datetime64(""NaT"", ""ns"")
+    should_store = DatetimeBlock.should_store
 
     @property
     def _holder(self):
@@ -2481,9 +2483,6 @@ class TimeDeltaBlock(DatetimeLikeBlockMixin, IntBlock):
             )
         return super().fillna(value, **kwargs)
 
-    def should_store(self, value) -> bool:
-        return is_timedelta64_dtype(value.dtype)
-
     def to_native_types(self, slicer=None, na_rep=None, quoting=None, **kwargs):
         """""" convert to our native types format, slicing if desired """"""
         values = self.values
@@ -2525,7 +2524,7 @@ class BoolBlock(NumericBlock):
             return issubclass(tipo.type, np.bool_)
         return isinstance(element, (bool, np.bool_))
 
-    def should_store(self, value) -> bool:
+    def should_store(self, value: ArrayLike) -> bool:
         return issubclass(value.dtype.type, np.bool_) and not is_extension_array_dtype(
             value
         )
@@ -2617,7 +2616,7 @@ class ObjectBlock(Block):
     def _can_hold_element(self, element: Any) -> bool:
         return True
 
-    def should_store(self, value) -> bool:
+    def should_store(self, value: ArrayLike) -> bool:
         return not (
             issubclass(
                 value.dtype.type,
@@ -2866,6 +2865,9 @@ class CategoricalBlock(ExtensionBlock):
     def _holder(self):
         return Categorical
 
+    def should_store(self, arr: ArrayLike):
+        return isinstance(arr, self._holder) and is_dtype_equal(self.dtype, arr.dtype)
+
     def to_native_types(self, slicer=None, na_rep="""", quoting=None, **kwargs):
         """""" convert to our native types format, slicing if desired """"""
         values = self.values
"
"pandas","152","eb8cce0","f61deb962ac0853595a43ad024c482b018d1792b","pandas/core/series.py","pandas/core/series.py","diff --git a/pandas/core/series.py b/pandas/core/series.py","pandas/tests/series/test_combine_concat.py","","diff --git a/pandas/core/series.py b/pandas/core/series.py
index 10d50e89c..922977bc0 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -2730,7 +2730,8 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         from pandas.core.reshape.concat import concat
 
         if isinstance(to_append, (list, tuple)):
-            to_concat = [self] + to_append
+            to_concat = [self]
+            to_concat.extend(to_append)
         else:
             to_concat = [self, to_append]
         return concat(
"
"pandas","79","38ea154","0b0cd08524e4472eb15835c2b91621dc0a6eeeb0","pandas/core/groupby/grouper.py;pandas/core/indexes/datetimes.py;pandas/core/indexes/multi.py;pandas/core/series.py","pandas/core/groupby/grouper.py;pandas/core/indexes/datetimes.py;pandas/core/indexes/multi.py;pandas/core/series.py","diff --git a/pandas/core/groupby/grouper.py b/pandas/core/groupby/grouper.py;diff --git a/pandas/core/indexes/datetimes.py b/pandas/core/indexes/datetimes.py;diff --git a/pandas/core/indexes/multi.py b/pandas/core/indexes/multi.py;diff --git a/pandas/core/series.py b/pandas/core/series.py","pandas/tests/indexes/datetimes/test_indexing.py","","diff --git a/pandas/core/groupby/grouper.py b/pandas/core/groupby/grouper.py
index 0b89e702c..f0c6eedf5 100644
--- a/pandas/core/groupby/grouper.py
+++ b/pandas/core/groupby/grouper.py
@@ -27,6 +27,7 @@ from pandas.core.frame import DataFrame
 from pandas.core.groupby import ops
 from pandas.core.groupby.categorical import recode_for_groupby, recode_from_groupby
 from pandas.core.indexes.api import CategoricalIndex, Index, MultiIndex
+from pandas.core.indexes.base import InvalidIndexError
 from pandas.core.series import Series
 
 from pandas.io.formats.printing import pprint_thing
@@ -565,7 +566,7 @@ def get_grouper(
             items = obj._data.items
             try:
                 items.get_loc(key)
-            except (KeyError, TypeError):
+            except (KeyError, TypeError, InvalidIndexError):
                 # TypeError shows up here if we pass e.g. Int64Index
                 return False
 
diff --git a/pandas/core/indexes/datetimes.py b/pandas/core/indexes/datetimes.py
index b1463f523..fbcca270b 100644
--- a/pandas/core/indexes/datetimes.py
+++ b/pandas/core/indexes/datetimes.py
@@ -27,7 +27,7 @@ from pandas.core.arrays.datetimes import (
     validate_tz_from_dtype,
 )
 import pandas.core.common as com
-from pandas.core.indexes.base import Index, maybe_extract_name
+from pandas.core.indexes.base import Index, InvalidIndexError, maybe_extract_name
 from pandas.core.indexes.datetimelike import (
     DatetimelikeDelegateMixin,
     DatetimeTimedeltaMixin,
@@ -641,6 +641,8 @@ class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):
         Fast lookup of value from 1-dimensional ndarray. Only use this if you
         know what you're doing
         """"""
+        if not is_scalar(key):
+            raise InvalidIndexError(key)
 
         if isinstance(key, (datetime, np.datetime64)):
             return self.get_value_maybe_box(series, key)
@@ -677,6 +679,9 @@ class DatetimeIndex(DatetimeTimedeltaMixin, DatetimeDelegateMixin):
         -------
         loc : int
         """"""
+        if not is_scalar(key):
+            raise InvalidIndexError(key)
+
         if is_valid_nat_for_dtype(key, self.dtype):
             key = NaT
 
diff --git a/pandas/core/indexes/multi.py b/pandas/core/indexes/multi.py
index a26a01ab7..704430fbb 100644
--- a/pandas/core/indexes/multi.py
+++ b/pandas/core/indexes/multi.py
@@ -2778,7 +2778,7 @@ class MultiIndex(Index):
                     indexer = self._get_level_indexer(key, level=level)
                     new_index = maybe_mi_droplevels(indexer, [0], drop_level)
                     return indexer, new_index
-            except TypeError:
+            except (TypeError, InvalidIndexError):
                 pass
 
             if not any(isinstance(k, slice) for k in key):
diff --git a/pandas/core/series.py b/pandas/core/series.py
index b92ac3731..d8eb98d06 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -980,6 +980,9 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
                 self[:] = value
             else:
                 self.loc[key] = value
+        except InvalidIndexError:
+            # e.g. slice
+            self._set_with(key, value)
 
         except TypeError as e:
             if isinstance(key, tuple) and not isinstance(self.index, MultiIndex):
"
"pandas","85","f1aaf62","29edd119d31a9ee7d4f89e8c1dc8af96f0c19dce","pandas/core/indexes/multi.py","pandas/core/indexes/multi.py","diff --git a/pandas/core/indexes/multi.py b/pandas/core/indexes/multi.py","pandas/tests/groupby/test_apply.py","","diff --git a/pandas/core/indexes/multi.py b/pandas/core/indexes/multi.py
index 8682af6ab..b684908c2 100644
--- a/pandas/core/indexes/multi.py
+++ b/pandas/core/indexes/multi.py
@@ -1256,6 +1256,10 @@ class MultiIndex(Index):
         if len(uniques) < len(level_index):
             # Remove unobserved levels from level_index
             level_index = level_index.take(uniques)
+        else:
+            # break references back to us so that setting the name
+            # on the output of a groupby doesn't reflect back here.
+            level_index = level_index.copy()
 
         if len(level_index):
             grouper = level_index.take(codes)
"
"pandas","159","e55b698","62ab439b168d972546e06d329916c6be7ddd1288","pandas/core/frame.py","pandas/core/frame.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py","pandas/tests/arithmetic/test_numeric.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 603a615c1..1be7e0736 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -108,6 +108,7 @@ from pandas.core.internals.construction import (
     sanitize_index,
     to_arrays,
 )
+from pandas.core.ops.missing import dispatch_fill_zeros
 from pandas.core.series import Series
 
 from pandas.io.formats import console, format as fmt
@@ -5305,7 +5306,9 @@ class DataFrame(NDFrame):
             # iterate over columns
             return ops.dispatch_to_series(this, other, _arith_op)
         else:
-            result = _arith_op(this.values, other.values)
+            with np.errstate(all=""ignore""):
+                result = _arith_op(this.values, other.values)
+            result = dispatch_fill_zeros(func, this.values, other.values, result)
             return self._constructor(
                 result, index=new_index, columns=new_columns, copy=False
             )
"
"pandas","99","2b1b3da","b8043724c48890e86fda0265ad5b6ac3d31f1940","pandas/core/tools/datetimes.py","pandas/core/tools/datetimes.py","diff --git a/pandas/core/tools/datetimes.py b/pandas/core/tools/datetimes.py","pandas/tests/indexes/datetimes/test_tools.py","","diff --git a/pandas/core/tools/datetimes.py b/pandas/core/tools/datetimes.py
index f193865d9..85094ce74 100644
--- a/pandas/core/tools/datetimes.py
+++ b/pandas/core/tools/datetimes.py
@@ -38,6 +38,7 @@ from pandas.core.dtypes.generic import (
 )
 from pandas.core.dtypes.missing import notna
 
+from pandas.arrays import IntegerArray
 from pandas.core import algorithms
 from pandas.core.algorithms import unique
 
@@ -316,8 +317,21 @@ def _convert_listlike_datetimes(
     elif unit is not None:
         if format is not None:
             raise ValueError(""cannot specify both format and unit"")
-        arg = getattr(arg, ""values"", arg)
-        result, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)
+        arg = getattr(arg, ""_values"", arg)
+
+        # GH 30050 pass an ndarray to tslib.array_with_unit_to_datetime
+        # because it expects an ndarray argument
+        if isinstance(arg, IntegerArray):
+            # Explicitly pass NaT mask to array_with_unit_to_datetime
+            mask = arg.isna()
+            arg = arg._ndarray_values
+        else:
+            mask = None
+
+        result, tz_parsed = tslib.array_with_unit_to_datetime(
+            arg, mask, unit, errors=errors
+        )
+
         if errors == ""ignore"":
             from pandas import Index
 
"
"pandas","158","a76df79","b1c871ce4b5e76b3cffe1ebd4216d36379872352","pandas/core/series.py","pandas/core/series.py","diff --git a/pandas/core/series.py b/pandas/core/series.py","pandas/tests/series/test_alter_axes.py","","diff --git a/pandas/core/series.py b/pandas/core/series.py
index 3f04970ee..8b6c963e4 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -4165,12 +4165,10 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         """"""
         kwargs[""inplace""] = validate_bool_kwarg(kwargs.get(""inplace"", False), ""inplace"")
 
-        non_mapping = is_scalar(index) or (
-            is_list_like(index) and not is_dict_like(index)
-        )
-        if non_mapping:
+        if callable(index) or is_dict_like(index):
+            return super().rename(index=index, **kwargs)
+        else:
             return self._set_name(index, inplace=kwargs.get(""inplace""))
-        return super().rename(index=index, **kwargs)
 
     @Substitution(**_shared_doc_kwargs)
     @Appender(generic.NDFrame.reindex.__doc__)
"
"pandas","105","7e6125a","cb5f9d1ff407f5ccef7c717e0c23bbd6ed96cf5f","pandas/core/frame.py;pandas/core/generic.py;pandas/tests/arithmetic/conftest.py;pandas/tests/extension/base/reshaping.py;pandas/tests/extension/json/test_json.py;pandas/tests/extension/test_numpy.py","pandas/core/frame.py;pandas/core/generic.py;pandas/tests/arithmetic/conftest.py;pandas/tests/extension/base/reshaping.py;pandas/tests/extension/json/test_json.py;pandas/tests/extension/test_numpy.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py;diff --git a/pandas/core/generic.py b/pandas/core/generic.py;diff --git a/pandas/tests/arithmetic/conftest.py b/pandas/tests/arithmetic/conftest.py;diff --git a/pandas/tests/extension/base/reshaping.py b/pandas/tests/extension/base/reshaping.py;diff --git a/pandas/tests/extension/json/test_json.py b/pandas/tests/extension/json/test_json.py;diff --git a/pandas/tests/extension/test_numpy.py b/pandas/tests/extension/test_numpy.py","pandas/tests/arithmetic/test_period.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index dfda14704..45b89e842 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -2485,7 +2485,7 @@ class DataFrame(NDFrame):
             )
         return result
 
-    def transpose(self, *args, **kwargs):
+    def transpose(self, *args, copy: bool = False):
         """"""
         Transpose index and columns.
 
@@ -2495,9 +2495,14 @@ class DataFrame(NDFrame):
 
         Parameters
         ----------
-        *args, **kwargs
-            Additional arguments and keywords have no effect but might be
-            accepted for compatibility with numpy.
+        *args : tuple, optional
+            Accepted for compatibility with NumPy.
+        copy : bool, default False
+            Whether to copy the data after transposing, even for DataFrames
+            with a single dtype.
+
+            Note that a copy is always required for mixed dtype DataFrames,
+            or for DataFrames with any extension types.
 
         Returns
         -------
@@ -2578,7 +2583,29 @@ class DataFrame(NDFrame):
         dtype: object
         """"""
         nv.validate_transpose(args, dict())
-        return super().transpose(1, 0, **kwargs)
+        # construct the args
+
+        dtypes = list(self.dtypes)
+        if self._is_homogeneous_type and dtypes and is_extension_array_dtype(dtypes[0]):
+            # We have EAs with the same dtype. We can preserve that dtype in transpose.
+            dtype = dtypes[0]
+            arr_type = dtype.construct_array_type()
+            values = self.values
+
+            new_values = [arr_type._from_sequence(row, dtype=dtype) for row in values]
+            result = self._constructor(
+                dict(zip(self.index, new_values)), index=self.columns
+            )
+
+        else:
+            new_values = self.values.T
+            if copy:
+                new_values = new_values.copy()
+            result = self._constructor(
+                new_values, index=self.columns, columns=self.index
+            )
+
+        return result.__finalize__(self)
 
     T = property(transpose)
 
diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index f846d5883..ab5ee8414 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -643,50 +643,6 @@ class NDFrame(PandasObject, SelectionMixin):
         self._data.set_axis(axis, labels)
         self._clear_item_cache()
 
-    def transpose(self, *args, **kwargs):
-        """"""
-        Permute the dimensions of the %(klass)s
-
-        Parameters
-        ----------
-        args : %(args_transpose)s
-        copy : bool, default False
-            Make a copy of the underlying data. Mixed-dtype data will
-            always result in a copy
-        **kwargs
-            Additional keyword arguments will be passed to the function.
-
-        Returns
-        -------
-        y : same as input
-
-        Examples
-        --------
-        >>> p.transpose(2, 0, 1)
-        >>> p.transpose(2, 0, 1, copy=True)
-        """"""
-
-        # construct the args
-        axes, kwargs = self._construct_axes_from_arguments(
-            args, kwargs, require_all=True
-        )
-        axes_names = tuple(self._get_axis_name(axes[a]) for a in self._AXIS_ORDERS)
-        axes_numbers = tuple(self._get_axis_number(axes[a]) for a in self._AXIS_ORDERS)
-
-        # we must have unique axes
-        if len(axes) != len(set(axes)):
-            raise ValueError(f""Must specify {self._AXIS_LEN} unique axes"")
-
-        new_axes = self._construct_axes_dict_from(
-            self, [self._get_axis(x) for x in axes_names]
-        )
-        new_values = self.values.transpose(axes_numbers)
-        if kwargs.pop(""copy"", None) or (len(args) and args[-1]):
-            new_values = new_values.copy()
-
-        nv.validate_transpose(tuple(), kwargs)
-        return self._constructor(new_values, **new_axes).__finalize__(self)
-
     def swapaxes(self, axis1, axis2, copy=True):
         """"""
         Interchange axes and swap values axes appropriately.
diff --git a/pandas/tests/arithmetic/conftest.py b/pandas/tests/arithmetic/conftest.py
index 33dda75e2..64588af3e 100644
--- a/pandas/tests/arithmetic/conftest.py
+++ b/pandas/tests/arithmetic/conftest.py
@@ -235,25 +235,6 @@ def box_df_fail(request):
     return request.param
 
 
-@pytest.fixture(
-    params=[
-        (pd.Index, False),
-        (pd.Series, False),
-        (pd.DataFrame, False),
-        pytest.param((pd.DataFrame, True), marks=pytest.mark.xfail),
-        (tm.to_array, False),
-    ],
-    ids=id_func,
-)
-def box_transpose_fail(request):
-    """"""
-    Fixture similar to `box` but testing both transpose cases for DataFrame,
-    with the transpose=True case xfailed.
-    """"""
-    # GH#23620
-    return request.param
-
-
 @pytest.fixture(params=[pd.Index, pd.Series, pd.DataFrame, tm.to_array], ids=id_func)
 def box_with_array(request):
     """"""
diff --git a/pandas/tests/extension/base/reshaping.py b/pandas/tests/extension/base/reshaping.py
index 90e607343..89c9ed367 100644
--- a/pandas/tests/extension/base/reshaping.py
+++ b/pandas/tests/extension/base/reshaping.py
@@ -295,3 +295,19 @@ class BaseReshapingTests(BaseExtensionTests):
         # Check that we have a view, not a copy
         result[0] = result[1]
         assert data[0] == data[1]
+
+    def test_transpose(self, data):
+        df = pd.DataFrame({""A"": data[:4], ""B"": data[:4]}, index=[""a"", ""b"", ""c"", ""d""])
+        result = df.T
+        expected = pd.DataFrame(
+            {
+                ""a"": type(data)._from_sequence([data[0]] * 2, dtype=data.dtype),
+                ""b"": type(data)._from_sequence([data[1]] * 2, dtype=data.dtype),
+                ""c"": type(data)._from_sequence([data[2]] * 2, dtype=data.dtype),
+                ""d"": type(data)._from_sequence([data[3]] * 2, dtype=data.dtype),
+            },
+            index=[""A"", ""B""],
+        )
+        self.assert_frame_equal(result, expected)
+        self.assert_frame_equal(np.transpose(np.transpose(df)), df)
+        self.assert_frame_equal(np.transpose(np.transpose(df[[""A""]])), df[[""A""]])
diff --git a/pandas/tests/extension/json/test_json.py b/pandas/tests/extension/json/test_json.py
index 16a4caa7d..01f2565e2 100644
--- a/pandas/tests/extension/json/test_json.py
+++ b/pandas/tests/extension/json/test_json.py
@@ -163,6 +163,10 @@ class TestReshaping(BaseJSON, base.BaseReshapingTests):
         # this matches otherwise
         return super().test_unstack(data, index)
 
+    @pytest.mark.xfail(reason=""Inconsistent sizes."")
+    def test_transpose(self, data):
+        super().test_transpose(data)
+
 
 class TestGetitem(BaseJSON, base.BaseGetitemTests):
     pass
diff --git a/pandas/tests/extension/test_numpy.py b/pandas/tests/extension/test_numpy.py
index beb3fc80e..55a617caf 100644
--- a/pandas/tests/extension/test_numpy.py
+++ b/pandas/tests/extension/test_numpy.py
@@ -332,6 +332,10 @@ class TestReshaping(BaseNumPyTests, base.BaseReshapingTests):
         # Fails creating expected
         super().test_merge_on_extension_array_duplicates(data)
 
+    @skip_nested
+    def test_transpose(self, data):
+        super().test_transpose(data)
+
 
 class TestSetitem(BaseNumPyTests, base.BaseSetitemTests):
     @skip_nested
"
"pandas","42","b3a0fe4","05780a760400e42ce1b00200dd8204ae4f94044a","pandas/_testing.py","pandas/_testing.py","diff --git a/pandas/_testing.py b/pandas/_testing.py","pandas/tests/util/test_assert_frame_equal.py;pandas/tests/util/test_assert_series_equal.py","","diff --git a/pandas/_testing.py b/pandas/_testing.py
index d473b453d..f96e3872e 100644
--- a/pandas/_testing.py
+++ b/pandas/_testing.py
@@ -1160,7 +1160,7 @@ def assert_series_equal(
                 f""is not equal to {right._values}.""
             )
             raise AssertionError(msg)
-    elif is_interval_dtype(left.dtype) or is_interval_dtype(right.dtype):
+    elif is_interval_dtype(left.dtype) and is_interval_dtype(right.dtype):
         assert_interval_array_equal(left.array, right.array)
     elif is_categorical_dtype(left.dtype) or is_categorical_dtype(right.dtype):
         _testing.assert_almost_equal(
@@ -1170,7 +1170,7 @@ def assert_series_equal(
             check_dtype=check_dtype,
             obj=str(obj),
         )
-    elif is_extension_array_dtype(left.dtype) or is_extension_array_dtype(right.dtype):
+    elif is_extension_array_dtype(left.dtype) and is_extension_array_dtype(right.dtype):
         assert_extension_array_equal(left._values, right._values)
     elif needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype):
         # DatetimeArray or TimedeltaArray
"
"pandas","19","17dc6b0","c6a1638bcd99df677a8f76f036c0b30027eb243c","pandas/core/indexing.py","pandas/core/indexing.py","diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py","pandas/tests/indexing/multiindex/test_loc.py;pandas/tests/indexing/multiindex/test_slice.py;pandas/tests/series/indexing/test_getitem.py","","diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index 303365f50..5752f00ca 100644
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -1082,37 +1082,6 @@ class _LocIndexer(_LocationIndexer):
             return self._getbool_axis(key, axis=axis)
         elif is_list_like_indexer(key):
 
-            # convert various list-like indexers
-            # to a list of keys
-            # we will use the *values* of the object
-            # and NOT the index if its a PandasObject
-            if isinstance(labels, ABCMultiIndex):
-
-                if isinstance(key, (ABCSeries, np.ndarray)) and key.ndim <= 1:
-                    # Series, or 0,1 ndim ndarray
-                    # GH 14730
-                    key = list(key)
-                elif isinstance(key, ABCDataFrame):
-                    # GH 15438
-                    raise NotImplementedError(
-                        ""Indexing a MultiIndex with a ""
-                        ""DataFrame key is not ""
-                        ""implemented""
-                    )
-                elif hasattr(key, ""ndim"") and key.ndim > 1:
-                    raise NotImplementedError(
-                        ""Indexing a MultiIndex with a ""
-                        ""multidimensional key is not ""
-                        ""implemented""
-                    )
-
-                if (
-                    not isinstance(key, tuple)
-                    and len(key)
-                    and not isinstance(key[0], tuple)
-                ):
-                    key = tuple([key])
-
             # an iterable multi-selection
             if not (isinstance(key, tuple) and isinstance(labels, ABCMultiIndex)):
 
"
"pandas","109","8b39de1","68b3eb4f5a7fbc223accbbeddbf03ec8ea31af00","pandas/core/arrays/categorical.py","pandas/core/arrays/categorical.py","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py","pandas/tests/arrays/categorical/test_analytics.py","","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index 71fe94482..6b422adef 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -2115,6 +2115,10 @@ class Categorical(ExtensionArray, PandasObject):
 
         Only ordered `Categoricals` have a minimum!
 
+        .. versionchanged:: 1.0.0
+
+           Returns an NA value on empty arrays
+
         Raises
         ------
         TypeError
@@ -2125,6 +2129,10 @@ class Categorical(ExtensionArray, PandasObject):
         min : the minimum of this `Categorical`
         """"""
         self.check_for_ordered(""min"")
+
+        if not len(self._codes):
+            return self.dtype.na_value
+
         good = self._codes != -1
         if not good.all():
             if skipna:
@@ -2142,6 +2150,10 @@ class Categorical(ExtensionArray, PandasObject):
 
         Only ordered `Categoricals` have a maximum!
 
+        .. versionchanged:: 1.0.0
+
+           Returns an NA value on empty arrays
+
         Raises
         ------
         TypeError
@@ -2152,6 +2164,10 @@ class Categorical(ExtensionArray, PandasObject):
         max : the maximum of this `Categorical`
         """"""
         self.check_for_ordered(""max"")
+
+        if not len(self._codes):
+            return self.dtype.na_value
+
         good = self._codes != -1
         if not good.all():
             if skipna:
"
"pandas","161","a818281","ca5198a6daa7757e398112a17ccadc9e7d078d96","pandas/core/arrays/categorical.py","pandas/core/arrays/categorical.py","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py","pandas/tests/series/test_missing.py","","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index bbbeb812d..a895da618 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -1840,8 +1840,8 @@ class Categorical(ExtensionArray, PandasObject):
                     raise ValueError(""fill value must be in categories"")
 
                 values_codes = _get_codes_for_values(value, self.categories)
-                indexer = np.where(values_codes != -1)
-                codes[indexer] = values_codes[values_codes != -1]
+                indexer = np.where(codes == -1)
+                codes[indexer] = values_codes[indexer]
 
             # If value is not a dict or Series it should be a scalar
             elif is_hashable(value):
"
"pandas","160","489d1ff","fb62fcf91c874e9c24fa83693c4e6e613f35f864","pandas/core/computation/expressions.py","pandas/core/computation/expressions.py","diff --git a/pandas/core/computation/expressions.py b/pandas/core/computation/expressions.py","pandas/tests/test_expressions.py","","diff --git a/pandas/core/computation/expressions.py b/pandas/core/computation/expressions.py
index 5b6d27500..29c8239fa 100644
--- a/pandas/core/computation/expressions.py
+++ b/pandas/core/computation/expressions.py
@@ -76,16 +76,17 @@ def _can_use_numexpr(op, op_str, a, b, dtype_check):
 
         # required min elements (otherwise we are adding overhead)
         if np.prod(a.shape) > _MIN_ELEMENTS:
-
             # check for dtype compatibility
             dtypes = set()
             for o in [a, b]:
-                if hasattr(o, ""dtypes""):
+                # Series implements dtypes, check for dimension count as well
+                if hasattr(o, ""dtypes"") and o.ndim > 1:
                     s = o.dtypes.value_counts()
                     if len(s) > 1:
                         return False
                     dtypes |= set(s.index.astype(str))
-                elif isinstance(o, np.ndarray):
+                # ndarray and Series Case
+                elif hasattr(o, ""dtype""):
                     dtypes |= {o.dtype.name}
 
             # allowed are a superset
"
"pandas","138","3dcbec5","c59c2df94e5563819a824f49fa6f55636bdb4445","pandas/core/reshape/tile.py;pandas/tests/reshape/test_cut.py","pandas/core/reshape/tile.py;pandas/tests/reshape/test_cut.py","diff --git a/pandas/core/reshape/tile.py b/pandas/core/reshape/tile.py;diff --git a/pandas/tests/reshape/test_cut.py b/pandas/tests/reshape/test_cut.py","pandas/tests/reshape/test_qcut.py","","diff --git a/pandas/core/reshape/tile.py b/pandas/core/reshape/tile.py
index ab354a21a..be5d75224 100644
--- a/pandas/core/reshape/tile.py
+++ b/pandas/core/reshape/tile.py
@@ -11,6 +11,7 @@ from pandas._libs.lib import infer_dtype
 from pandas.core.dtypes.common import (
     _NS_DTYPE,
     ensure_int64,
+    is_bool_dtype,
     is_categorical_dtype,
     is_datetime64_dtype,
     is_datetime64tz_dtype,
@@ -423,8 +424,8 @@ def _bins_to_cuts(
 
 def _coerce_to_type(x):
     """"""
-    if the passed data is of datetime/timedelta type,
-    this method converts it to numeric so that cut method can
+    if the passed data is of datetime/timedelta or bool type,
+    this method converts it to numeric so that cut or qcut method can
     handle it
     """"""
     dtype = None
@@ -437,6 +438,9 @@ def _coerce_to_type(x):
     elif is_timedelta64_dtype(x):
         x = to_timedelta(x)
         dtype = np.dtype(""timedelta64[ns]"")
+    elif is_bool_dtype(x):
+        # GH 20303
+        x = x.astype(np.int64)
 
     if dtype is not None:
         # GH 19768: force NaT to NaN during integer conversion
diff --git a/pandas/tests/reshape/test_cut.py b/pandas/tests/reshape/test_cut.py
index a2ebf2359..611c3272c 100644
--- a/pandas/tests/reshape/test_cut.py
+++ b/pandas/tests/reshape/test_cut.py
@@ -585,3 +585,21 @@ def test_timedelta_cut_roundtrip():
         [""0 days 23:57:07.200000"", ""2 days 00:00:00"", ""3 days 00:00:00""]
     )
     tm.assert_index_equal(result_bins, expected_bins)
+
+
+@pytest.mark.parametrize(""bins"", [6, 7])
+@pytest.mark.parametrize(
+    ""box, compare"",
+    [
+        (Series, tm.assert_series_equal),
+        (np.array, tm.assert_categorical_equal),
+        (list, tm.assert_equal),
+    ],
+)
+def test_cut_bool_coercion_to_int(bins, box, compare):
+    # issue 20303
+    data_expected = box([0, 1, 1, 0, 1] * 10)
+    data_result = box([False, True, True, False, True] * 10)
+    expected = cut(data_expected, bins, duplicates=""drop"")
+    result = cut(data_result, bins, duplicates=""drop"")
+    compare(result, expected)
"
"pandas","54","25443f0","00e8e4ab0c5e4c7bfb3e356e660d9f088d4a82a4","pandas/core/dtypes/dtypes.py;pandas/tests/indexes/common.py","pandas/core/dtypes/dtypes.py;pandas/tests/indexes/common.py","diff --git a/pandas/core/dtypes/dtypes.py b/pandas/core/dtypes/dtypes.py;diff --git a/pandas/tests/indexes/common.py b/pandas/tests/indexes/common.py","pandas/tests/dtypes/test_dtypes.py","","diff --git a/pandas/core/dtypes/dtypes.py b/pandas/core/dtypes/dtypes.py
index d93ad973f..0730de934 100644
--- a/pandas/core/dtypes/dtypes.py
+++ b/pandas/core/dtypes/dtypes.py
@@ -324,6 +324,8 @@ class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):
                 raise ValueError(
                     ""Cannot specify `categories` or `ordered` together with `dtype`.""
                 )
+            elif not isinstance(dtype, CategoricalDtype):
+                raise ValueError(f""Cannot not construct CategoricalDtype from {dtype}"")
         elif is_categorical(values):
             # If no ""dtype"" was passed, use the one from ""values"", but honor
             # the ""ordered"" and ""categories"" arguments
diff --git a/pandas/tests/indexes/common.py b/pandas/tests/indexes/common.py
index 2073aa072..c9e762af3 100644
--- a/pandas/tests/indexes/common.py
+++ b/pandas/tests/indexes/common.py
@@ -605,7 +605,8 @@ class Base:
         assert not indices.equals(np.array(indices))
 
         # Cannot pass in non-int64 dtype to RangeIndex
-        if not isinstance(indices, RangeIndex):
+        if not isinstance(indices, (RangeIndex, CategoricalIndex)):
+            # TODO: CategoricalIndex can be re-allowed following GH#32167
             same_values = Index(indices, dtype=object)
             assert indices.equals(same_values)
             assert same_values.equals(indices)
"
"pandas","107","47ac4b3","fa4949f27ccfbc255bb8dbcd5ec5464b8663f1d2","pandas/core/frame.py","pandas/core/frame.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py","pandas/tests/frame/test_combine_concat.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 51330bfc5..dfda14704 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -6757,25 +6757,18 @@ class DataFrame(NDFrame):
                     "" or if the Series has a name""
                 )
 
-            if other.name is None:
-                index = None
-            else:
-                # other must have the same index name as self, otherwise
-                # index name will be reset
-                index = Index([other.name], name=self.index.name)
-
+            index = Index([other.name], name=self.index.name)
             idx_diff = other.index.difference(self.columns)
             try:
                 combined_columns = self.columns.append(idx_diff)
             except TypeError:
                 combined_columns = self.columns.astype(object).append(idx_diff)
-            other = other.reindex(combined_columns, copy=False)
-            other = DataFrame(
-                other.values.reshape((1, len(other))),
-                index=index,
-                columns=combined_columns,
+            other = (
+                other.reindex(combined_columns, copy=False)
+                .to_frame()
+                .T.infer_objects()
+                .rename_axis(index.names, copy=False)
             )
-            other = other._convert(datetime=True, timedelta=True)
             if not self.columns.equals(combined_columns):
                 self = self.reindex(columns=combined_columns)
         elif isinstance(other, list):
"
"pandas","9","c557ab5","ebb727e5cd8865a7f5d6cfb4b22d3278b6bf5e6b","pandas/core/arrays/categorical.py;pandas/core/indexes/category.py","pandas/core/arrays/categorical.py;pandas/core/indexes/category.py","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py;diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py","pandas/tests/indexes/categorical/test_indexing.py","","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index 0c9db271e..9cdb5dfc4 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -37,7 +37,7 @@ from pandas.core.dtypes.common import (
 from pandas.core.dtypes.dtypes import CategoricalDtype
 from pandas.core.dtypes.generic import ABCIndexClass, ABCSeries
 from pandas.core.dtypes.inference import is_hashable
-from pandas.core.dtypes.missing import isna, notna
+from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna, notna
 
 from pandas.core import ops
 from pandas.core.accessor import PandasDelegate, delegate_names
@@ -1834,7 +1834,7 @@ class Categorical(NDArrayBackedExtensionArray, PandasObject):
         Returns True if `key` is in this Categorical.
         """"""
         # if key is a NaN, check if any NaN is in self.
-        if is_scalar(key) and isna(key):
+        if is_valid_nat_for_dtype(key, self.categories.dtype):
             return self.isna().any()
 
         return contains(self, key, container=self._codes)
diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py
index 0cf6698d3..80d3e5c8a 100644
--- a/pandas/core/indexes/category.py
+++ b/pandas/core/indexes/category.py
@@ -19,7 +19,7 @@ from pandas.core.dtypes.common import (
     is_scalar,
 )
 from pandas.core.dtypes.dtypes import CategoricalDtype
-from pandas.core.dtypes.missing import isna
+from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna
 
 from pandas.core import accessor
 from pandas.core.algorithms import take_1d
@@ -365,10 +365,9 @@ class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):
     @doc(Index.__contains__)
     def __contains__(self, key: Any) -> bool:
         # if key is a NaN, check if any NaN is in self.
-        if is_scalar(key) and isna(key):
+        if is_valid_nat_for_dtype(key, self.categories.dtype):
             return self.hasnans
 
-        hash(key)
         return contains(self, key, container=self._engine)
 
     @doc(Index.astype)
"
"pandas","50","ebf9668","821aa25c9039e72da9a7b236cf2f9e7d549cbb7b","pandas/core/arrays/categorical.py","pandas/core/arrays/categorical.py","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py","pandas/tests/extension/test_categorical.py","","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index 4167c75eb..40a169d03 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -103,7 +103,10 @@ def _cat_compare_op(op):
             mask = (self._codes == -1) | (other_codes == -1)
             if mask.any():
                 # In other series, the leads to False, so do that here too
-                ret[mask] = False
+                if opname == ""__ne__"":
+                    ret[(self._codes == -1) & (other_codes == -1)] = True
+                else:
+                    ret[mask] = False
             return ret
 
         if is_scalar(other):
"
"pandas","21","4071c3b","56d0934092b8296c90f940c56fce3b731e0de81b","pandas/core/series.py","pandas/core/series.py","diff --git a/pandas/core/series.py b/pandas/core/series.py","pandas/tests/series/indexing/test_boolean.py;pandas/tests/series/indexing/test_getitem.py","","diff --git a/pandas/core/series.py b/pandas/core/series.py
index 854c87071..5a1d7f3b9 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -949,11 +949,8 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
             else:
                 return self.iloc[key]
 
-        if isinstance(key, list):
-            # handle the dup indexing case GH#4246
-            return self.loc[key]
-
-        return self.reindex(key)
+        # handle the dup indexing case GH#4246
+        return self.loc[key]
 
     def _get_values_tuple(self, key):
         # mpl hackaround
"
"pandas","106","114d552","e46026ff4669a30192b91e362ce8cdcbc9693870","pandas/core/indexes/base.py","pandas/core/indexes/base.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py","pandas/tests/indexes/multi/test_drop.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index ce7a238da..272e97481 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -4551,7 +4551,7 @@ class Index(IndexOpsMixin, PandasObject):
 
         if is_categorical(target):
             tgt_values = np.asarray(target)
-        elif self.is_all_dates:
+        elif self.is_all_dates and target.is_all_dates:  # GH 30399
             tgt_values = target.asi8
         else:
             tgt_values = target._ndarray_values
"
"pandas","108","20e4c18","53a0dfd41a65a33dd7b0963734b24c749212e625","pandas/core/dtypes/cast.py","pandas/core/dtypes/cast.py","diff --git a/pandas/core/dtypes/cast.py b/pandas/core/dtypes/cast.py","pandas/tests/dtypes/cast/test_infer_dtype.py","","diff --git a/pandas/core/dtypes/cast.py b/pandas/core/dtypes/cast.py
index b398a197a..1ab21f18f 100644
--- a/pandas/core/dtypes/cast.py
+++ b/pandas/core/dtypes/cast.py
@@ -41,7 +41,7 @@ from .common import (
     is_unsigned_integer_dtype,
     pandas_dtype,
 )
-from .dtypes import DatetimeTZDtype, ExtensionDtype, PeriodDtype
+from .dtypes import DatetimeTZDtype, ExtensionDtype, IntervalDtype, PeriodDtype
 from .generic import (
     ABCDataFrame,
     ABCDatetimeArray,
@@ -601,6 +601,9 @@ def infer_dtype_from_scalar(val, pandas_dtype: bool = False):
         if lib.is_period(val):
             dtype = PeriodDtype(freq=val.freq)
             val = val.ordinal
+        elif lib.is_interval(val):
+            subtype = infer_dtype_from_scalar(val.left, pandas_dtype=True)[0]
+            dtype = IntervalDtype(subtype=subtype)
 
     return dtype, val
 
"
"pandas","30","60d6f28","d857cd12b3ae11be788ba96015383a5b7464ecc9","pandas/io/json/_json.py","pandas/io/json/_json.py","diff --git a/pandas/io/json/_json.py b/pandas/io/json/_json.py","pandas/tests/io/json/test_pandas.py","","diff --git a/pandas/io/json/_json.py b/pandas/io/json/_json.py
index 886387a7a..20724a498 100644
--- a/pandas/io/json/_json.py
+++ b/pandas/io/json/_json.py
@@ -982,7 +982,7 @@ class Parser:
         for date_unit in date_units:
             try:
                 new_data = to_datetime(new_data, errors=""raise"", unit=date_unit)
-            except (ValueError, OverflowError):
+            except (ValueError, OverflowError, TypeError):
                 continue
             return new_data, True
         return data, False
"
"pandas","15","f3fdab3","71d610596ed128055614eb660f13c88168bfe22f","pandas/core/arrays/datetimelike.py;pandas/tests/indexes/timedeltas/test_timedelta.py","pandas/core/arrays/datetimelike.py;pandas/tests/indexes/timedeltas/test_timedelta.py","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py;diff --git a/pandas/tests/indexes/timedeltas/test_timedelta.py b/pandas/tests/indexes/timedeltas/test_timedelta.py","pandas/tests/indexes/datetimes/test_datetime.py","","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py
index 407daf15d..ae119e72e 100644
--- a/pandas/core/arrays/datetimelike.py
+++ b/pandas/core/arrays/datetimelike.py
@@ -1,1054 +1,672 @@
+""""""
+Base and utility classes for tseries type pandas objects.
+""""""
 from datetime import datetime, timedelta
-import operator
-from typing import Any, Sequence, Type, Union, cast
-import warnings
+from typing import Any, List, Optional, Union, cast
 
 import numpy as np
 
-from pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib
-from pandas._libs.tslibs.c_timestamp import integer_op_not_supported
-from pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period
-from pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds
-from pandas._libs.tslibs.timestamps import RoundTo, round_nsint64
-from pandas._typing import DatetimeLikeScalar
-from pandas.compat import set_function_name
+from pandas._libs import NaT, iNaT, join as libjoin, lib
+from pandas._libs.tslibs import timezones
+from pandas._typing import Label
 from pandas.compat.numpy import function as nv
-from pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning
-from pandas.util._decorators import Appender, Substitution
-from pandas.util._validators import validate_fillna_kwargs
+from pandas.errors import AbstractMethodError
+from pandas.util._decorators import Appender, cache_readonly, doc
 
 from pandas.core.dtypes.common import (
-    is_categorical_dtype,
+    ensure_int64,
+    is_bool_dtype,
     is_datetime64_any_dtype,
-    is_datetime64_dtype,
-    is_datetime64tz_dtype,
-    is_datetime_or_timedelta_dtype,
     is_dtype_equal,
-    is_float_dtype,
-    is_integer_dtype,
+    is_integer,
     is_list_like,
     is_object_dtype,
     is_period_dtype,
-    is_string_dtype,
+    is_scalar,
     is_timedelta64_dtype,
-    is_unsigned_integer_dtype,
-    pandas_dtype,
 )
-from pandas.core.dtypes.generic import ABCSeries
-from pandas.core.dtypes.inference import is_array_like
-from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna
-
-from pandas.core import missing, nanops, ops
-from pandas.core.algorithms import checked_add_with_arr, unique1d, value_counts
-from pandas.core.array_algos.transforms import shift
-from pandas.core.arrays._mixins import _T, NDArrayBackedExtensionArray
-from pandas.core.arrays.base import ExtensionArray, ExtensionOpsMixin
-import pandas.core.common as com
-from pandas.core.construction import array, extract_array
-from pandas.core.indexers import check_array_indexer
-from pandas.core.ops.common import unpack_zerodim_and_defer
-from pandas.core.ops.invalid import invalid_comparison, make_invalid_op
-
-from pandas.tseries import frequencies
-from pandas.tseries.offsets import DateOffset, Tick
-
-
-def _datetimelike_array_cmp(cls, op):
-    """"""
-    Wrap comparison operations to convert Timestamp/Timedelta/Period-like to
-    boxed scalars/arrays.
-    """"""
-    opname = f""__{op.__name__}__""
-    nat_result = opname == ""__ne__""
-
-    class InvalidComparison(Exception):
-        pass
-
-    def _validate_comparison_value(self, other):
-        if isinstance(other, str):
-            try:
-                # GH#18435 strings get a pass from tzawareness compat
-                other = self._scalar_from_string(other)
-            except ValueError:
-                # failed to parse as Timestamp/Timedelta/Period
-                raise InvalidComparison(other)
+from pandas.core.dtypes.concat import concat_compat
+from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
+from pandas.core.dtypes.missing import isna
+
+from pandas.core import algorithms
+from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray
+from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin
+from pandas.core.base import IndexOpsMixin
+import pandas.core.indexes.base as ibase
+from pandas.core.indexes.base import Index, _index_shared_docs
+from pandas.core.indexes.extension import (
+    ExtensionIndex,
+    inherit_names,
+    make_wrapped_arith_op,
+)
+from pandas.core.indexes.numeric import Int64Index
+from pandas.core.ops import get_op_result_name
+from pandas.core.tools.timedeltas import to_timedelta
 
-        if isinstance(other, self._recognized_scalars) or other is NaT:
-            other = self._scalar_type(other)
-            self._check_compatible_with(other)
+from pandas.tseries.frequencies import DateOffset
+from pandas.tseries.offsets import Tick
 
-        elif not is_list_like(other):
-            raise InvalidComparison(other)
+_index_doc_kwargs = dict(ibase._index_doc_kwargs)
 
-        elif len(other) != len(self):
-            raise ValueError(""Lengths must match"")
 
-        else:
-            if isinstance(other, list):
-                # TODO: could use pd.Index to do inference?
-                other = np.array(other)
+def _join_i8_wrapper(joinf, with_indexers: bool = True):
+    """"""
+    Create the join wrapper methods.
+    """"""
 
-            if not isinstance(other, (np.ndarray, type(self))):
-                raise InvalidComparison(other)
+    @staticmethod  # type: ignore
+    def wrapper(left, right):
+        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
+            left = left.view(""i8"")
+        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
+            right = right.view(""i8"")
 
-            elif is_object_dtype(other.dtype):
-                pass
+        results = joinf(left, right)
+        if with_indexers:
+            # dtype should be timedelta64[ns] for TimedeltaIndex
+            #  and datetime64[ns] for DatetimeIndex
+            dtype = left.dtype.base
 
-            elif not type(self)._is_recognized_dtype(other.dtype):
-                raise InvalidComparison(other)
+            join_index, left_indexer, right_indexer = results
+            join_index = join_index.view(dtype)
+            return join_index, left_indexer, right_indexer
+        return results
 
-            else:
-                # For PeriodDType this casting is unnecessary
-                # TODO: use Index to do inference?
-                other = type(self)._from_sequence(other)
-                self._check_compatible_with(other)
+    return wrapper
 
-        return other
 
-    @unpack_zerodim_and_defer(opname)
-    def wrapper(self, other):
+def _make_wrapped_arith_op_with_freq(opname: str):
+    """"""
+    Dispatch the operation to the underlying ExtensionArray, and infer
+    the appropriate frequency for the result.
+    """"""
+    meth = make_wrapped_arith_op(opname)
 
-        try:
-            other = _validate_comparison_value(self, other)
-        except InvalidComparison:
-            return invalid_comparison(self, other, op)
-
-        dtype = getattr(other, ""dtype"", None)
-        if is_object_dtype(dtype):
-            # We have to use comp_method_OBJECT_ARRAY instead of numpy
-            #  comparison otherwise it would fail to raise when
-            #  comparing tz-aware and tz-naive
-            with np.errstate(all=""ignore""):
-                result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)
-            return result
+    def wrapped(self, other):
+        result = meth(self, other)
+        if result is NotImplemented:
+            return NotImplemented
 
-        if isinstance(other, self._scalar_type) or other is NaT:
-            other_i8 = self._unbox_scalar(other)
-        else:
-            # Then type(other) == type(self)
-            other_i8 = other.asi8
+        new_freq = self._get_addsub_freq(other, result)
+        result._freq = new_freq
+        return result
 
-        result = op(self.asi8, other_i8)
+    wrapped.__name__ = opname
+    return wrapped
 
-        o_mask = isna(other)
-        if self._hasnans | np.any(o_mask):
-            result[self._isnan | o_mask] = nat_result
 
-        return result
+@inherit_names(
+    [""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
+    DatetimeLikeArrayMixin,
+    cache=True,
+)
+@inherit_names(
+    [""mean"", ""asi8"", ""_box_func""], DatetimeLikeArrayMixin,
+)
+class DatetimeIndexOpsMixin(ExtensionIndex):
+    """"""
+    Common ops mixin to support a unified interface datetimelike Index.
+    """"""
 
-    return set_function_name(wrapper, opname, cls)
+    _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]
+    freq: Optional[DateOffset]
+    freqstr: Optional[str]
+    _resolution: int
+    _bool_ops: List[str] = []
+    _field_ops: List[str] = []
 
+    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
+    _hasnans = hasnans  # for index / array -agnostic code
 
-class AttributesMixin:
-    _data: np.ndarray
+    @property
+    def is_all_dates(self) -> bool:
+        return True
 
-    @classmethod
-    def _simple_new(cls, values: np.ndarray, **kwargs):
-        raise AbstractMethodError(cls)
+    # ------------------------------------------------------------------------
+    # Abstract data attributes
 
     @property
-    def _scalar_type(self) -> Type[DatetimeLikeScalar]:
-        """"""
-        The scalar associated with this datelike
+    def values(self):
+        # Note: PeriodArray overrides this to return an ndarray of objects.
+        return self._data._data
 
-        * PeriodArray : Period
-        * DatetimeArray : Timestamp
-        * TimedeltaArray : Timedelta
+    def __array_wrap__(self, result, context=None):
         """"""
-        raise AbstractMethodError(self)
-
-    def _scalar_from_string(
-        self, value: str
-    ) -> Union[Period, Timestamp, Timedelta, NaTType]:
+        Gets called after a ufunc.
         """"""
-        Construct a scalar type from a string.
+        result = lib.item_from_zerodim(result)
+        if is_bool_dtype(result) or lib.is_scalar(result):
+            return result
 
-        Parameters
-        ----------
-        value : str
+        attrs = self._get_attributes_dict()
+        if not is_period_dtype(self) and attrs[""freq""]:
+            # no need to infer if freq is None
+            attrs[""freq""] = ""infer""
+        return Index(result, **attrs)
 
-        Returns
-        -------
-        Period, Timestamp, or Timedelta, or NaT
-            Whatever the type of ``self._scalar_type`` is.
+    # ------------------------------------------------------------------------
 
-        Notes
-        -----
-        This should call ``self._check_compatible_with`` before
-        unboxing the result.
+    def equals(self, other) -> bool:
         """"""
-        raise AbstractMethodError(self)
-
-    def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:
+        Determines if two Index objects contain the same elements.
         """"""
-        Unbox the integer value of a scalar `value`.
+        if self.is_(other):
+            return True
 
-        Parameters
-        ----------
-        value : Union[Period, Timestamp, Timedelta]
-
-        Returns
-        -------
-        int
-
-        Examples
-        --------
-        >>> self._unbox_scalar(Timedelta(""10s""))  # doctest: +SKIP
-        10000000000
-        """"""
-        raise AbstractMethodError(self)
+        if not isinstance(other, ABCIndexClass):
+            return False
+        elif not isinstance(other, type(self)):
+            try:
+                other = type(self)(other)
+            except (ValueError, TypeError, OverflowError):
+                # e.g.
+                #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime
+                #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex
+                #  OverflowError -> Index([very_large_timedeltas])
+                return False
+
+        if not is_dtype_equal(self.dtype, other.dtype):
+            # have different timezone
+            return False
+
+        return np.array_equal(self.asi8, other.asi8)
+
+    @Appender(Index.__contains__.__doc__)
+    def __contains__(self, key: Any) -> bool:
+        hash(key)
+        try:
+            res = self.get_loc(key)
+        except (KeyError, TypeError, ValueError):
+            return False
+        return bool(
+            is_scalar(res) or isinstance(res, slice) or (is_list_like(res) and len(res))
+        )
 
-    def _check_compatible_with(
-        self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False
-    ) -> None:
+    def sort_values(self, return_indexer=False, ascending=True):
         """"""
-        Verify that `self` and `other` are compatible.
-
-        * DatetimeArray verifies that the timezones (if any) match
-        * PeriodArray verifies that the freq matches
-        * Timedelta has no verification
-
-        In each case, NaT is considered compatible.
-
-        Parameters
-        ----------
-        other
-        setitem : bool, default False
-            For __setitem__ we may have stricter compatibility resrictions than
-            for comparisons.
-
-        Raises
-        ------
-        Exception
+        Return sorted copy of Index.
         """"""
-        raise AbstractMethodError(self)
-
-
-class DatelikeOps:
-    """"""
-    Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.
-    """"""
-
-    @Substitution(
-        URL=""https://docs.python.org/3/library/datetime.html""
-        ""#strftime-and-strptime-behavior""
-    )
-    def strftime(self, date_format):
-        """"""
-        Convert to Index using specified date_format.
-
-        Return an Index of formatted strings specified by date_format, which
-        supports the same string format as the python standard library. Details
-        of the string format can be found in `python string format
-        doc <%(URL)s>`__.
+        if return_indexer:
+            _as = self.argsort()
+            if not ascending:
+                _as = _as[::-1]
+            sorted_index = self.take(_as)
+            return sorted_index, _as
+        else:
+            # NB: using asi8 instead of _data matters in numpy 1.18
+            #  because the treatment of NaT has been changed to put NaT last
+            #  instead of first.
+            sorted_values = np.sort(self.asi8)
 
-        Parameters
-        ----------
-        date_format : str
-            Date format string (e.g. ""%%Y-%%m-%%d"").
+            freq = self.freq
+            if freq is not None and not is_period_dtype(self):
+                if freq.n > 0 and not ascending:
+                    freq = freq * -1
+                elif freq.n < 0 and ascending:
+                    freq = freq * -1
 
-        Returns
-        -------
-        ndarray
-            NumPy ndarray of formatted strings.
+            if not ascending:
+                sorted_values = sorted_values[::-1]
 
-        See Also
-        --------
-        to_datetime : Convert the given argument to datetime.
-        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.
-        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.
-        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.
+            arr = type(self._data)._simple_new(
+                sorted_values, dtype=self.dtype, freq=freq
+            )
+            return type(self)._simple_new(arr, name=self.name)
 
-        Examples
-        --------
-        >>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),
-        ...                     periods=3, freq='s')
-        >>> rng.strftime('%%B %%d, %%Y, %%r')
-        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',
-               'March 10, 2018, 09:00:02 AM'],
-              dtype='object')
-        """"""
-        result = self._format_native_types(date_format=date_format, na_rep=np.nan)
-        return result.astype(object)
+    @Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
+    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
+        nv.validate_take(tuple(), kwargs)
+        indices = ensure_int64(indices)
 
+        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
+        if isinstance(maybe_slice, slice):
+            return self[maybe_slice]
 
-class TimelikeOps:
-    """"""
-    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.
-    """"""
+        return ExtensionIndex.take(
+            self, indices, axis, allow_fill, fill_value, **kwargs
+        )
 
-    _round_doc = """"""
-        Perform {op} operation on the data to the specified `freq`.
+    @doc(IndexOpsMixin.searchsorted, klass=""Datetime-like Index"")
+    def searchsorted(self, value, side=""left"", sorter=None):
+        if isinstance(value, str):
+            raise TypeError(
+                ""searchsorted requires compatible dtype or scalar, ""
+                f""not {type(value).__name__}""
+            )
+        if isinstance(value, Index):
+            value = value._data
 
-        Parameters
-        ----------
-        freq : str or Offset
-            The frequency level to {op} the index to. Must be a fixed
-            frequency like 'S' (second) not 'ME' (month end). See
-            :ref:`frequency aliases <timeseries.offset_aliases>` for
-            a list of possible `freq` values.
-        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
-            Only relevant for DatetimeIndex:
-
-            - 'infer' will attempt to infer fall dst-transition hours based on
-              order
-            - bool-ndarray where True signifies a DST time, False designates
-              a non-DST time (note that this flag is only applicable for
-              ambiguous times)
-            - 'NaT' will return NaT where there are ambiguous times
-            - 'raise' will raise an AmbiguousTimeError if there are ambiguous
-              times.
+        return self._data.searchsorted(value, side=side, sorter=sorter)
 
-            .. versionadded:: 0.24.0
+    _can_hold_na = True
 
-        nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \
-default 'raise'
-            A nonexistent time does not exist in a particular timezone
-            where clocks moved forward due to DST.
+    _na_value = NaT
+    """"""The expected NA value to use with this index.""""""
 
-            - 'shift_forward' will shift the nonexistent time forward to the
-              closest existing time
-            - 'shift_backward' will shift the nonexistent time backward to the
-              closest existing time
-            - 'NaT' will return NaT where there are nonexistent times
-            - timedelta objects will shift nonexistent times by the timedelta
-            - 'raise' will raise an NonExistentTimeError if there are
-              nonexistent times.
+    def _convert_tolerance(self, tolerance, target):
+        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())
 
-            .. versionadded:: 0.24.0
+        if target.size != tolerance.size and tolerance.size > 1:
+            raise ValueError(""list-like tolerance size must match target index size"")
+        return tolerance
 
-        Returns
-        -------
-        DatetimeIndex, TimedeltaIndex, or Series
-            Index of the same type for a DatetimeIndex or TimedeltaIndex,
-            or a Series with the same index for a Series.
+    def tolist(self) -> List:
+        """"""
+        Return a list of the underlying data.
+        """"""
+        return list(self.astype(object))
 
-        Raises
-        ------
-        ValueError if the `freq` cannot be converted.
+    def min(self, axis=None, skipna=True, *args, **kwargs):
+        """"""
+        Return the minimum value of the Index or minimum along
+        an axis.
 
-        Examples
+        See Also
         --------
-        **DatetimeIndex**
-
-        >>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')
-        >>> rng
-        DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:01:00'],
-                      dtype='datetime64[ns]', freq='T')
+        numpy.ndarray.min
+        Series.min : Return the minimum value in a Series.
         """"""
+        nv.validate_min(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    _round_example = """""">>> rng.round('H')
-        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:00:00'],
-                      dtype='datetime64[ns]', freq=None)
+        if not len(self):
+            return self._na_value
 
-        **Series**
+        i8 = self.asi8
+        try:
+            # quick check
+            if len(i8) and self.is_monotonic:
+                if i8[0] != iNaT:
+                    return self._box_func(i8[0])
+
+            if self.hasnans:
+                if skipna:
+                    min_stamp = self[~self._isnan].asi8.min()
+                else:
+                    return self._na_value
+            else:
+                min_stamp = i8.min()
+            return self._box_func(min_stamp)
+        except ValueError:
+            return self._na_value
 
-        >>> pd.Series(rng).dt.round(""H"")
-        0   2018-01-01 12:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 12:00:00
-        dtype: datetime64[ns]
+    def argmin(self, axis=None, skipna=True, *args, **kwargs):
         """"""
+        Returns the indices of the minimum values along an axis.
 
-    _floor_example = """""">>> rng.floor('H')
-        DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:00:00'],
-                      dtype='datetime64[ns]', freq=None)
+        See `numpy.ndarray.argmin` for more information on the
+        `axis` parameter.
 
-        **Series**
-
-        >>> pd.Series(rng).dt.floor(""H"")
-        0   2018-01-01 11:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 12:00:00
-        dtype: datetime64[ns]
+        See Also
+        --------
+        numpy.ndarray.argmin
         """"""
+        nv.validate_argmin(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    _ceil_example = """""">>> rng.ceil('H')
-        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 13:00:00'],
-                      dtype='datetime64[ns]', freq=None)
-
-        **Series**
+        i8 = self.asi8
+        if self.hasnans:
+            mask = self._isnan
+            if mask.all() or not skipna:
+                return -1
+            i8 = i8.copy()
+            i8[mask] = np.iinfo(""int64"").max
+        return i8.argmin()
 
-        >>> pd.Series(rng).dt.ceil(""H"")
-        0   2018-01-01 12:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 13:00:00
-        dtype: datetime64[ns]
+    def max(self, axis=None, skipna=True, *args, **kwargs):
         """"""
+        Return the maximum value of the Index or maximum along
+        an axis.
 
-    def _round(self, freq, mode, ambiguous, nonexistent):
-        # round the local times
-        if is_datetime64tz_dtype(self):
-            # operate on naive timestamps, then convert back to aware
-            naive = self.tz_localize(None)
-            result = naive._round(freq, mode, ambiguous, nonexistent)
-            aware = result.tz_localize(
-                self.tz, ambiguous=ambiguous, nonexistent=nonexistent
-            )
-            return aware
-
-        values = self.view(""i8"")
-        result = round_nsint64(values, mode, freq)
-        result = self._maybe_mask_results(result, fill_value=NaT)
-        return self._simple_new(result, dtype=self.dtype)
-
-    @Appender((_round_doc + _round_example).format(op=""round""))
-    def round(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)
+        See Also
+        --------
+        numpy.ndarray.max
+        Series.max : Return the maximum value in a Series.
+        """"""
+        nv.validate_max(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    @Appender((_round_doc + _floor_example).format(op=""floor""))
-    def floor(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)
+        if not len(self):
+            return self._na_value
 
-    @Appender((_round_doc + _ceil_example).format(op=""ceil""))
-    def ceil(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)
+        i8 = self.asi8
+        try:
+            # quick check
+            if len(i8) and self.is_monotonic:
+                if i8[-1] != iNaT:
+                    return self._box_func(i8[-1])
+
+            if self.hasnans:
+                if skipna:
+                    max_stamp = self[~self._isnan].asi8.max()
+                else:
+                    return self._na_value
+            else:
+                max_stamp = i8.max()
+            return self._box_func(max_stamp)
+        except ValueError:
+            return self._na_value
 
-    def _with_freq(self, freq):
+    def argmax(self, axis=None, skipna=True, *args, **kwargs):
         """"""
-        Helper to set our freq in-place, returning self to allow method chaining.
+        Returns the indices of the maximum values along an axis.
 
-        Parameters
-        ----------
-        freq : DateOffset, None, or ""infer""
+        See `numpy.ndarray.argmax` for more information on the
+        `axis` parameter.
 
-        Returns
-        -------
-        self
+        See Also
+        --------
+        numpy.ndarray.argmax
         """"""
-        # GH#29843
-        if freq is None:
-            # Always valid
-            pass
-        elif len(self) == 0 and isinstance(freq, DateOffset):
-            # Always valid.  In the TimedeltaArray case, we assume this
-            #  is a Tick offset.
-            pass
-        else:
-            # As an internal method, we can ensure this assertion always holds
-            assert freq == ""infer""
-            freq = frequencies.to_offset(self.inferred_freq)
-
-        self._freq = freq
-        return self
-
-
-class DatetimeLikeArrayMixin(
-    ExtensionOpsMixin, AttributesMixin, NDArrayBackedExtensionArray
-):
-    """"""
-    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray
-
-    Assumes that __new__/__init__ defines:
-        _data
-        _freq
-
-    and that the inheriting class has methods:
-        _generate_range
-    """"""
-
-    # ------------------------------------------------------------------
-    # NDArrayBackedExtensionArray compat
+        nv.validate_argmax(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    @property
-    def _ndarray(self) -> np.ndarray:
-        # NB: A bunch of Interval tests fail if we use ._data
-        return self.asi8
+        i8 = self.asi8
+        if self.hasnans:
+            mask = self._isnan
+            if mask.all() or not skipna:
+                return -1
+            i8 = i8.copy()
+            i8[mask] = 0
+        return i8.argmax()
 
-    def _from_backing_data(self: _T, arr: np.ndarray) -> _T:
-        # Note: we do not retain `freq`
-        return type(self)(arr, dtype=self.dtype)  # type: ignore
+    # --------------------------------------------------------------------
+    # Rendering Methods
 
-    # ------------------------------------------------------------------
+    def _format_with_header(self, header, na_rep=""NaT"", **kwargs):
+        return header + list(self._format_native_types(na_rep, **kwargs))
 
     @property
-    def _box_func(self):
-        """"""
-        box function to get object from internal representation
-        """"""
+    def _formatter_func(self):
         raise AbstractMethodError(self)
 
-    def _box_values(self, values):
+    def _format_attrs(self):
         """"""
-        apply box func to passed values
+        Return a list of tuples of the (attr,formatted_value).
         """"""
-        return lib.map_infer(values, self._box_func)
+        attrs = super()._format_attrs()
+        for attrib in self._attributes:
+            if attrib == ""freq"":
+                freq = self.freqstr
+                if freq is not None:
+                    freq = repr(freq)
+                attrs.append((""freq"", freq))
+        return attrs
 
-    def __iter__(self):
-        return (self._box_func(v) for v in self.asi8)
+    # --------------------------------------------------------------------
+    # Indexing Methods
 
-    @property
-    def asi8(self) -> np.ndarray:
-        """"""
-        Integer representation of the values.
-
-        Returns
-        -------
-        ndarray
-            An ndarray with int64 dtype.
-        """"""
-        # do not cache or you'll create a memory leak
-        return self._data.view(""i8"")
+    def _validate_partial_date_slice(self, reso: str):
+        raise NotImplementedError
 
-    # ----------------------------------------------------------------
-    # Rendering Methods
+    def _parsed_string_to_bounds(self, reso: str, parsed: datetime):
+        raise NotImplementedError
 
-    def _format_native_types(self, na_rep=""NaT"", date_format=None):
+    def _partial_date_slice(
+        self, reso: str, parsed: datetime, use_lhs: bool = True, use_rhs: bool = True
+    ):
         """"""
-        Helper method for astype when converting to strings.
+        Parameters
+        ----------
+        reso : str
+        parsed : datetime
+        use_lhs : bool, default True
+        use_rhs : bool, default True
 
         Returns
         -------
-        ndarray[str]
+        slice or ndarray[intp]
         """"""
-        raise AbstractMethodError(self)
-
-    def _formatter(self, boxed=False):
-        # TODO: Remove Datetime & DatetimeTZ formatters.
-        return ""'{}'"".format
+        self._validate_partial_date_slice(reso)
 
-    # ----------------------------------------------------------------
-    # Array-Like / EA-Interface Methods
+        t1, t2 = self._parsed_string_to_bounds(reso, parsed)
+        i8vals = self.asi8
+        unbox = self._data._unbox_scalar
 
-    def __array__(self, dtype=None) -> np.ndarray:
-        # used for Timedelta/DatetimeArray, overwritten by PeriodArray
-        if is_object_dtype(dtype):
-            return np.array(list(self), dtype=object)
-        return self._data
+        if self.is_monotonic:
 
-    def __getitem__(self, key):
-        """"""
-        This getitem defers to the underlying array, which by-definition can
-        only handle list-likes, slices, and integer scalars
-        """"""
+            if len(self) and (
+                (use_lhs and t1 < self[0] and t2 < self[0])
+                or ((use_rhs and t1 > self[-1] and t2 > self[-1]))
+            ):
+                # we are out of range
+                raise KeyError
 
-        if com.is_bool_indexer(key):
-            # first convert to boolean, because check_array_indexer doesn't
-            # allow object dtype
-            if is_object_dtype(key):
-                key = np.asarray(key, dtype=bool)
-
-            key = check_array_indexer(self, key)
-            key = lib.maybe_booleans_to_slice(key.view(np.uint8))
-        elif isinstance(key, list) and len(key) == 1 and isinstance(key[0], slice):
-            # see https://github.com/pandas-dev/pandas/issues/31299, need to allow
-            # this for now (would otherwise raise in check_array_indexer)
-            pass
-        else:
-            key = check_array_indexer(self, key)
+            # TODO: does this depend on being monotonic _increasing_?
 
-        freq = self._get_getitem_freq(key)
-        result = self._data[key]
-        if lib.is_scalar(result):
-            return self._box_func(result)
-        return self._simple_new(result, dtype=self.dtype, freq=freq)
+            # a monotonic (sorted) series can be sliced
+            # Use asi8.searchsorted to avoid re-validating Periods/Timestamps
+            left = i8vals.searchsorted(unbox(t1), side=""left"") if use_lhs else None
+            right = i8vals.searchsorted(unbox(t2), side=""right"") if use_rhs else None
+            return slice(left, right)
 
-    def _get_getitem_freq(self, key):
-        """"""
-        Find the `freq` attribute to assign to the result of a __getitem__ lookup.
-        """"""
-        is_period = is_period_dtype(self.dtype)
-        if is_period:
-            freq = self.freq
-        else:
-            freq = None
-            if isinstance(key, slice):
-                if self.freq is not None and key.step is not None:
-                    freq = key.step * self.freq
-                else:
-                    freq = self.freq
-            elif key is Ellipsis:
-                # GH#21282 indexing with Ellipsis is similar to a full slice,
-                #  should preserve `freq` attribute
-                freq = self.freq
-        return freq
-
-    def __setitem__(
-        self,
-        key: Union[int, Sequence[int], Sequence[bool], slice],
-        value: Union[NaTType, Any, Sequence[Any]],
-    ) -> None:
-        # I'm fudging the types a bit here. ""Any"" above really depends
-        # on type(self). For PeriodArray, it's Period (or stuff coercible
-        # to a period in from_sequence). For DatetimeArray, it's Timestamp...
-        # I don't know if mypy can do that, possibly with Generics.
-        # https://mypy.readthedocs.io/en/latest/generics.html
-        if is_list_like(value):
-            is_slice = isinstance(key, slice)
-
-            if lib.is_scalar(key):
-                raise ValueError(""setting an array element with a sequence."")
-
-            if not is_slice:
-                key = cast(Sequence, key)
-                if len(key) != len(value) and not com.is_bool_indexer(key):
-                    msg = (
-                        f""shape mismatch: value array of length '{len(key)}' ""
-                        ""does not match indexing result of length ""
-                        f""'{len(value)}'.""
-                    )
-                    raise ValueError(msg)
-                elif not len(key):
-                    return
-
-        value = self._validate_setitem_value(value)
-        key = check_array_indexer(self, key)
-        self._data[key] = value
-        self._maybe_clear_freq()
-
-    def _maybe_clear_freq(self):
-        # inplace operations like __setitem__ may invalidate the freq of
-        # DatetimeArray and TimedeltaArray
-        pass
-
-    def astype(self, dtype, copy=True):
-        # Some notes on cases we don't have to handle here in the base class:
-        #   1. PeriodArray.astype handles period -> period
-        #   2. DatetimeArray.astype handles conversion between tz.
-        #   3. DatetimeArray.astype handles datetime -> period
-        dtype = pandas_dtype(dtype)
-
-        if is_object_dtype(dtype):
-            return self._box_values(self.asi8.ravel()).reshape(self.shape)
-        elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):
-            return self._format_native_types()
-        elif is_integer_dtype(dtype):
-            # we deliberately ignore int32 vs. int64 here.
-            # See https://github.com/pandas-dev/pandas/issues/24381 for more.
-            values = self.asi8
-
-            if is_unsigned_integer_dtype(dtype):
-                # Again, we ignore int32 vs. int64
-                values = values.view(""uint64"")
-
-            if copy:
-                values = values.copy()
-            return values
-        elif (
-            is_datetime_or_timedelta_dtype(dtype)
-            and not is_dtype_equal(self.dtype, dtype)
-        ) or is_float_dtype(dtype):
-            # disallow conversion between datetime/timedelta,
-            # and conversions for any datetimelike to float
-            msg = f""Cannot cast {type(self).__name__} to dtype {dtype}""
-            raise TypeError(msg)
-        elif is_categorical_dtype(dtype):
-            arr_cls = dtype.construct_array_type()
-            return arr_cls(self, dtype=dtype)
         else:
-            return np.asarray(self, dtype=dtype)
-
-    def view(self, dtype=None):
-        if dtype is None or dtype is self.dtype:
-            return type(self)(self._data, dtype=self.dtype)
-        return self._data.view(dtype=dtype)
-
-    # ------------------------------------------------------------------
-    # ExtensionArray Interface
-
-    @classmethod
-    def _concat_same_type(cls, to_concat, axis: int = 0):
+            lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True
+            rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True
 
-        # do not pass tz to set because tzlocal cannot be hashed
-        dtypes = {str(x.dtype) for x in to_concat}
-        if len(dtypes) != 1:
-            raise ValueError(""to_concat must have the same dtype (tz)"", dtypes)
+            # try to find the dates
+            return (lhs_mask & rhs_mask).nonzero()[0]
 
-        obj = to_concat[0]
-        dtype = obj.dtype
-
-        i8values = [x.asi8 for x in to_concat]
-        values = np.concatenate(i8values, axis=axis)
-
-        new_freq = None
-        if is_period_dtype(dtype):
-            new_freq = obj.freq
-        elif axis == 0:
-            # GH 3232: If the concat result is evenly spaced, we can retain the
-            # original frequency
-            to_concat = [x for x in to_concat if len(x)]
-
-            if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):
-                pairs = zip(to_concat[:-1], to_concat[1:])
-                if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):
-                    new_freq = obj.freq
-
-        return cls._simple_new(values, dtype=dtype, freq=new_freq)
-
-    def copy(self):
-        values = self.asi8.copy()
-        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)
-
-    def _values_for_factorize(self):
-        return self.asi8, iNaT
-
-    @classmethod
-    def _from_factorized(cls, values, original):
-        return cls(values, dtype=original.dtype)
-
-    def _values_for_argsort(self):
-        return self._data
-
-    @Appender(ExtensionArray.shift.__doc__)
-    def shift(self, periods=1, fill_value=None, axis=0):
-        if not self.size or periods == 0:
-            return self.copy()
+    # --------------------------------------------------------------------
+    # Arithmetic Methods
 
-        fill_value = self._validate_shift_value(fill_value)
-        new_values = shift(self._data, periods, axis, fill_value)
+    def _get_addsub_freq(self, other, result) -> Optional[DateOffset]:
+        """"""
+        Find the freq we expect the result of an addition/subtraction operation
+        to have.
+        """"""
+        if is_period_dtype(self.dtype):
+            if is_period_dtype(result.dtype):
+                # Only used for ops that stay PeriodDtype
+                return self.freq
+            return None
+        elif self.freq is None:
+            return None
+        elif lib.is_scalar(other) and isna(other):
+            return None
 
-        return type(self)._simple_new(new_values, dtype=self.dtype)
+        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
+            new_freq = None
+            if isinstance(self.freq, Tick):
+                new_freq = self.freq
+            return new_freq
 
-    # ------------------------------------------------------------------
-    # Validation Methods
-    # TODO: try to de-duplicate these, ensure identical behavior
+        elif isinstance(other, DateOffset):
+            # otherwise just DatetimeArray
+            return None  # TODO: Should we infer if it matches self.freq * n?
+        elif isinstance(other, (datetime, np.datetime64)):
+            return self.freq
 
-    def _validate_fill_value(self, fill_value):
-        """"""
-        If a fill_value is passed to `take` convert it to an i8 representation,
-        raising ValueError if this is not possible.
+        elif is_timedelta64_dtype(other):
+            return None  # TODO: shouldnt we be able to do self.freq + other.freq?
+        elif is_object_dtype(other):
+            return None  # TODO: is this quite right?  sometimes we unpack singletons
+        elif is_datetime64_any_dtype(other):
+            return None  # TODO: shouldnt we be able to do self.freq + other.freq?
+        else:
+            raise NotImplementedError
+
+    __add__ = _make_wrapped_arith_op_with_freq(""__add__"")
+    __sub__ = _make_wrapped_arith_op_with_freq(""__sub__"")
+    __radd__ = make_wrapped_arith_op(""__radd__"")
+    __rsub__ = make_wrapped_arith_op(""__rsub__"")
+    __pow__ = make_wrapped_arith_op(""__pow__"")
+    __rpow__ = make_wrapped_arith_op(""__rpow__"")
+    __mul__ = make_wrapped_arith_op(""__mul__"")
+    __rmul__ = make_wrapped_arith_op(""__rmul__"")
+    __floordiv__ = make_wrapped_arith_op(""__floordiv__"")
+    __rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
+    __mod__ = make_wrapped_arith_op(""__mod__"")
+    __rmod__ = make_wrapped_arith_op(""__rmod__"")
+    __divmod__ = make_wrapped_arith_op(""__divmod__"")
+    __rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
+    __truediv__ = make_wrapped_arith_op(""__truediv__"")
+    __rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
+
+    def isin(self, values, level=None):
+        """"""
+        Compute boolean array of whether each index value is found in the
+        passed set of values.
 
         Parameters
         ----------
-        fill_value : object
+        values : set or sequence of values
 
         Returns
         -------
-        fill_value : np.int64
-
-        Raises
-        ------
-        ValueError
+        is_contained : ndarray (boolean dtype)
         """"""
-        if is_valid_nat_for_dtype(fill_value, self.dtype):
-            fill_value = iNaT
-        elif isinstance(fill_value, self._recognized_scalars):
-            self._check_compatible_with(fill_value)
-            fill_value = self._scalar_type(fill_value)
-            fill_value = self._unbox_scalar(fill_value)
-        else:
-            raise ValueError(
-                f""'fill_value' should be a {self._scalar_type}. ""
-                f""Got '{str(fill_value)}'.""
-            )
-        return fill_value
-
-    def _validate_shift_value(self, fill_value):
-        # TODO(2.0): once this deprecation is enforced, used _validate_fill_value
-        if is_valid_nat_for_dtype(fill_value, self.dtype):
-            fill_value = NaT
-        elif not isinstance(fill_value, self._recognized_scalars):
-            # only warn if we're not going to raise
-            if self._scalar_type is Period and lib.is_integer(fill_value):
-                # kludge for #31971 since Period(integer) tries to cast to str
-                new_fill = Period._from_ordinal(fill_value, freq=self.freq)
-            else:
-                new_fill = self._scalar_type(fill_value)
-
-            # stacklevel here is chosen to be correct when called from
-            #  DataFrame.shift or Series.shift
-            warnings.warn(
-                f""Passing {type(fill_value)} to shift is deprecated and ""
-                ""will raise in a future version, pass ""
-                f""{self._scalar_type.__name__} instead."",
-                FutureWarning,
-                stacklevel=10,
-            )
-            fill_value = new_fill
-
-        fill_value = self._unbox_scalar(fill_value)
-        return fill_value
+        if level is not None:
+            self._validate_index_level(level)
 
-    def _validate_searchsorted_value(self, value):
-        if isinstance(value, str):
+        if not isinstance(values, type(self)):
             try:
-                value = self._scalar_from_string(value)
-            except ValueError as err:
-                raise TypeError(
-                    ""searchsorted requires compatible dtype or scalar""
-                ) from err
-
-        elif is_valid_nat_for_dtype(value, self.dtype):
-            value = NaT
-
-        elif isinstance(value, self._recognized_scalars):
-            value = self._scalar_type(value)
-
-        elif is_list_like(value) and not isinstance(value, type(self)):
-            value = array(value)
-
-            if not type(self)._is_recognized_dtype(value):
-                raise TypeError(
-                    ""searchsorted requires compatible dtype or scalar, ""
-                    f""not {type(value).__name__}""
-                )
-
-        if not (isinstance(value, (self._scalar_type, type(self))) or (value is NaT)):
-            raise TypeError(f""Unexpected type for 'value': {type(value)}"")
-
-        if isinstance(value, type(self)):
-            self._check_compatible_with(value)
-            value = value.asi8
-        else:
-            value = self._unbox_scalar(value)
-
-        return value
-
-    def _validate_setitem_value(self, value):
-        if lib.is_scalar(value) and not isna(value):
-            value = com.maybe_box_datetimelike(value)
-
-        if is_list_like(value):
-            value = type(self)._from_sequence(value, dtype=self.dtype)
-            self._check_compatible_with(value, setitem=True)
-            value = value.asi8
-        elif isinstance(value, self._scalar_type):
-            self._check_compatible_with(value, setitem=True)
-            value = self._unbox_scalar(value)
-        elif is_valid_nat_for_dtype(value, self.dtype):
-            value = iNaT
-        else:
-            msg = (
-                f""'value' should be a '{self._scalar_type.__name__}', 'NaT', ""
-                f""or array of those. Got '{type(value).__name__}' instead.""
-            )
-            raise TypeError(msg)
-
-        return value
-
-    def _validate_insert_value(self, value):
-        if isinstance(value, self._recognized_scalars):
-            value = self._scalar_type(value)
-            self._check_compatible_with(value, setitem=True)
-            # TODO: if we dont have compat, should we raise or astype(object)?
-            #  PeriodIndex does astype(object)
-        elif is_valid_nat_for_dtype(value, self.dtype):
-            # GH#18295
-            value = NaT
-        else:
-            raise TypeError(
-                f""cannot insert {type(self).__name__} with incompatible label""
-            )
-
-        return value
+                values = type(self)(values)
+            except ValueError:
+                return self.astype(object).isin(values)
 
-    def _validate_where_value(self, other):
-        if is_valid_nat_for_dtype(other, self.dtype):
-            other = NaT
-        elif isinstance(other, self._recognized_scalars):
-            other = self._scalar_type(other)
-            self._check_compatible_with(other, setitem=True)
-        elif not is_list_like(other):
-            raise TypeError(f""Where requires matching dtype, not {type(other)}"")
+        return algorithms.isin(self.asi8, values.asi8)
 
-        else:
-            # Do type inference if necessary up front
-            # e.g. we passed PeriodIndex.values and got an ndarray of Periods
-            other = array(other)
-            other = extract_array(other, extract_numpy=True)
-
-            if is_categorical_dtype(other.dtype):
-                # e.g. we have a Categorical holding self.dtype
-                if is_dtype_equal(other.categories.dtype, self.dtype):
-                    other = other._internal_get_values()
-
-            if not type(self)._is_recognized_dtype(other.dtype):
-                raise TypeError(f""Where requires matching dtype, not {other.dtype}"")
-            self._check_compatible_with(other, setitem=True)
-
-        if lib.is_scalar(other):
-            other = self._unbox_scalar(other)
-        else:
-            other = other.view(""i8"")
+    @Appender(Index.where.__doc__)
+    def where(self, cond, other=None):
+        values = self.view(""i8"")
 
-        return other
+        try:
+            other = self._data._validate_where_value(other)
+        except (TypeError, ValueError) as err:
+            # Includes tzawareness mismatch and IncompatibleFrequencyError
+            oth = getattr(other, ""dtype"", other)
+            raise TypeError(f""Where requires matching dtype, not {oth}"") from err
 
-    # ------------------------------------------------------------------
-    # Additional array methods
-    #  These are not part of the EA API, but we implement them because
-    #  pandas assumes they're there.
+        result = np.where(cond, values, other).astype(""i8"")
+        arr = type(self._data)._simple_new(result, dtype=self.dtype)
+        return type(self)._simple_new(arr, name=self.name)
 
-    def searchsorted(self, value, side=""left"", sorter=None):
+    def _summary(self, name=None) -> str:
         """"""
-        Find indices where elements should be inserted to maintain order.
-
-        Find the indices into a sorted array `self` such that, if the
-        corresponding elements in `value` were inserted before the indices,
-        the order of `self` would be preserved.
+        Return a summarized representation.
 
         Parameters
         ----------
-        value : array_like
-            Values to insert into `self`.
-        side : {'left', 'right'}, optional
-            If 'left', the index of the first suitable location found is given.
-            If 'right', return the last such index.  If there is no suitable
-            index, return either 0 or N (where N is the length of `self`).
-        sorter : 1-D array_like, optional
-            Optional array of integer indices that sort `self` into ascending
-            order. They are typically the result of ``np.argsort``.
+        name : str
+            Name to use in the summary representation.
 
         Returns
         -------
-        indices : array of ints
-            Array of insertion points with the same shape as `value`.
+        str
+            Summarized representation of the index.
         """"""
-        value = self._validate_searchsorted_value(value)
+        formatter = self._formatter_func
+        if len(self) > 0:
+            index_summary = f"", {formatter(self[0])} to {formatter(self[-1])}""
+        else:
+            index_summary = """"
 
-        # TODO: Use datetime64 semantics for sorting, xref GH#29844
-        return self.asi8.searchsorted(value, side=side, sorter=sorter)
+        if name is None:
+            name = type(self).__name__
+        result = f""{name}: {len(self)} entries{index_summary}""
+        if self.freq:
+            result += f""\nFreq: {self.freqstr}""
 
-    def value_counts(self, dropna=False):
+        # display as values, not quoted
+        result = result.replace(""'"", """")
+        return result
+
+    def shift(self, periods=1, freq=None):
         """"""
-        Return a Series containing counts of unique values.
+        Shift index by desired number of time frequency increments.
+
+        This method is for shifting the values of datetime-like indexes
+        by a specified time increment a given number of times.
 
         Parameters
         ----------
-        dropna : bool, default True
-            Don't include counts of NaT values.
+        periods : int, default 1
+            Number of periods (or increments) to shift by,
+            can be positive or negative.
 
-        Returns
-        -------
-        Series
-        """"""
-        from pandas import Series, Index
+            .. versionchanged:: 0.24.0
 
-        if dropna:
-            values = self[~self.isna()]._data
-        else:
-            values = self._data
+        freq : pandas.DateOffset, pandas.Timedelta or string, optional
+            Frequency increment to shift by.
+            If None, the index is shifted by its own `freq` attribute.
+            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.
 
-        cls = type(self)
+        Returns
+        -------
+        pandas.DatetimeIndex
+            Shifted index.
 
-        result = value_counts(values, sort=False, dropna=dropna)
-        index = Index(
-            cls(result.index.view(""i8""), dtype=self.dtype), name=result.index.name
-        )
-        return Series(result._values, index=index, name=result.name)
+        See Also
+        --------
+        Index.shift : Shift values of Index.
+        PeriodIndex.shift : Shift values of PeriodIndex.
+        """"""
+        arr = self._data.view()
+        arr._freq = self.freq
+        result = arr._time_shift(periods, freq=freq)
+        return type(self)(result, name=self.name)
 
-    def map(self, mapper):
-        # TODO(GH-23179): Add ExtensionArray.map
-        # Need to figure out if we want ExtensionArray.map first.
-        # If so, then we can refactor IndexOpsMixin._map_values to
-        # a standalone function and call from here..
-        # Else, just rewrite _map_infer_values to do the right thing.
-        from pandas import Index
+    # --------------------------------------------------------------------
+    # List-like Methods
 
-        return Index(self).map(mapper).array
+    def delete(self, loc):
+        new_i8s = np.delete(self.asi8, loc)
 
-    # ------------------------------------------------------------------
-    # Null Handling
+        freq = None
+        if is_period_dtype(self):
+            freq = self.freq
+        elif is_integer(loc):
+            if loc in (0, -len(self), -1, len(self) - 1):
+                freq = self.freq
+        else:
+            if is_list_like(loc):
+                loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))
+            if isinstance(loc, slice) and loc.step in (1, None):
+                if loc.start in (0, None) or loc.stop in (len(self), None):
+                    freq = self.freq
 
-    def isna(self):
-        return self._isnan
+        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
+        return type(self)._simple_new(arr, name=self.name)
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _isnan(self):
-        """"""
-        return if each value is nan
-        """"""
-        return self.asi8 == iNaT
+    # --------------------------------------------------------------------
+    # Join/Set Methods
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _hasnans(self):
-        """"""
-        return if I have any nans; enables various perf speedups
-        """"""
-        return bool(self._isnan.any())
+    def _wrap_joined_index(self, joined: np.ndarray, other):
+        assert other.dtype == self.dtype, (other.dtype, self.dtype)
+        name = get_op_result_name(self, other)
 
-    def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):
-        """"""
-        Parameters
-        ----------
-        result : a ndarray
-        fill_value : object, default iNaT
-        convert : str, dtype or None
+        if is_period_dtype(self.dtype):
+            freq = self.freq
+        else:
+            self = cast(DatetimeTimedeltaMixin, self)
+            freq = self.freq if self._can_fast_union(other) else None
+        new_data = type(self._data)._simple_new(joined, dtype=self.dtype, freq=freq)
 
-        Returns
-        -------
-        result : ndarray with values replace by the fill_value
+        return type(self)._simple_new(new_data, name=name)
 
-        mask the result if needed, convert to the provided dtype if its not
-        None
 
-        This is an internal routine.
-        """"""
-        if self._hasnans:
-            if convert:
-                result = result.astype(convert)
-            if fill_value is None:
-                fill_value = np.nan
-            result[self._isnan] = fill_value
-        return result
-
-    def fillna(self, value=None, method=None, limit=None):
-        # TODO(GH-20300): remove this
-        # Just overriding to ensure that we avoid an astype(object).
-        # Either 20300 or a `_values_for_fillna` would avoid this duplication.
-        if isinstance(value, ABCSeries):
-            value = value.array
-
-        value, method = validate_fillna_kwargs(value, method)
-
-        mask = self.isna()
-
-        if is_array_like(value):
-            if len(value) != len(self):
-                raise ValueError(
-                    f""Length of 'value' does not match. Got ({len(value)}) ""
-                    f"" expected {len(self)}""
-                )
-            value = value[mask]
-
-        if mask.any():
-            if method is not None:
-                if method == ""pad"":
-                    func = missing.pad_1d
-                else:
-                    func = missing.backfill_1d
-
-                values = self._data
-                if not is_period_dtype(self):
-                    # For PeriodArray self._data is i8, which gets copied
-                    #  by `func`.  Otherwise we need to make a copy manually
-                    # to avoid modifying `self` in-place.
-                    values = values.copy()
-
-                new_values = func(values, limit=limit, mask=mask)
-                if is_datetime64tz_dtype(self):
-                    # we need to pass int64 values to the constructor to avoid
-                    #  re-localizing incorrectly
-                    new_values = new_values.view(""i8"")
-                new_values = type(self)(new_values, dtype=self.dtype)
-            else:
-                # fill with value
-                new_values = self.copy()
-                new_values[mask] = value
-        else:
-            new_values = self.copy()
-        return new_values
+class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):
+    """"""
+    Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,
+    but not PeriodIndex
+    """"""
 
-    # ------------------------------------------------------------------
-    # Frequency Properties/Methods
+    # Compat for frequency inference, see GH#23789
+    _is_monotonic_increasing = Index.is_monotonic_increasing
+    _is_monotonic_decreasing = Index.is_monotonic_decreasing
+    _is_unique = Index.is_unique
+    _freq = lib.no_default
 
     @property
     def freq(self):
         """"""
-        Return the frequency object if it is set, otherwise None.
+        In limited circumstances, our freq may differ from that of our _data.
         """"""
-        return self._freq
-
-    @freq.setter
-    def freq(self, value):
-        if value is not None:
-            value = frequencies.to_offset(value)
-            self._validate_frequency(self, value)
-
-        self._freq = value
+        if self._freq is not lib.no_default:
+            return self._freq
+        return self._data.freq
 
     @property
     def freqstr(self):
@@ -1059,708 +677,309 @@ class DatetimeLikeArrayMixin(
             return None
         return self.freq.freqstr
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def inferred_freq(self):
-        """"""
-        Tryies to return a string representing a frequency guess,
-        generated by infer_freq.  Returns None if it can't autodetect the
-        frequency.
-        """"""
-        if self.ndim != 1:
-            return None
-        try:
-            return frequencies.infer_freq(self)
-        except ValueError:
-            return None
-
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _resolution(self):
-        return frequencies.Resolution.get_reso_from_freq(self.freqstr)
-
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def resolution(self):
-        """"""
-        Returns day, hour, minute, second, millisecond or microsecond
-        """"""
-        return frequencies.Resolution.get_str(self._resolution)
-
-    @classmethod
-    def _validate_frequency(cls, index, freq, **kwargs):
-        """"""
-        Validate that a frequency is compatible with the values of a given
-        Datetime Array/Index or Timedelta Array/Index
-
-        Parameters
-        ----------
-        index : DatetimeIndex or TimedeltaIndex
-            The index on which to determine if the given frequency is valid
-        freq : DateOffset
-            The frequency to validate
-        """"""
-        if is_period_dtype(cls):
-            # Frequency validation is not meaningful for Period Array/Index
-            return None
-
-        inferred = index.inferred_freq
-        if index.size == 0 or inferred == freq.freqstr:
-            return None
-
-        try:
-            on_freq = cls._generate_range(
-                start=index[0], end=None, periods=len(index), freq=freq, **kwargs
-            )
-            if not np.array_equal(index.asi8, on_freq.asi8):
-                raise ValueError
-        except ValueError as e:
-            if ""non-fixed"" in str(e):
-                # non-fixed frequencies are not meaningful for timedelta64;
-                #  we retain that error message
-                raise e
-            # GH#11587 the main way this is reached is if the `np.array_equal`
-            #  check above is False.  This can also be reached if index[0]
-            #  is `NaT`, in which case the call to `cls._generate_range` will
-            #  raise a ValueError, which we re-raise with a more targeted
-            #  message.
-            raise ValueError(
-                f""Inferred frequency {inferred} from passed values ""
-                f""does not conform to passed frequency {freq.freqstr}""
-            ) from e
-
-    # monotonicity/uniqueness properties are called via frequencies.infer_freq,
-    #  see GH#23789
-
-    @property
-    def _is_monotonic_increasing(self):
-        return algos.is_monotonic(self.asi8, timelike=True)[0]
-
-    @property
-    def _is_monotonic_decreasing(self):
-        return algos.is_monotonic(self.asi8, timelike=True)[1]
-
-    @property
-    def _is_unique(self):
-        return len(unique1d(self.asi8)) == len(self)
-
-    # ------------------------------------------------------------------
-    # Arithmetic Methods
-    _create_comparison_method = classmethod(_datetimelike_array_cmp)
-
-    # pow is invalid for all three subclasses; TimedeltaArray will override
-    #  the multiplication and division ops
-    __pow__ = make_invalid_op(""__pow__"")
-    __rpow__ = make_invalid_op(""__rpow__"")
-    __mul__ = make_invalid_op(""__mul__"")
-    __rmul__ = make_invalid_op(""__rmul__"")
-    __truediv__ = make_invalid_op(""__truediv__"")
-    __rtruediv__ = make_invalid_op(""__rtruediv__"")
-    __floordiv__ = make_invalid_op(""__floordiv__"")
-    __rfloordiv__ = make_invalid_op(""__rfloordiv__"")
-    __mod__ = make_invalid_op(""__mod__"")
-    __rmod__ = make_invalid_op(""__rmod__"")
-    __divmod__ = make_invalid_op(""__divmod__"")
-    __rdivmod__ = make_invalid_op(""__rdivmod__"")
-
-    def _add_datetimelike_scalar(self, other):
-        # Overridden by TimedeltaArray
-        raise TypeError(f""cannot add {type(self).__name__} and {type(other).__name__}"")
-
-    _add_datetime_arraylike = _add_datetimelike_scalar
-
-    def _sub_datetimelike_scalar(self, other):
-        # Overridden by DatetimeArray
-        assert other is not NaT
-        raise TypeError(f""cannot subtract a datelike from a {type(self).__name__}"")
-
-    _sub_datetime_arraylike = _sub_datetimelike_scalar
-
-    def _sub_period(self, other):
-        # Overridden by PeriodArray
-        raise TypeError(f""cannot subtract Period from a {type(self).__name__}"")
-
-    def _add_offset(self, offset):
-        raise AbstractMethodError(self)
-
-    def _add_timedeltalike_scalar(self, other):
-        """"""
-        Add a delta of a timedeltalike
-
-        Returns
-        -------
-        Same type as self
-        """"""
-        if isna(other):
-            # i.e np.timedelta64(""NaT""), not recognized by delta_to_nanoseconds
-            new_values = np.empty(self.shape, dtype=""i8"")
-            new_values[:] = iNaT
-            return type(self)(new_values, dtype=self.dtype)
-
-        inc = delta_to_nanoseconds(other)
-        new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(
-            ""i8""
-        )
-        new_values = self._maybe_mask_results(new_values)
-
-        new_freq = None
-        if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):
-            # adding a scalar preserves freq
-            new_freq = self.freq
-
-        return type(self)(new_values, dtype=self.dtype, freq=new_freq)
+    def _with_freq(self, freq):
+        arr = self._data._with_freq(freq)
+        return type(self)._simple_new(arr, name=self.name)
 
-    def _add_timedelta_arraylike(self, other):
-        """"""
-        Add a delta of a TimedeltaIndex
+    def _shallow_copy(self, values=None, name: Label = lib.no_default):
+        name = self.name if name is lib.no_default else name
+        cache = self._cache.copy() if values is None else {}
 
-        Returns
-        -------
-        Same type as self
-        """"""
-        # overridden by PeriodArray
-
-        if len(self) != len(other):
-            raise ValueError(""cannot add indices of unequal length"")
+        if values is None:
+            values = self._data
 
-        if isinstance(other, np.ndarray):
-            # ndarray[timedelta64]; wrap in TimedeltaIndex for op
-            from pandas.core.arrays import TimedeltaArray
+        if isinstance(values, np.ndarray):
+            # TODO: We would rather not get here
+            values = type(self._data)(values, dtype=self.dtype)
 
-            other = TimedeltaArray._from_sequence(other)
+        result = type(self)._simple_new(values, name=name)
+        result._cache = cache
+        return result
 
-        self_i8 = self.asi8
-        other_i8 = other.asi8
-        new_values = checked_add_with_arr(
-            self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan
-        )
-        if self._hasnans or other._hasnans:
-            mask = (self._isnan) | (other._isnan)
-            new_values[mask] = iNaT
+    # --------------------------------------------------------------------
+    # Set Operation Methods
 
-        return type(self)(new_values, dtype=self.dtype)
+    @Appender(Index.difference.__doc__)
+    def difference(self, other, sort=None):
+        new_idx = super().difference(other, sort=sort)._with_freq(None)
+        return new_idx
 
-    def _add_nat(self):
+    def intersection(self, other, sort=False):
         """"""
-        Add pd.NaT to self
-        """"""
-        if is_period_dtype(self):
-            raise TypeError(
-                f""Cannot add {type(self).__name__} and {type(NaT).__name__}""
-            )
-
-        # GH#19124 pd.NaT is treated like a timedelta for both timedelta
-        # and datetime dtypes
-        result = np.zeros(self.shape, dtype=np.int64)
-        result.fill(iNaT)
-        return type(self)(result, dtype=self.dtype, freq=None)
+        Specialized intersection for DatetimeIndex/TimedeltaIndex.
 
-    def _sub_nat(self):
-        """"""
-        Subtract pd.NaT from self
-        """"""
-        # GH#19124 Timedelta - datetime is not in general well-defined.
-        # We make an exception for pd.NaT, which in this case quacks
-        # like a timedelta.
-        # For datetime64 dtypes by convention we treat NaT as a datetime, so
-        # this subtraction returns a timedelta64 dtype.
-        # For period dtype, timedelta64 is a close-enough return dtype.
-        result = np.zeros(self.shape, dtype=np.int64)
-        result.fill(iNaT)
-        return result.view(""timedelta64[ns]"")
-
-    def _sub_period_array(self, other):
-        """"""
-        Subtract a Period Array/Index from self.  This is only valid if self
-        is itself a Period Array/Index, raises otherwise.  Both objects must
-        have the same frequency.
+        May be much faster than Index.intersection
 
         Parameters
         ----------
-        other : PeriodIndex or PeriodArray
-
-        Returns
-        -------
-        result : np.ndarray[object]
-            Array of DateOffset objects; nulls represented by NaT.
-        """"""
-        if not is_period_dtype(self):
-            raise TypeError(
-                f""cannot subtract {other.dtype}-dtype from {type(self).__name__}""
-            )
+        other : Same type as self or array-like
+        sort : False or None, default False
+            Sort the resulting index if possible.
 
-        if self.freq != other.freq:
-            msg = DIFFERENT_FREQ.format(
-                cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr
-            )
-            raise IncompatibleFrequency(msg)
+            .. versionadded:: 0.24.0
 
-        new_values = checked_add_with_arr(
-            self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan
-        )
+            .. versionchanged:: 0.24.1
 
-        new_values = np.array([self.freq.base * x for x in new_values])
-        if self._hasnans or other._hasnans:
-            mask = (self._isnan) | (other._isnan)
-            new_values[mask] = NaT
-        return new_values
+               Changed the default to ``False`` to match the behaviour
+               from before 0.24.0.
 
-    def _addsub_object_array(self, other: np.ndarray, op):
-        """"""
-        Add or subtract array-like of DateOffset objects
+            .. versionchanged:: 0.25.0
 
-        Parameters
-        ----------
-        other : np.ndarray[object]
-        op : {operator.add, operator.sub}
+               The `sort` keyword is added
 
         Returns
         -------
-        result : same class as self
+        y : Index or same type as self
         """"""
-        assert op in [operator.add, operator.sub]
-        if len(other) == 1:
-            return op(self, other[0])
-
-        warnings.warn(
-            ""Adding/subtracting array of DateOffsets to ""
-            f""{type(self).__name__} not vectorized"",
-            PerformanceWarning,
-        )
+        self._validate_sort_keyword(sort)
+        self._assert_can_do_setop(other)
 
-        # Caller is responsible for broadcasting if necessary
-        assert self.shape == other.shape, (self.shape, other.shape)
-
-        res_values = op(self.astype(""O""), np.array(other))
-        result = array(res_values.ravel())
-        result = extract_array(result, extract_numpy=True).reshape(self.shape)
-        return result
+        if self.equals(other):
+            return self._get_reconciled_name_object(other)
 
-    def _time_shift(self, periods, freq=None):
-        """"""
-        Shift each value by `periods`.
-
-        Note this is different from ExtensionArray.shift, which
-        shifts the *position* of each element, padding the end with
-        missing values.
+        if len(self) == 0:
+            return self.copy()
+        if len(other) == 0:
+            return other.copy()
+
+        if not isinstance(other, type(self)):
+            result = Index.intersection(self, other, sort=sort)
+            if isinstance(result, type(self)):
+                if result.freq is None:
+                    result = result._with_freq(""infer"")
+            return result
 
-        Parameters
-        ----------
-        periods : int
-            Number of periods to shift by.
-        freq : pandas.DateOffset, pandas.Timedelta, or str
-            Frequency increment to shift by.
-        """"""
-        if freq is not None and freq != self.freq:
-            if isinstance(freq, str):
-                freq = frequencies.to_offset(freq)
-            offset = periods * freq
-            result = self + offset
+        elif (
+            other.freq is None
+            or self.freq is None
+            or other.freq != self.freq
+            or not other.freq.is_anchored()
+            or (not self.is_monotonic or not other.is_monotonic)
+        ):
+            result = Index.intersection(self, other, sort=sort)
+            result = result._with_freq(""infer"")
             return result
 
-        if periods == 0:
-            # immutable so OK
-            return self.copy()
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        else:
+            left, right = other, self
 
-        if self.freq is None:
-            raise NullFrequencyError(""Cannot shift with no freq"")
+        # after sorting, the intersection always starts with the right index
+        # and ends with the index of which the last elements is smallest
+        end = min(left[-1], right[-1])
+        start = right[0]
 
-        start = self[0] + periods * self.freq
-        end = self[-1] + periods * self.freq
+        if end < start:
+            return type(self)(data=[], dtype=self.dtype, freq=self.freq)
+        else:
+            lslice = slice(*left.slice_locs(start, end))
+            left_chunk = left._values[lslice]
+            return self._shallow_copy(left_chunk)
 
-        # Note: in the DatetimeTZ case, _generate_range will infer the
-        #  appropriate timezone from `start` and `end`, so tz does not need
-        #  to be passed explicitly.
-        return self._generate_range(start=start, end=end, periods=None, freq=self.freq)
+    def _can_fast_union(self, other) -> bool:
+        if not isinstance(other, type(self)):
+            return False
 
-    @unpack_zerodim_and_defer(""__add__"")
-    def __add__(self, other):
+        freq = self.freq
 
-        # scalar others
-        if other is NaT:
-            result = self._add_nat()
-        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
-            result = self._add_timedeltalike_scalar(other)
-        elif isinstance(other, DateOffset):
-            # specifically _not_ a Tick
-            result = self._add_offset(other)
-        elif isinstance(other, (datetime, np.datetime64)):
-            result = self._add_datetimelike_scalar(other)
-        elif lib.is_integer(other):
-            # This check must come after the check for np.timedelta64
-            # as is_integer returns True for these
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._time_shift(other)
-
-        # array-like others
-        elif is_timedelta64_dtype(other):
-            # TimedeltaIndex, ndarray[timedelta64]
-            result = self._add_timedelta_arraylike(other)
-        elif is_object_dtype(other):
-            # e.g. Array/Index of DateOffset objects
-            result = self._addsub_object_array(other, operator.add)
-        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
-            # DatetimeIndex, ndarray[datetime64]
-            return self._add_datetime_arraylike(other)
-        elif is_integer_dtype(other):
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._addsub_int_array(other, operator.add)
-        else:
-            # Includes Categorical, other ExtensionArrays
-            # For PeriodDtype, if self is a TimedeltaArray and other is a
-            #  PeriodArray with  a timedelta-like (i.e. Tick) freq, this
-            #  operation is valid.  Defer to the PeriodArray implementation.
-            #  In remaining cases, this will end up raising TypeError.
-            return NotImplemented
+        if freq is None or freq != other.freq:
+            return False
 
-        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
-            from pandas.core.arrays import TimedeltaArray
+        if not self.is_monotonic or not other.is_monotonic:
+            return False
 
-            return TimedeltaArray(result)
-        return result
+        if len(self) == 0 or len(other) == 0:
+            return True
 
-    def __radd__(self, other):
-        # alias for __add__
-        return self.__add__(other)
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        else:
+            left, right = other, self
 
-    @unpack_zerodim_and_defer(""__sub__"")
-    def __sub__(self, other):
+        right_start = right[0]
+        left_end = left[-1]
 
-        # scalar others
-        if other is NaT:
-            result = self._sub_nat()
-        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
-            result = self._add_timedeltalike_scalar(-other)
-        elif isinstance(other, DateOffset):
-            # specifically _not_ a Tick
-            result = self._add_offset(-other)
-        elif isinstance(other, (datetime, np.datetime64)):
-            result = self._sub_datetimelike_scalar(other)
-        elif lib.is_integer(other):
-            # This check must come after the check for np.timedelta64
-            # as is_integer returns True for these
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._time_shift(-other)
-
-        elif isinstance(other, Period):
-            result = self._sub_period(other)
-
-        # array-like others
-        elif is_timedelta64_dtype(other):
-            # TimedeltaIndex, ndarray[timedelta64]
-            result = self._add_timedelta_arraylike(-other)
-        elif is_object_dtype(other):
-            # e.g. Array/Index of DateOffset objects
-            result = self._addsub_object_array(other, operator.sub)
-        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
-            # DatetimeIndex, ndarray[datetime64]
-            result = self._sub_datetime_arraylike(other)
-        elif is_period_dtype(other):
-            # PeriodIndex
-            result = self._sub_period_array(other)
-        elif is_integer_dtype(other):
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._addsub_int_array(other, operator.sub)
+        # Only need to ""adjoin"", not overlap
+        try:
+            return (right_start == left_end + freq) or right_start in left
+        except ValueError:
+            # if we are comparing a freq that does not propagate timezones
+            # this will raise
+            return False
+
+    def _fast_union(self, other, sort=None):
+        if len(other) == 0:
+            return self.view(type(self))
+
+        if len(self) == 0:
+            return other.view(type(self))
+
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        elif sort is False:
+            # TDIs are not in the ""correct"" order and we don't want
+            #  to sort but want to remove overlaps
+            left, right = self, other
+            left_start = left[0]
+            loc = right.searchsorted(left_start, side=""left"")
+            right_chunk = right._values[:loc]
+            dates = concat_compat((left._values, right_chunk))
+            # TODO: can we infer that it has self.freq?
+            result = self._shallow_copy(dates)._with_freq(""infer"")
+            return result
         else:
-            # Includes ExtensionArrays, float_dtype
-            return NotImplemented
+            left, right = other, self
+
+        left_end = left[-1]
+        right_end = right[-1]
+
+        # concatenate
+        if left_end < right_end:
+            loc = right.searchsorted(left_end, side=""right"")
+            right_chunk = right._values[loc:]
+            dates = concat_compat([left._values, right_chunk])
+            # TODO: can we infer that it has self.freq?
+            result = self._shallow_copy(dates)._with_freq(""infer"")
+            return result
+        else:
+            return left
 
-        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
-            from pandas.core.arrays import TimedeltaArray
+    def _union(self, other, sort):
+        if not len(other) or self.equals(other) or not len(self):
+            return super()._union(other, sort=sort)
 
-            return TimedeltaArray(result)
-        return result
+        # We are called by `union`, which is responsible for this validation
+        assert isinstance(other, type(self))
 
-    def __rsub__(self, other):
-        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):
-            # ndarray[datetime64] cannot be subtracted from self, so
-            # we need to wrap in DatetimeArray/Index and flip the operation
-            if lib.is_scalar(other):
-                # i.e. np.datetime64 object
-                return Timestamp(other) - self
-            if not isinstance(other, DatetimeLikeArrayMixin):
-                # Avoid down-casting DatetimeIndex
-                from pandas.core.arrays import DatetimeArray
-
-                other = DatetimeArray(other)
-            return other - self
-        elif (
-            is_datetime64_any_dtype(self.dtype)
-            and hasattr(other, ""dtype"")
-            and not is_datetime64_any_dtype(other.dtype)
-        ):
-            # GH#19959 datetime - datetime is well-defined as timedelta,
-            # but any other type - datetime is not well-defined.
-            raise TypeError(
-                f""cannot subtract {type(self).__name__} from {type(other).__name__}""
-            )
-        elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):
-            # TODO: Can we simplify/generalize these cases at all?
-            raise TypeError(f""cannot subtract {type(self).__name__} from {other.dtype}"")
-        elif is_timedelta64_dtype(self.dtype):
-            if lib.is_integer(other) or is_integer_dtype(other):
-                # need to subtract before negating, since that flips freq
-                # -self flips self.freq, messing up results
-                return -(self - other)
-
-            return (-self) + other
-
-        return -(self - other)
-
-    def __iadd__(self, other):
-        result = self + other
-        self[:] = result[:]
-
-        if not is_period_dtype(self):
-            # restore freq, which is invalidated by setitem
-            self._freq = result._freq
-        return self
-
-    def __isub__(self, other):
-        result = self - other
-        self[:] = result[:]
-
-        if not is_period_dtype(self):
-            # restore freq, which is invalidated by setitem
-            self._freq = result._freq
-        return self
-
-    # --------------------------------------------------------------
-    # Reductions
-
-    def _reduce(self, name, axis=0, skipna=True, **kwargs):
-        op = getattr(self, name, None)
-        if op:
-            return op(skipna=skipna, **kwargs)
+        this, other = self._maybe_utc_convert(other)
+
+        if this._can_fast_union(other):
+            result = this._fast_union(other, sort=sort)
+            if result.freq is None:
+                result = result._with_freq(""infer"")
+            return result
         else:
-            return super()._reduce(name, skipna, **kwargs)
+            i8self = Int64Index._simple_new(self.asi8, name=self.name)
+            i8other = Int64Index._simple_new(other.asi8, name=other.name)
+            i8result = i8self._union(i8other, sort=sort)
+            result = type(self)(i8result, dtype=self.dtype, freq=""infer"")
+            return result
 
-    def min(self, axis=None, skipna=True, *args, **kwargs):
-        """"""
-        Return the minimum value of the Array or minimum along
-        an axis.
+    # --------------------------------------------------------------------
+    # Join Methods
+    _join_precedence = 10
 
-        See Also
-        --------
-        numpy.ndarray.min
-        Index.min : Return the minimum value in an Index.
-        Series.min : Return the minimum value in a Series.
+    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)
+    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)
+    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)
+    _left_indexer_unique = _join_i8_wrapper(
+        libjoin.left_join_indexer_unique, with_indexers=False
+    )
+
+    def join(
+        self, other, how: str = ""left"", level=None, return_indexers=False, sort=False
+    ):
         """"""
-        nv.validate_min(args, kwargs)
-        nv.validate_minmax_axis(axis)
+        See Index.join
+        """"""
+        if self._is_convertible_to_index_for_join(other):
+            try:
+                other = type(self)(other)
+            except (TypeError, ValueError):
+                pass
 
-        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())
-        if isna(result):
-            # Period._from_ordinal does not handle np.nan gracefully
-            return NaT
-        return self._box_func(result)
+        this, other = self._maybe_utc_convert(other)
+        return Index.join(
+            this,
+            other,
+            how=how,
+            level=level,
+            return_indexers=return_indexers,
+            sort=sort,
+        )
 
-    def max(self, axis=None, skipna=True, *args, **kwargs):
-        """"""
-        Return the maximum value of the Array or maximum along
-        an axis.
+    def _maybe_utc_convert(self, other):
+        this = self
+        if not hasattr(self, ""tz""):
+            return this, other
 
-        See Also
-        --------
-        numpy.ndarray.max
-        Index.max : Return the maximum value in an Index.
-        Series.max : Return the maximum value in a Series.
-        """"""
-        # TODO: skipna is broken with max.
-        # See https://github.com/pandas-dev/pandas/issues/24265
-        nv.validate_max(args, kwargs)
-        nv.validate_minmax_axis(axis)
+        if isinstance(other, type(self)):
+            if self.tz is not None:
+                if other.tz is None:
+                    raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
+            elif other.tz is not None:
+                raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
 
-        mask = self.isna()
-        if skipna:
-            values = self[~mask].asi8
-        elif mask.any():
-            return NaT
-        else:
-            values = self.asi8
+            if not timezones.tz_compare(self.tz, other.tz):
+                this = self.tz_convert(""UTC"")
+                other = other.tz_convert(""UTC"")
+        return this, other
 
-        if not len(values):
-            # short-circuit for empty max / min
-            return NaT
+    @classmethod
+    def _is_convertible_to_index_for_join(cls, other: Index) -> bool:
+        """"""
+        return a boolean whether I can attempt conversion to a
+        DatetimeIndex/TimedeltaIndex
+        """"""
+        if isinstance(other, cls):
+            return False
+        elif len(other) > 0 and other.inferred_type not in (
+            ""floating"",
+            ""mixed-integer"",
+            ""integer"",
+            ""integer-na"",
+            ""mixed-integer-float"",
+            ""mixed"",
+        ):
+            return True
+        return False
 
-        result = nanops.nanmax(values, skipna=skipna)
-        # Don't have to worry about NA `result`, since no NA went in.
-        return self._box_func(result)
+    # --------------------------------------------------------------------
+    # List-Like Methods
 
-    def mean(self, skipna=True):
+    def insert(self, loc, item):
         """"""
-        Return the mean value of the Array.
-
-        .. versionadded:: 0.25.0
+        Make new Index inserting new item at location
 
         Parameters
         ----------
-        skipna : bool, default True
-            Whether to ignore any NaT elements.
+        loc : int
+        item : object
+            if not either a Python datetime or a numpy integer-like, returned
+            Index dtype will be object rather than datetime.
 
         Returns
         -------
-        scalar
-            Timestamp or Timedelta.
-
-        See Also
-        --------
-        numpy.ndarray.mean : Returns the average of array elements along a given axis.
-        Series.mean : Return the mean value in a Series.
-
-        Notes
-        -----
-        mean is only defined for Datetime and Timedelta dtypes, not for Period.
-        """"""
-        if is_period_dtype(self):
-            # See discussion in GH#24757
-            raise TypeError(
-                f""mean is not implemented for {type(self).__name__} since the ""
-                ""meaning is ambiguous.  An alternative is ""
-                ""obj.to_timestamp(how='start').mean()""
-            )
-
-        mask = self.isna()
-        if skipna:
-            values = self[~mask]
-        elif mask.any():
-            return NaT
-        else:
-            values = self
-
-        if not len(values):
-            # short-circuit for empty max / min
-            return NaT
-
-        result = nanops.nanmean(values.view(""i8""), skipna=skipna)
-        # Don't have to worry about NA `result`, since no NA went in.
-        return self._box_func(result)
-
-
-DatetimeLikeArrayMixin._add_comparison_ops()
-
-# -------------------------------------------------------------------
-# Shared Constructor Helpers
-
-
-def validate_periods(periods):
-    """"""
-    If a `periods` argument is passed to the Datetime/Timedelta Array/Index
-    constructor, cast it to an integer.
-
-    Parameters
-    ----------
-    periods : None, float, int
-
-    Returns
-    -------
-    periods : None or int
-
-    Raises
-    ------
-    TypeError
-        if periods is None, float, or int
-    """"""
-    if periods is not None:
-        if lib.is_float(periods):
-            periods = int(periods)
-        elif not lib.is_integer(periods):
-            raise TypeError(f""periods must be a number, got {periods}"")
-    return periods
-
-
-def validate_endpoints(closed):
-    """"""
-    Check that the `closed` argument is among [None, ""left"", ""right""]
-
-    Parameters
-    ----------
-    closed : {None, ""left"", ""right""}
-
-    Returns
-    -------
-    left_closed : bool
-    right_closed : bool
-
-    Raises
-    ------
-    ValueError : if argument is not among valid values
-    """"""
-    left_closed = False
-    right_closed = False
-
-    if closed is None:
-        left_closed = True
-        right_closed = True
-    elif closed == ""left"":
-        left_closed = True
-    elif closed == ""right"":
-        right_closed = True
-    else:
-        raise ValueError(""Closed has to be either 'left', 'right' or None"")
-
-    return left_closed, right_closed
-
-
-def validate_inferred_freq(freq, inferred_freq, freq_infer):
-    """"""
-    If the user passes a freq and another freq is inferred from passed data,
-    require that they match.
-
-    Parameters
-    ----------
-    freq : DateOffset or None
-    inferred_freq : DateOffset or None
-    freq_infer : bool
-
-    Returns
-    -------
-    freq : DateOffset or None
-    freq_infer : bool
-
-    Notes
-    -----
-    We assume at this point that `maybe_infer_freq` has been called, so
-    `freq` is either a DateOffset object or None.
-    """"""
-    if inferred_freq is not None:
-        if freq is not None and freq != inferred_freq:
-            raise ValueError(
-                f""Inferred frequency {inferred_freq} from passed ""
-                ""values does not conform to passed frequency ""
-                f""{freq.freqstr}""
-            )
-        elif freq is None:
-            freq = inferred_freq
-        freq_infer = False
-
-    return freq, freq_infer
+        new_index : Index
+        """"""
+        if isinstance(item, str):
+            # TODO: Why are strings special?
+            # TODO: Should we attempt _scalar_from_string?
+            return self.astype(object).insert(loc, item)
+
+        item = self._data._validate_insert_value(item)
+
+        freq = None
+        # check freq can be preserved on edge cases
+        if self.freq is not None:
+            if self.size:
+                if item is NaT:
+                    pass
+                elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:
+                    freq = self.freq
+                elif (loc == len(self)) and item - self.freq == self[-1]:
+                    freq = self.freq
+            else:
+                # Adding a single item to an empty index may preserve freq
+                if self.freq.is_on_offset(item):
+                    freq = self.freq
 
+        item = self._data._unbox_scalar(item)
 
-def maybe_infer_freq(freq):
-    """"""
-    Comparing a DateOffset to the string ""infer"" raises, so we need to
-    be careful about comparisons.  Make a dummy variable `freq_infer` to
-    signify the case where the given freq is ""infer"" and set freq to None
-    to avoid comparison trouble later on.
-
-    Parameters
-    ----------
-    freq : {DateOffset, None, str}
-
-    Returns
-    -------
-    freq : {DateOffset, None}
-    freq_infer : bool
-        Whether we should inherit the freq of passed data.
-    """"""
-    freq_infer = False
-    if not isinstance(freq, DateOffset):
-        # if a passed freq is None, don't infer automatically
-        if freq != ""infer"":
-            freq = frequencies.to_offset(freq)
-        else:
-            freq_infer = True
-            freq = None
-    return freq, freq_infer
+        new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])
+        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
+        return type(self)._simple_new(arr, name=self.name)
diff --git a/pandas/tests/indexes/timedeltas/test_timedelta.py b/pandas/tests/indexes/timedeltas/test_timedelta.py
index 637a2629d..5efa1a757 100644
--- a/pandas/tests/indexes/timedeltas/test_timedelta.py
+++ b/pandas/tests/indexes/timedeltas/test_timedelta.py
@@ -47,6 +47,13 @@ class TestTimedeltaIndex(DatetimeLike):
     def test_pickle_compat_construction(self):
         pass
 
+    def test_pickle_after_set_freq(self):
+        tdi = timedelta_range(""1 day"", periods=4, freq=""s"")
+        tdi = tdi._with_freq(None)
+
+        res = tm.round_trip_pickle(tdi)
+        tm.assert_index_equal(res, tdi)
+
     def test_isin(self):
 
         index = tm.makeTimedeltaIndex(4)
"
"pandas","51","4800ab4","ea1d8fadb95fbc7cafe036274006228400817fd4","pandas/core/indexes/category.py","pandas/core/indexes/category.py","diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py","pandas/tests/reshape/merge/test_merge.py","","diff --git a/pandas/core/indexes/category.py b/pandas/core/indexes/category.py
index 5f0d6ea2d..4475cb414 100644
--- a/pandas/core/indexes/category.py
+++ b/pandas/core/indexes/category.py
@@ -29,6 +29,7 @@ import pandas.core.indexes.base as ibase
 from pandas.core.indexes.base import Index, _index_shared_docs, maybe_extract_name
 from pandas.core.indexes.extension import ExtensionIndex, inherit_names
 import pandas.core.missing as missing
+from pandas.core.ops import get_op_result_name
 
 _index_doc_kwargs = dict(ibase._index_doc_kwargs)
 _index_doc_kwargs.update(dict(target_klass=""CategoricalIndex""))
@@ -763,6 +764,12 @@ class CategoricalIndex(ExtensionIndex, accessor.PandasDelegate):
             return res
         return CategoricalIndex(res, name=self.name)
 
+    def _wrap_joined_index(
+        self, joined: np.ndarray, other: ""CategoricalIndex""
+    ) -> ""CategoricalIndex"":
+        name = get_op_result_name(self, other)
+        return self._create_from_codes(joined, name=name)
+
 
 CategoricalIndex._add_numeric_methods_add_sub_disabled()
 CategoricalIndex._add_numeric_methods_disabled()
"
"pandas","95","036dc88","c99dfea33612f44e97c2365f78c0ca6d5754a1bc","pandas/core/arrays/period.py","pandas/core/arrays/period.py","diff --git a/pandas/core/arrays/period.py b/pandas/core/arrays/period.py","pandas/tests/arithmetic/test_period.py","","diff --git a/pandas/core/arrays/period.py b/pandas/core/arrays/period.py
index a0e806fb8..e7e1c84b1 100644
--- a/pandas/core/arrays/period.py
+++ b/pandas/core/arrays/period.py
@@ -73,7 +73,6 @@ def _period_array_cmp(cls, op):
 
     @unpack_zerodim_and_defer(opname)
     def wrapper(self, other):
-        ordinal_op = getattr(self.asi8, opname)
 
         if isinstance(other, str):
             try:
@@ -81,11 +80,6 @@ def _period_array_cmp(cls, op):
             except ValueError:
                 # string that can't be parsed as Period
                 return invalid_comparison(self, other, op)
-        elif isinstance(other, int):
-            # TODO: sure we want to allow this?  we dont for DTA/TDA
-            #  2 tests rely on this
-            other = Period(other, freq=self.freq)
-            result = ordinal_op(other.ordinal)
 
         if isinstance(other, self._recognized_scalars) or other is NaT:
             other = self._scalar_type(other)
"
"pandas","55","6ab00bc","628dfba239865adc09c94108b288bcb60c619950","pandas/core/generic.py;pandas/core/indexing.py","pandas/core/generic.py;pandas/core/indexing.py","diff --git a/pandas/core/generic.py b/pandas/core/generic.py;diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py","pandas/tests/indexing/test_iloc.py","","diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 579daae2b..ac03843a0 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -3467,13 +3467,13 @@ class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):
             res._is_copy = self._is_copy
         return res
 
-    def _iget_item_cache(self, item):
+    def _iget_item_cache(self, item: int):
         """"""Return the cached item, item represents a positional indexer.""""""
         ax = self._info_axis
         if ax.is_unique:
             lower = self._get_item_cache(ax[item])
         else:
-            lower = self._take_with_is_copy(item, axis=self._info_axis_number)
+            return self._ixs(item, axis=1)
         return lower
 
     def _box_item_values(self, key, values):
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index 745456109..c53e690b5 100755
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -1414,10 +1414,6 @@ class _iLocIndexer(_LocationIndexer):
             if not is_integer(k):
                 return False
 
-            ax = self.obj.axes[i]
-            if not ax.is_unique:
-                return False
-
         return True
 
     def _validate_integer(self, key: int, axis: int) -> None:
"
"pandas","136","3954fa7","6241b9d3b3b8fd688cf32e45539719f1b9ec25c1","pandas/core/reshape/merge.py","pandas/core/reshape/merge.py","diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py","pandas/tests/reshape/merge/test_merge_asof.py","","diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py
index 910c7ea56..7bfc8153d 100644
--- a/pandas/core/reshape/merge.py
+++ b/pandas/core/reshape/merge.py
@@ -28,7 +28,6 @@ from pandas.core.dtypes.common import (
     is_dtype_equal,
     is_extension_array_dtype,
     is_float_dtype,
-    is_int64_dtype,
     is_integer,
     is_integer_dtype,
     is_list_like,
@@ -1641,7 +1640,7 @@ class _AsOfMerge(_OrderedMerge):
                 if self.tolerance < Timedelta(0):
                     raise MergeError(""tolerance must be positive"")
 
-            elif is_int64_dtype(lt):
+            elif is_integer_dtype(lt):
                 if not is_integer(self.tolerance):
                     raise MergeError(msg)
                 if self.tolerance < 0:
"
"pandas","32","467e1c2","c82cb179affed1c1136431ce39e4c66f4f3a65c0","pandas/io/sas/sas_xport.py","pandas/io/sas/sas_xport.py","diff --git a/pandas/io/sas/sas_xport.py b/pandas/io/sas/sas_xport.py","pandas/tests/io/sas/test_xport.py","","diff --git a/pandas/io/sas/sas_xport.py b/pandas/io/sas/sas_xport.py
index e67d68f7e..85b7fd497 100644
--- a/pandas/io/sas/sas_xport.py
+++ b/pandas/io/sas/sas_xport.py
@@ -9,7 +9,6 @@ https://support.sas.com/techsup/technote/ts140.pdf
 """"""
 from collections import abc
 from datetime import datetime
-from io import BytesIO
 import struct
 import warnings
 
@@ -263,13 +262,9 @@ class XportReader(abc.Iterator):
         if isinstance(filepath_or_buffer, (str, bytes)):
             self.filepath_or_buffer = open(filepath_or_buffer, ""rb"")
         else:
-            # Copy to BytesIO, and ensure no encoding
-            contents = filepath_or_buffer.read()
-            try:
-                contents = contents.encode(self._encoding)
-            except UnicodeEncodeError:
-                pass
-            self.filepath_or_buffer = BytesIO(contents)
+            # Since xport files include non-text byte sequences, xport files
+            # should already be opened in binary mode in Python 3.
+            self.filepath_or_buffer = filepath_or_buffer
 
         self._read_header()
 
"
"pandas","130","7adc14a","8efc717e4652e1e4bfbc4455da1d40eb676eed91","pandas/core/groupby/ops.py","pandas/core/groupby/ops.py","diff --git a/pandas/core/groupby/ops.py b/pandas/core/groupby/ops.py","pandas/tests/groupby/test_value_counts.py","","diff --git a/pandas/core/groupby/ops.py b/pandas/core/groupby/ops.py
index 2c8aa1294..9599ce0bf 100644
--- a/pandas/core/groupby/ops.py
+++ b/pandas/core/groupby/ops.py
@@ -767,6 +767,11 @@ class BinGrouper(BaseGrouper):
             ngroups,
         )
 
+    @cache_readonly
+    def recons_codes(self):
+        # get unique result indices, and prepend 0 as groupby starts from the first
+        return [np.r_[0, np.flatnonzero(self.bins[1:] != self.bins[:-1]) + 1]]
+
     @cache_readonly
     def result_index(self):
         if len(self.binlabels) != 0 and isna(self.binlabels[0]):
"
"pandas","77","667bb37","daef69c1366e31c3c49aea6f2e55f577d0c832fd","pandas/core/ops/array_ops.py","pandas/core/ops/array_ops.py","diff --git a/pandas/core/ops/array_ops.py b/pandas/core/ops/array_ops.py","pandas/tests/arithmetic/test_array_ops.py","","diff --git a/pandas/core/ops/array_ops.py b/pandas/core/ops/array_ops.py
index b84d468ff..cb7b8a598 100644
--- a/pandas/core/ops/array_ops.py
+++ b/pandas/core/ops/array_ops.py
@@ -277,7 +277,7 @@ def na_logical_op(x: np.ndarray, y, op):
             assert not (is_bool_dtype(x.dtype) and is_bool_dtype(y.dtype))
             x = ensure_object(x)
             y = ensure_object(y)
-            result = libops.vec_binop(x, y, op)
+            result = libops.vec_binop(x.ravel(), y.ravel(), op)
         else:
             # let null fall thru
             assert lib.is_scalar(y)
@@ -298,7 +298,7 @@ def na_logical_op(x: np.ndarray, y, op):
                     f""and scalar of type [{typ}]""
                 )
 
-    return result
+    return result.reshape(x.shape)
 
 
 def logical_op(
"
"pandas","102","efaadd5","765d8db7eef1befef33f4c99d3e206d26e8444c8","pandas/core/internals/construction.py","pandas/core/internals/construction.py","diff --git a/pandas/core/internals/construction.py b/pandas/core/internals/construction.py","pandas/tests/frame/test_constructors.py","","diff --git a/pandas/core/internals/construction.py b/pandas/core/internals/construction.py
index 897dbe2e8..3a92cfd9b 100644
--- a/pandas/core/internals/construction.py
+++ b/pandas/core/internals/construction.py
@@ -152,9 +152,17 @@ def init_ndarray(values, index, columns, dtype=None, copy=False):
         return arrays_to_mgr([values], columns, index, columns, dtype=dtype)
     elif is_extension_array_dtype(values) or is_extension_array_dtype(dtype):
         # GH#19157
+
+        if isinstance(values, np.ndarray) and values.ndim > 1:
+            # GH#12513 a EA dtype passed with a 2D array, split into
+            #  multiple EAs that view the values
+            values = [values[:, n] for n in range(values.shape[1])]
+        else:
+            values = [values]
+
         if columns is None:
-            columns = [0]
-        return arrays_to_mgr([values], columns, index, columns, dtype=dtype)
+            columns = list(range(len(values)))
+        return arrays_to_mgr(values, columns, index, columns, dtype=dtype)
 
     # by definition an array here
     # the dtypes will be coerced to a single dtype
"
"pandas","11","1c88e6a","b7f061c3d24df943e16918ad3932e767f5639a38","pandas/core/reshape/concat.py","pandas/core/reshape/concat.py","diff --git a/pandas/core/reshape/concat.py b/pandas/core/reshape/concat.py","pandas/tests/reshape/test_concat.py","","diff --git a/pandas/core/reshape/concat.py b/pandas/core/reshape/concat.py
index a868e663b..2f66cbf44 100644
--- a/pandas/core/reshape/concat.py
+++ b/pandas/core/reshape/concat.py
@@ -619,10 +619,10 @@ def _make_concat_multiindex(indexes, keys, levels=None, names=None) -> MultiInde
         for hlevel, level in zip(zipped, levels):
             to_concat = []
             for key, index in zip(hlevel, indexes):
-                try:
-                    i = level.get_loc(key)
-                except KeyError as err:
-                    raise ValueError(f""Key {key} not in level {level}"") from err
+                mask = level == key
+                if not mask.any():
+                    raise ValueError(f""Key {key} not in level {level}"")
+                i = np.nonzero(level == key)[0][0]
 
                 to_concat.append(np.repeat(i, len(index)))
             codes_list.append(np.concatenate(to_concat))
"
"pandas","6","21a10d1","8cd8ed3657e52ad9f67e17b7f5c20f7340ab6a2c","pandas/core/groupby/grouper.py","pandas/core/groupby/grouper.py","diff --git a/pandas/core/groupby/grouper.py b/pandas/core/groupby/grouper.py","pandas/tests/groupby/test_size.py","","diff --git a/pandas/core/groupby/grouper.py b/pandas/core/groupby/grouper.py
index 9660fb9c2..39892d87b 100644
--- a/pandas/core/groupby/grouper.py
+++ b/pandas/core/groupby/grouper.py
@@ -754,7 +754,9 @@ def get_grouper(
             return False
         try:
             return gpr is obj[gpr.name]
-        except (KeyError, IndexError):
+        except (KeyError, IndexError, ValueError):
+            # TODO: ValueError: Given date string not likely a datetime.
+            # should be KeyError?
             return False
 
     for i, (gpr, level) in enumerate(zip(keys, levels)):
"
"pandas","125","e639af2","fb08ceeeeba2ba62f92b47d424b3ae83c20ed9db","pandas/core/arrays/categorical.py;pandas/core/internals/blocks.py","pandas/core/arrays/categorical.py;pandas/core/internals/blocks.py","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py;diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py","pandas/tests/arrays/categorical/test_algos.py;pandas/tests/frame/test_replace.py","","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index 53689b6bc..c6e2a7b7a 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -2470,6 +2470,51 @@ class Categorical(ExtensionArray, PandasObject):
         code_values = code_values[null_mask | (code_values >= 0)]
         return algorithms.isin(self.codes, code_values)
 
+    def replace(self, to_replace, value, inplace: bool = False):
+        """"""
+        Replaces all instances of one value with another
+
+        Parameters
+        ----------
+        to_replace: object
+            The value to be replaced
+
+        value: object
+            The value to replace it with
+
+        inplace: bool
+            Whether the operation is done in-place
+
+        Returns
+        -------
+        None if inplace is True, otherwise the new Categorical after replacement
+
+
+        Examples
+        --------
+        >>> s = pd.Categorical([1, 2, 1, 3])
+        >>> s.replace(1, 3)
+        [3, 3, 2, 3]
+        Categories (2, int64): [2, 3]
+        """"""
+        inplace = validate_bool_kwarg(inplace, ""inplace"")
+        cat = self if inplace else self.copy()
+        if to_replace in cat.categories:
+            if isna(value):
+                cat.remove_categories(to_replace, inplace=True)
+            else:
+                categories = cat.categories.tolist()
+                index = categories.index(to_replace)
+                if value in cat.categories:
+                    value_index = categories.index(value)
+                    cat._codes[cat._codes == index] = value_index
+                    cat.remove_categories(to_replace, inplace=True)
+                else:
+                    categories[index] = value
+                    cat.rename_categories(categories, inplace=True)
+        if not inplace:
+            return cat
+
 
 # The Series.cat accessor
 
diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index 38e1f241c..7ace80415 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -2924,6 +2924,30 @@ class CategoricalBlock(ExtensionBlock):
             )
         return result
 
+    def replace(
+        self,
+        to_replace,
+        value,
+        inplace: bool = False,
+        filter=None,
+        regex: bool = False,
+        convert: bool = True,
+    ):
+        inplace = validate_bool_kwarg(inplace, ""inplace"")
+        result = self if inplace else self.copy()
+        if filter is None:  # replace was called on a series
+            result.values.replace(to_replace, value, inplace=True)
+            if convert:
+                return result.convert(numeric=False, copy=not inplace)
+            else:
+                return result
+        else:  # replace was called on a DataFrame
+            if not isna(value):
+                result.values.add_categories(value, inplace=True)
+            return super(CategoricalBlock, result).replace(
+                to_replace, value, inplace, filter, regex, convert
+            )
+
 
 # -----------------------------------------------------------------
 # Constructor Helpers
"
"pandas","65","2f70e41","2f9a44635bd8d468cf008f2855ce2dcfb9e90586","pandas/io/common.py;pandas/io/parsers.py","pandas/io/common.py;pandas/io/parsers.py","diff --git a/pandas/io/common.py b/pandas/io/common.py;diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py","pandas/tests/io/parser/test_encoding.py","","diff --git a/pandas/io/common.py b/pandas/io/common.py
index 00f2961e4..e506cc155 100644
--- a/pandas/io/common.py
+++ b/pandas/io/common.py
@@ -3,7 +3,7 @@
 import bz2
 from collections import abc
 import gzip
-from io import BufferedIOBase, BytesIO
+from io import BufferedIOBase, BytesIO, RawIOBase
 import mmap
 import os
 import pathlib
@@ -359,9 +359,9 @@ def get_handle(
     try:
         from s3fs import S3File
 
-        need_text_wrapping = (BufferedIOBase, S3File)
+        need_text_wrapping = (BufferedIOBase, RawIOBase, S3File)
     except ImportError:
-        need_text_wrapping = BufferedIOBase  # type: ignore
+        need_text_wrapping = (BufferedIOBase, RawIOBase)  # type: ignore
 
     handles: List[IO] = list()
     f = path_or_buf
@@ -437,7 +437,7 @@ def get_handle(
         from io import TextIOWrapper
 
         g = TextIOWrapper(f, encoding=encoding, newline="""")
-        if not isinstance(f, BufferedIOBase):
+        if not isinstance(f, (BufferedIOBase, RawIOBase)):
             handles.append(g)
         f = g
 
diff --git a/pandas/io/parsers.py b/pandas/io/parsers.py
index b38aa9770..8bc8470ae 100755
--- a/pandas/io/parsers.py
+++ b/pandas/io/parsers.py
@@ -5,7 +5,7 @@ Module contains tools for processing files into DataFrames or other objects
 from collections import abc, defaultdict
 import csv
 import datetime
-from io import BufferedIOBase, StringIO, TextIOWrapper
+from io import BufferedIOBase, RawIOBase, StringIO, TextIOWrapper
 import re
 import sys
 from textwrap import fill
@@ -1872,7 +1872,7 @@ class CParserWrapper(ParserBase):
 
             # Handle the file object with universal line mode enabled.
             # We will handle the newline character ourselves later on.
-            if isinstance(src, BufferedIOBase):
+            if isinstance(src, (BufferedIOBase, RawIOBase)):
                 src = TextIOWrapper(src, encoding=encoding, newline="""")
 
             kwds[""encoding""] = ""utf-8""
"
"pandas","122","07e6b9d","30059081e946a2020d08d49bf4fa7b771d10089a","pandas/core/internals/managers.py","pandas/core/internals/managers.py","diff --git a/pandas/core/internals/managers.py b/pandas/core/internals/managers.py","pandas/tests/internals/test_internals.py","","diff --git a/pandas/core/internals/managers.py b/pandas/core/internals/managers.py
index 8a9410c07..0e6ba8a2c 100644
--- a/pandas/core/internals/managers.py
+++ b/pandas/core/internals/managers.py
@@ -1394,12 +1394,12 @@ class BlockManager(PandasObject):
         if len(self.blocks) != len(other.blocks):
             return False
 
-        # canonicalize block order, using a tuple combining the type
-        # name and then mgr_locs because there might be unconsolidated
+        # canonicalize block order, using a tuple combining the mgr_locs
+        # then type name because there might be unconsolidated
         # blocks (say, Categorical) which can only be distinguished by
         # the iteration order
         def canonicalize(block):
-            return (block.dtype.name, block.mgr_locs.as_array.tolist())
+            return (block.mgr_locs.as_array.tolist(), block.dtype.name)
 
         self_blocks = sorted(self.blocks, key=canonicalize)
         other_blocks = sorted(other.blocks, key=canonicalize)
"
"pandas","35","a7dd3ea","f10ec595eccaf86a9f52fe9683e1181a51ba675b","pandas/core/indexes/period.py","pandas/core/indexes/period.py","diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py","pandas/tests/indexes/multi/test_get_level_values.py","","diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py
index 0646acab5..8aaf82878 100644
--- a/pandas/core/indexes/period.py
+++ b/pandas/core/indexes/period.py
@@ -324,8 +324,8 @@ class PeriodIndex(DatetimeIndexOpsMixin, Int64Index):
 
     @cache_readonly
     def _engine(self):
-        # To avoid a reference cycle, pass a weakref of self to _engine_type.
-        period = weakref.ref(self)
+        # To avoid a reference cycle, pass a weakref of self._values to _engine_type.
+        period = weakref.ref(self._values)
         return self._engine_type(period, len(self))
 
     @doc(Index.__contains__)
"
"pandas","101","765d8db","27b713ba677869893552cbeff6bc98a5dd231950","pandas/core/dtypes/cast.py","pandas/core/dtypes/cast.py","diff --git a/pandas/core/dtypes/cast.py b/pandas/core/dtypes/cast.py","pandas/tests/dtypes/test_common.py","","diff --git a/pandas/core/dtypes/cast.py b/pandas/core/dtypes/cast.py
index fa7b45ec4..0579c9774 100644
--- a/pandas/core/dtypes/cast.py
+++ b/pandas/core/dtypes/cast.py
@@ -823,6 +823,8 @@ def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):
         if is_object_dtype(dtype):
             return tslib.ints_to_pydatetime(arr.view(np.int64))
         elif dtype == np.int64:
+            if isna(arr).any():
+                raise ValueError(""Cannot convert NaT values to integer"")
             return arr.view(dtype)
 
         # allow frequency conversions
@@ -835,6 +837,8 @@ def astype_nansafe(arr, dtype, copy: bool = True, skipna: bool = False):
         if is_object_dtype(dtype):
             return tslibs.ints_to_pytimedelta(arr.view(np.int64))
         elif dtype == np.int64:
+            if isna(arr).any():
+                raise ValueError(""Cannot convert NaT values to integer"")
             return arr.view(dtype)
 
         if dtype not in [_INT64_DTYPE, _TD_DTYPE]:
"
"pandas","33","03dacc1","89d8aba76a2bb930e520590d145e3d67b2046e39","pandas/core/arrays/integer.py","pandas/core/arrays/integer.py","diff --git a/pandas/core/arrays/integer.py b/pandas/core/arrays/integer.py","pandas/tests/arrays/integer/test_function.py","","diff --git a/pandas/core/arrays/integer.py b/pandas/core/arrays/integer.py
index 4f3c68aa0..f5189068d 100644
--- a/pandas/core/arrays/integer.py
+++ b/pandas/core/arrays/integer.py
@@ -499,7 +499,8 @@ class IntegerArray(BaseMaskedArray):
         ExtensionArray.argsort
         """"""
         data = self._data.copy()
-        data[self._mask] = data.min() - 1
+        if self._mask.any():
+            data[self._mask] = data.min() - 1
         return data
 
     @classmethod
"
"pandas","88","698920f","586bcb16023ae870b0ad7769f6d9077903705486","pandas/core/reshape/pivot.py","pandas/core/reshape/pivot.py","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py","pandas/tests/reshape/test_pivot.py","","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py
index b443ba142..7109f2376 100644
--- a/pandas/core/reshape/pivot.py
+++ b/pandas/core/reshape/pivot.py
@@ -117,7 +117,9 @@ def pivot_table(
                 agged[v] = maybe_downcast_to_dtype(agged[v], data[v].dtype)
 
     table = agged
-    if table.index.nlevels > 1:
+
+    # GH17038, this check should only happen if index is defined (not None)
+    if table.index.nlevels > 1 and index:
         # Related GH #17123
         # If index_names are integers, determine whether the integers refer
         # to the level position or name.
"
"pandas","153","ae22b80","0c0a0cfbadcf01864d499599712edc9022eea12e","pandas/core/internals/blocks.py","pandas/core/internals/blocks.py","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py","pandas/tests/io/formats/test_to_csv.py","","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index 2a44177d4..29d225443 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -7,7 +7,7 @@ import warnings
 
 import numpy as np
 
-from pandas._libs import NaT, Timestamp, lib, tslib
+from pandas._libs import NaT, Timestamp, lib, tslib, writers
 import pandas._libs.internals as libinternals
 from pandas._libs.tslibs import Timedelta, conversion
 from pandas._libs.tslibs.timezones import tz_compare
@@ -706,7 +706,8 @@ class Block(PandasObject):
         mask = isna(values)
 
         if not self.is_object and not quoting:
-            values = values.astype(str)
+            itemsize = writers.word_len(na_rep)
+            values = values.astype(""<U{size}"".format(size=itemsize))
         else:
             values = np.array(values, dtype=""object"")
 
"
"pandas","119","3f69d62","e0bd4d5dd07cc481cb52de3cf3c7bf199cb2df07","pandas/core/reshape/pivot.py","pandas/core/reshape/pivot.py","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py","pandas/tests/reshape/test_pivot.py","","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py
index c7d3adece..27d6a28a3 100644
--- a/pandas/core/reshape/pivot.py
+++ b/pandas/core/reshape/pivot.py
@@ -261,9 +261,12 @@ def _add_margins(
 
     row_names = result.index.names
     try:
+        # check the result column and leave floats
         for dtype in set(result.dtypes):
             cols = result.select_dtypes([dtype]).columns
-            margin_dummy[cols] = margin_dummy[cols].astype(dtype)
+            margin_dummy[cols] = margin_dummy[cols].apply(
+                maybe_downcast_to_dtype, args=(dtype,)
+            )
         result = result.append(margin_dummy)
     except TypeError:
 
"
"pandas","17","1e5ff23","afb04645b5b3361814f7d00ef68ce8d68e19ddb8","pandas/core/arrays/datetimelike.py;pandas/tests/indexing/test_coercion.py;pandas/tests/indexing/test_partial.py","pandas/core/arrays/datetimelike.py;pandas/tests/indexing/test_coercion.py;pandas/tests/indexing/test_partial.py","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py;diff --git a/pandas/tests/indexing/test_coercion.py b/pandas/tests/indexing/test_coercion.py;diff --git a/pandas/tests/indexing/test_partial.py b/pandas/tests/indexing/test_partial.py","pandas/tests/indexes/datetimes/test_insert.py","","diff --git a/pandas/core/arrays/datetimelike.py b/pandas/core/arrays/datetimelike.py
index 1d5f48797..4ddac12ae 100644
--- a/pandas/core/arrays/datetimelike.py
+++ b/pandas/core/arrays/datetimelike.py
@@ -1,1097 +1,670 @@
+""""""
+Base and utility classes for tseries type pandas objects.
+""""""
 from datetime import datetime, timedelta
-import operator
-from typing import Any, Sequence, Type, Union, cast
-import warnings
+from typing import Any, List, Optional, Union, cast
 
 import numpy as np
 
-from pandas._libs import NaT, NaTType, Timestamp, algos, iNaT, lib
-from pandas._libs.tslibs.c_timestamp import integer_op_not_supported
-from pandas._libs.tslibs.period import DIFFERENT_FREQ, IncompatibleFrequency, Period
-from pandas._libs.tslibs.timedeltas import Timedelta, delta_to_nanoseconds
-from pandas._libs.tslibs.timestamps import RoundTo, round_nsint64
-from pandas._typing import DatetimeLikeScalar
-from pandas.compat import set_function_name
+from pandas._libs import NaT, iNaT, join as libjoin, lib
+from pandas._libs.tslibs import timezones
+from pandas._typing import Label
 from pandas.compat.numpy import function as nv
-from pandas.errors import AbstractMethodError, NullFrequencyError, PerformanceWarning
-from pandas.util._decorators import Appender, Substitution
-from pandas.util._validators import validate_fillna_kwargs
+from pandas.errors import AbstractMethodError
+from pandas.util._decorators import Appender, cache_readonly, doc
 
 from pandas.core.dtypes.common import (
-    is_categorical_dtype,
+    ensure_int64,
+    is_bool_dtype,
     is_datetime64_any_dtype,
-    is_datetime64_dtype,
-    is_datetime64tz_dtype,
-    is_datetime_or_timedelta_dtype,
     is_dtype_equal,
-    is_float_dtype,
-    is_integer_dtype,
+    is_integer,
     is_list_like,
     is_object_dtype,
     is_period_dtype,
-    is_string_dtype,
+    is_scalar,
     is_timedelta64_dtype,
-    is_unsigned_integer_dtype,
-    pandas_dtype,
 )
-from pandas.core.dtypes.generic import ABCSeries
-from pandas.core.dtypes.inference import is_array_like
-from pandas.core.dtypes.missing import is_valid_nat_for_dtype, isna
-
-from pandas.core import missing, nanops, ops
-from pandas.core.algorithms import checked_add_with_arr, unique1d, value_counts
-from pandas.core.array_algos.transforms import shift
-from pandas.core.arrays._mixins import _T, NDArrayBackedExtensionArray
-from pandas.core.arrays.base import ExtensionArray, ExtensionOpsMixin
-import pandas.core.common as com
-from pandas.core.construction import array, extract_array
-from pandas.core.indexers import check_array_indexer
-from pandas.core.ops.common import unpack_zerodim_and_defer
-from pandas.core.ops.invalid import invalid_comparison, make_invalid_op
-
-from pandas.tseries import frequencies
-from pandas.tseries.offsets import DateOffset, Tick
-
-
-def _datetimelike_array_cmp(cls, op):
-    """"""
-    Wrap comparison operations to convert Timestamp/Timedelta/Period-like to
-    boxed scalars/arrays.
-    """"""
-    opname = f""__{op.__name__}__""
-    nat_result = opname == ""__ne__""
-
-    class InvalidComparison(Exception):
-        pass
-
-    def _validate_comparison_value(self, other):
-        if isinstance(other, str):
-            try:
-                # GH#18435 strings get a pass from tzawareness compat
-                other = self._scalar_from_string(other)
-            except ValueError:
-                # failed to parse as Timestamp/Timedelta/Period
-                raise InvalidComparison(other)
-
-        if isinstance(other, self._recognized_scalars) or other is NaT:
-            other = self._scalar_type(other)
-            self._check_compatible_with(other)
-
-        elif not is_list_like(other):
-            raise InvalidComparison(other)
-
-        elif len(other) != len(self):
-            raise ValueError(""Lengths must match"")
+from pandas.core.dtypes.concat import concat_compat
+from pandas.core.dtypes.generic import ABCIndex, ABCIndexClass, ABCSeries
+from pandas.core.dtypes.missing import isna
+
+from pandas.core import algorithms
+from pandas.core.arrays import DatetimeArray, PeriodArray, TimedeltaArray
+from pandas.core.arrays.datetimelike import DatetimeLikeArrayMixin
+from pandas.core.base import IndexOpsMixin
+import pandas.core.indexes.base as ibase
+from pandas.core.indexes.base import Index, _index_shared_docs
+from pandas.core.indexes.extension import (
+    ExtensionIndex,
+    inherit_names,
+    make_wrapped_arith_op,
+)
+from pandas.core.indexes.numeric import Int64Index
+from pandas.core.ops import get_op_result_name
+from pandas.core.tools.timedeltas import to_timedelta
 
-        else:
-            if isinstance(other, list):
-                # TODO: could use pd.Index to do inference?
-                other = np.array(other)
+from pandas.tseries.frequencies import DateOffset, to_offset
+from pandas.tseries.offsets import Tick
 
-            if not isinstance(other, (np.ndarray, type(self))):
-                raise InvalidComparison(other)
+_index_doc_kwargs = dict(ibase._index_doc_kwargs)
 
-            elif is_object_dtype(other.dtype):
-                pass
 
-            elif not type(self)._is_recognized_dtype(other.dtype):
-                raise InvalidComparison(other)
+def _join_i8_wrapper(joinf, with_indexers: bool = True):
+    """"""
+    Create the join wrapper methods.
+    """"""
 
-            else:
-                # For PeriodDType this casting is unnecessary
-                # TODO: use Index to do inference?
-                other = type(self)._from_sequence(other)
-                self._check_compatible_with(other)
+    @staticmethod  # type: ignore
+    def wrapper(left, right):
+        if isinstance(left, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
+            left = left.view(""i8"")
+        if isinstance(right, (np.ndarray, ABCIndex, ABCSeries, DatetimeLikeArrayMixin)):
+            right = right.view(""i8"")
 
-        return other
+        results = joinf(left, right)
+        if with_indexers:
+            # dtype should be timedelta64[ns] for TimedeltaIndex
+            #  and datetime64[ns] for DatetimeIndex
+            dtype = left.dtype.base
 
-    @unpack_zerodim_and_defer(opname)
-    def wrapper(self, other):
+            join_index, left_indexer, right_indexer = results
+            join_index = join_index.view(dtype)
+            return join_index, left_indexer, right_indexer
+        return results
 
-        try:
-            other = _validate_comparison_value(self, other)
-        except InvalidComparison:
-            return invalid_comparison(self, other, op)
-
-        dtype = getattr(other, ""dtype"", None)
-        if is_object_dtype(dtype):
-            # We have to use comp_method_OBJECT_ARRAY instead of numpy
-            #  comparison otherwise it would fail to raise when
-            #  comparing tz-aware and tz-naive
-            with np.errstate(all=""ignore""):
-                result = ops.comp_method_OBJECT_ARRAY(op, self.astype(object), other)
-            return result
+    return wrapper
 
-        if isinstance(other, self._scalar_type) or other is NaT:
-            other_i8 = self._unbox_scalar(other)
-        else:
-            # Then type(other) == type(self)
-            other_i8 = other.asi8
 
-        result = op(self.asi8, other_i8)
+def _make_wrapped_arith_op_with_freq(opname: str):
+    """"""
+    Dispatch the operation to the underlying ExtensionArray, and infer
+    the appropriate frequency for the result.
+    """"""
+    meth = make_wrapped_arith_op(opname)
 
-        o_mask = isna(other)
-        if self._hasnans | np.any(o_mask):
-            result[self._isnan | o_mask] = nat_result
+    def wrapped(self, other):
+        result = meth(self, other)
+        if result is NotImplemented:
+            return NotImplemented
 
+        new_freq = self._get_addsub_freq(other)
+        result._freq = new_freq
         return result
 
-    return set_function_name(wrapper, opname, cls)
+    wrapped.__name__ = opname
+    return wrapped
 
 
-class AttributesMixin:
-    _data: np.ndarray
+@inherit_names(
+    [""inferred_freq"", ""_isnan"", ""_resolution"", ""resolution""],
+    DatetimeLikeArrayMixin,
+    cache=True,
+)
+@inherit_names(
+    [""mean"", ""asi8"", ""_box_func""], DatetimeLikeArrayMixin,
+)
+class DatetimeIndexOpsMixin(ExtensionIndex):
+    """"""
+    Common ops mixin to support a unified interface datetimelike Index.
+    """"""
 
-    @classmethod
-    def _simple_new(cls, values: np.ndarray, **kwargs):
-        raise AbstractMethodError(cls)
+    _data: Union[DatetimeArray, TimedeltaArray, PeriodArray]
+    freq: Optional[DateOffset]
+    freqstr: Optional[str]
+    _resolution: int
+    _bool_ops: List[str] = []
+    _field_ops: List[str] = []
 
-    @property
-    def _scalar_type(self) -> Type[DatetimeLikeScalar]:
-        """"""
-        The scalar associated with this datelike
+    hasnans = cache_readonly(DatetimeLikeArrayMixin._hasnans.fget)  # type: ignore
+    _hasnans = hasnans  # for index / array -agnostic code
 
-        * PeriodArray : Period
-        * DatetimeArray : Timestamp
-        * TimedeltaArray : Timedelta
-        """"""
-        raise AbstractMethodError(self)
-
-    def _scalar_from_string(
-        self, value: str
-    ) -> Union[Period, Timestamp, Timedelta, NaTType]:
-        """"""
-        Construct a scalar type from a string.
+    @property
+    def is_all_dates(self) -> bool:
+        return True
 
-        Parameters
-        ----------
-        value : str
+    # ------------------------------------------------------------------------
+    # Abstract data attributes
 
-        Returns
-        -------
-        Period, Timestamp, or Timedelta, or NaT
-            Whatever the type of ``self._scalar_type`` is.
+    @property
+    def values(self):
+        # Note: PeriodArray overrides this to return an ndarray of objects.
+        return self._data._data
 
-        Notes
-        -----
-        This should call ``self._check_compatible_with`` before
-        unboxing the result.
+    def __array_wrap__(self, result, context=None):
         """"""
-        raise AbstractMethodError(self)
-
-    def _unbox_scalar(self, value: Union[Period, Timestamp, Timedelta, NaTType]) -> int:
+        Gets called after a ufunc.
         """"""
-        Unbox the integer value of a scalar `value`.
+        result = lib.item_from_zerodim(result)
+        if is_bool_dtype(result) or lib.is_scalar(result):
+            return result
 
-        Parameters
-        ----------
-        value : Union[Period, Timestamp, Timedelta]
+        attrs = self._get_attributes_dict()
+        if not is_period_dtype(self) and attrs[""freq""]:
+            # no need to infer if freq is None
+            attrs[""freq""] = ""infer""
+        return Index(result, **attrs)
 
-        Returns
-        -------
-        int
+    # ------------------------------------------------------------------------
 
-        Examples
-        --------
-        >>> self._unbox_scalar(Timedelta(""10s""))  # doctest: +SKIP
-        10000000000
+    def equals(self, other) -> bool:
         """"""
-        raise AbstractMethodError(self)
-
-    def _check_compatible_with(
-        self, other: Union[Period, Timestamp, Timedelta, NaTType], setitem: bool = False
-    ) -> None:
+        Determines if two Index objects contain the same elements.
         """"""
-        Verify that `self` and `other` are compatible.
-
-        * DatetimeArray verifies that the timezones (if any) match
-        * PeriodArray verifies that the freq matches
-        * Timedelta has no verification
+        if self.is_(other):
+            return True
 
-        In each case, NaT is considered compatible.
+        if not isinstance(other, ABCIndexClass):
+            return False
+        elif not isinstance(other, type(self)):
+            try:
+                other = type(self)(other)
+            except (ValueError, TypeError, OverflowError):
+                # e.g.
+                #  ValueError -> cannot parse str entry, or OutOfBoundsDatetime
+                #  TypeError  -> trying to convert IntervalIndex to DatetimeIndex
+                #  OverflowError -> Index([very_large_timedeltas])
+                return False
+
+        if not is_dtype_equal(self.dtype, other.dtype):
+            # have different timezone
+            return False
+
+        return np.array_equal(self.asi8, other.asi8)
+
+    @Appender(Index.__contains__.__doc__)
+    def __contains__(self, key: Any) -> bool:
+        hash(key)
+        try:
+            res = self.get_loc(key)
+        except (KeyError, TypeError, ValueError):
+            return False
+        return bool(
+            is_scalar(res) or isinstance(res, slice) or (is_list_like(res) and len(res))
+        )
 
-        Parameters
-        ----------
-        other
-        setitem : bool, default False
-            For __setitem__ we may have stricter compatibility resrictions than
-            for comparisons.
-
-        Raises
-        ------
-        Exception
+    def sort_values(self, return_indexer=False, ascending=True):
         """"""
-        raise AbstractMethodError(self)
-
-
-class DatelikeOps:
-    """"""
-    Common ops for DatetimeIndex/PeriodIndex, but not TimedeltaIndex.
-    """"""
-
-    @Substitution(
-        URL=""https://docs.python.org/3/library/datetime.html""
-        ""#strftime-and-strptime-behavior""
-    )
-    def strftime(self, date_format):
+        Return sorted copy of Index.
         """"""
-        Convert to Index using specified date_format.
+        if return_indexer:
+            _as = self.argsort()
+            if not ascending:
+                _as = _as[::-1]
+            sorted_index = self.take(_as)
+            return sorted_index, _as
+        else:
+            # NB: using asi8 instead of _data matters in numpy 1.18
+            #  because the treatment of NaT has been changed to put NaT last
+            #  instead of first.
+            sorted_values = np.sort(self.asi8)
 
-        Return an Index of formatted strings specified by date_format, which
-        supports the same string format as the python standard library. Details
-        of the string format can be found in `python string format
-        doc <%(URL)s>`__.
+            freq = self.freq
+            if freq is not None and not is_period_dtype(self):
+                if freq.n > 0 and not ascending:
+                    freq = freq * -1
+                elif freq.n < 0 and ascending:
+                    freq = freq * -1
 
-        Parameters
-        ----------
-        date_format : str
-            Date format string (e.g. ""%%Y-%%m-%%d"").
+            if not ascending:
+                sorted_values = sorted_values[::-1]
 
-        Returns
-        -------
-        ndarray
-            NumPy ndarray of formatted strings.
+            arr = type(self._data)._simple_new(
+                sorted_values, dtype=self.dtype, freq=freq
+            )
+            return type(self)._simple_new(arr, name=self.name)
 
-        See Also
-        --------
-        to_datetime : Convert the given argument to datetime.
-        DatetimeIndex.normalize : Return DatetimeIndex with times to midnight.
-        DatetimeIndex.round : Round the DatetimeIndex to the specified freq.
-        DatetimeIndex.floor : Floor the DatetimeIndex to the specified freq.
-
-        Examples
-        --------
-        >>> rng = pd.date_range(pd.Timestamp(""2018-03-10 09:00""),
-        ...                     periods=3, freq='s')
-        >>> rng.strftime('%%B %%d, %%Y, %%r')
-        Index(['March 10, 2018, 09:00:00 AM', 'March 10, 2018, 09:00:01 AM',
-               'March 10, 2018, 09:00:02 AM'],
-              dtype='object')
-        """"""
-        result = self._format_native_types(date_format=date_format, na_rep=np.nan)
-        return result.astype(object)
+    @Appender(_index_shared_docs[""take""] % _index_doc_kwargs)
+    def take(self, indices, axis=0, allow_fill=True, fill_value=None, **kwargs):
+        nv.validate_take(tuple(), kwargs)
+        indices = ensure_int64(indices)
 
+        maybe_slice = lib.maybe_indices_to_slice(indices, len(self))
+        if isinstance(maybe_slice, slice):
+            return self[maybe_slice]
 
-class TimelikeOps:
-    """"""
-    Common ops for TimedeltaIndex/DatetimeIndex, but not PeriodIndex.
-    """"""
+        return ExtensionIndex.take(
+            self, indices, axis, allow_fill, fill_value, **kwargs
+        )
 
-    _round_doc = """"""
-        Perform {op} operation on the data to the specified `freq`.
+    @doc(IndexOpsMixin.searchsorted, klass=""Datetime-like Index"")
+    def searchsorted(self, value, side=""left"", sorter=None):
+        if isinstance(value, str):
+            raise TypeError(
+                ""searchsorted requires compatible dtype or scalar, ""
+                f""not {type(value).__name__}""
+            )
+        if isinstance(value, Index):
+            value = value._data
 
-        Parameters
-        ----------
-        freq : str or Offset
-            The frequency level to {op} the index to. Must be a fixed
-            frequency like 'S' (second) not 'ME' (month end). See
-            :ref:`frequency aliases <timeseries.offset_aliases>` for
-            a list of possible `freq` values.
-        ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise'
-            Only relevant for DatetimeIndex:
-
-            - 'infer' will attempt to infer fall dst-transition hours based on
-              order
-            - bool-ndarray where True signifies a DST time, False designates
-              a non-DST time (note that this flag is only applicable for
-              ambiguous times)
-            - 'NaT' will return NaT where there are ambiguous times
-            - 'raise' will raise an AmbiguousTimeError if there are ambiguous
-              times.
+        return self._data.searchsorted(value, side=side, sorter=sorter)
 
-            .. versionadded:: 0.24.0
+    _can_hold_na = True
 
-        nonexistent : 'shift_forward', 'shift_backward', 'NaT', timedelta, \
-default 'raise'
-            A nonexistent time does not exist in a particular timezone
-            where clocks moved forward due to DST.
+    _na_value = NaT
+    """"""The expected NA value to use with this index.""""""
 
-            - 'shift_forward' will shift the nonexistent time forward to the
-              closest existing time
-            - 'shift_backward' will shift the nonexistent time backward to the
-              closest existing time
-            - 'NaT' will return NaT where there are nonexistent times
-            - timedelta objects will shift nonexistent times by the timedelta
-            - 'raise' will raise an NonExistentTimeError if there are
-              nonexistent times.
+    def _convert_tolerance(self, tolerance, target):
+        tolerance = np.asarray(to_timedelta(tolerance).to_numpy())
 
-            .. versionadded:: 0.24.0
+        if target.size != tolerance.size and tolerance.size > 1:
+            raise ValueError(""list-like tolerance size must match target index size"")
+        return tolerance
 
-        Returns
-        -------
-        DatetimeIndex, TimedeltaIndex, or Series
-            Index of the same type for a DatetimeIndex or TimedeltaIndex,
-            or a Series with the same index for a Series.
+    def tolist(self) -> List:
+        """"""
+        Return a list of the underlying data.
+        """"""
+        return list(self.astype(object))
 
-        Raises
-        ------
-        ValueError if the `freq` cannot be converted.
+    def min(self, axis=None, skipna=True, *args, **kwargs):
+        """"""
+        Return the minimum value of the Index or minimum along
+        an axis.
 
-        Examples
+        See Also
         --------
-        **DatetimeIndex**
-
-        >>> rng = pd.date_range('1/1/2018 11:59:00', periods=3, freq='min')
-        >>> rng
-        DatetimeIndex(['2018-01-01 11:59:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:01:00'],
-                      dtype='datetime64[ns]', freq='T')
+        numpy.ndarray.min
+        Series.min : Return the minimum value in a Series.
         """"""
+        nv.validate_min(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    _round_example = """""">>> rng.round('H')
-        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:00:00'],
-                      dtype='datetime64[ns]', freq=None)
+        if not len(self):
+            return self._na_value
 
-        **Series**
+        i8 = self.asi8
+        try:
+            # quick check
+            if len(i8) and self.is_monotonic:
+                if i8[0] != iNaT:
+                    return self._box_func(i8[0])
+
+            if self.hasnans:
+                if skipna:
+                    min_stamp = self[~self._isnan].asi8.min()
+                else:
+                    return self._na_value
+            else:
+                min_stamp = i8.min()
+            return self._box_func(min_stamp)
+        except ValueError:
+            return self._na_value
 
-        >>> pd.Series(rng).dt.round(""H"")
-        0   2018-01-01 12:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 12:00:00
-        dtype: datetime64[ns]
+    def argmin(self, axis=None, skipna=True, *args, **kwargs):
         """"""
+        Returns the indices of the minimum values along an axis.
 
-    _floor_example = """""">>> rng.floor('H')
-        DatetimeIndex(['2018-01-01 11:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 12:00:00'],
-                      dtype='datetime64[ns]', freq=None)
+        See `numpy.ndarray.argmin` for more information on the
+        `axis` parameter.
 
-        **Series**
-
-        >>> pd.Series(rng).dt.floor(""H"")
-        0   2018-01-01 11:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 12:00:00
-        dtype: datetime64[ns]
+        See Also
+        --------
+        numpy.ndarray.argmin
         """"""
+        nv.validate_argmin(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    _ceil_example = """""">>> rng.ceil('H')
-        DatetimeIndex(['2018-01-01 12:00:00', '2018-01-01 12:00:00',
-                       '2018-01-01 13:00:00'],
-                      dtype='datetime64[ns]', freq=None)
-
-        **Series**
+        i8 = self.asi8
+        if self.hasnans:
+            mask = self._isnan
+            if mask.all() or not skipna:
+                return -1
+            i8 = i8.copy()
+            i8[mask] = np.iinfo(""int64"").max
+        return i8.argmin()
 
-        >>> pd.Series(rng).dt.ceil(""H"")
-        0   2018-01-01 12:00:00
-        1   2018-01-01 12:00:00
-        2   2018-01-01 13:00:00
-        dtype: datetime64[ns]
+    def max(self, axis=None, skipna=True, *args, **kwargs):
         """"""
+        Return the maximum value of the Index or maximum along
+        an axis.
 
-    def _round(self, freq, mode, ambiguous, nonexistent):
-        # round the local times
-        if is_datetime64tz_dtype(self):
-            # operate on naive timestamps, then convert back to aware
-            naive = self.tz_localize(None)
-            result = naive._round(freq, mode, ambiguous, nonexistent)
-            aware = result.tz_localize(
-                self.tz, ambiguous=ambiguous, nonexistent=nonexistent
-            )
-            return aware
-
-        values = self.view(""i8"")
-        result = round_nsint64(values, mode, freq)
-        result = self._maybe_mask_results(result, fill_value=NaT)
-        return self._simple_new(result, dtype=self.dtype)
-
-    @Appender((_round_doc + _round_example).format(op=""round""))
-    def round(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.NEAREST_HALF_EVEN, ambiguous, nonexistent)
+        See Also
+        --------
+        numpy.ndarray.max
+        Series.max : Return the maximum value in a Series.
+        """"""
+        nv.validate_max(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    @Appender((_round_doc + _floor_example).format(op=""floor""))
-    def floor(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.MINUS_INFTY, ambiguous, nonexistent)
+        if not len(self):
+            return self._na_value
 
-    @Appender((_round_doc + _ceil_example).format(op=""ceil""))
-    def ceil(self, freq, ambiguous=""raise"", nonexistent=""raise""):
-        return self._round(freq, RoundTo.PLUS_INFTY, ambiguous, nonexistent)
+        i8 = self.asi8
+        try:
+            # quick check
+            if len(i8) and self.is_monotonic:
+                if i8[-1] != iNaT:
+                    return self._box_func(i8[-1])
+
+            if self.hasnans:
+                if skipna:
+                    max_stamp = self[~self._isnan].asi8.max()
+                else:
+                    return self._na_value
+            else:
+                max_stamp = i8.max()
+            return self._box_func(max_stamp)
+        except ValueError:
+            return self._na_value
 
-    def _with_freq(self, freq):
+    def argmax(self, axis=None, skipna=True, *args, **kwargs):
         """"""
-        Helper to set our freq in-place, returning self to allow method chaining.
+        Returns the indices of the maximum values along an axis.
 
-        Parameters
-        ----------
-        freq : DateOffset, None, or ""infer""
+        See `numpy.ndarray.argmax` for more information on the
+        `axis` parameter.
 
-        Returns
-        -------
-        self
+        See Also
+        --------
+        numpy.ndarray.argmax
         """"""
-        # GH#29843
-        if freq is None:
-            # Always valid
-            pass
-        elif len(self) == 0 and isinstance(freq, DateOffset):
-            # Always valid.  In the TimedeltaArray case, we assume this
-            #  is a Tick offset.
-            pass
-        else:
-            # As an internal method, we can ensure this assertion always holds
-            assert freq == ""infer""
-            freq = frequencies.to_offset(self.inferred_freq)
-
-        self._freq = freq
-        return self
-
-
-class DatetimeLikeArrayMixin(
-    ExtensionOpsMixin, AttributesMixin, NDArrayBackedExtensionArray
-):
-    """"""
-    Shared Base/Mixin class for DatetimeArray, TimedeltaArray, PeriodArray
-
-    Assumes that __new__/__init__ defines:
-        _data
-        _freq
-
-    and that the inheriting class has methods:
-        _generate_range
-    """"""
-
-    # ------------------------------------------------------------------
-    # NDArrayBackedExtensionArray compat
-
-    @property
-    def _ndarray(self) -> np.ndarray:
-        # NB: A bunch of Interval tests fail if we use ._data
-        return self.asi8
-
-    def _from_backing_data(self: _T, arr: np.ndarray) -> _T:
-        # Note: we do not retain `freq`
-        return type(self)(arr, dtype=self.dtype)  # type: ignore
-
-    # ------------------------------------------------------------------
-
-    @property
-    def ndim(self) -> int:
-        return self._data.ndim
+        nv.validate_argmax(args, kwargs)
+        nv.validate_minmax_axis(axis)
 
-    @property
-    def shape(self):
-        return self._data.shape
+        i8 = self.asi8
+        if self.hasnans:
+            mask = self._isnan
+            if mask.all() or not skipna:
+                return -1
+            i8 = i8.copy()
+            i8[mask] = 0
+        return i8.argmax()
 
-    def reshape(self, *args, **kwargs):
-        # Note: we drop any freq
-        data = self._data.reshape(*args, **kwargs)
-        return type(self)(data, dtype=self.dtype)
+    # --------------------------------------------------------------------
+    # Rendering Methods
 
-    def ravel(self, *args, **kwargs):
-        # Note: we drop any freq
-        data = self._data.ravel(*args, **kwargs)
-        return type(self)(data, dtype=self.dtype)
+    def _format_with_header(self, header, na_rep=""NaT"", **kwargs):
+        return header + list(self._format_native_types(na_rep, **kwargs))
 
     @property
-    def _box_func(self):
-        """"""
-        box function to get object from internal representation
-        """"""
+    def _formatter_func(self):
         raise AbstractMethodError(self)
 
-    def _box_values(self, values):
+    def _format_attrs(self):
         """"""
-        apply box func to passed values
+        Return a list of tuples of the (attr,formatted_value).
         """"""
-        return lib.map_infer(values, self._box_func)
+        attrs = super()._format_attrs()
+        for attrib in self._attributes:
+            if attrib == ""freq"":
+                freq = self.freqstr
+                if freq is not None:
+                    freq = repr(freq)
+                attrs.append((""freq"", freq))
+        return attrs
 
-    def __iter__(self):
-        return (self._box_func(v) for v in self.asi8)
+    # --------------------------------------------------------------------
+    # Indexing Methods
 
-    @property
-    def asi8(self) -> np.ndarray:
-        """"""
-        Integer representation of the values.
+    def _validate_partial_date_slice(self, reso: str):
+        raise NotImplementedError
 
-        Returns
-        -------
-        ndarray
-            An ndarray with int64 dtype.
-        """"""
-        # do not cache or you'll create a memory leak
-        return self._data.view(""i8"")
+    def _parsed_string_to_bounds(self, reso: str, parsed: datetime):
+        raise NotImplementedError
 
-    # ----------------------------------------------------------------
-    # Rendering Methods
-
-    def _format_native_types(self, na_rep=""NaT"", date_format=None):
+    def _partial_date_slice(
+        self, reso: str, parsed: datetime, use_lhs: bool = True, use_rhs: bool = True
+    ):
         """"""
-        Helper method for astype when converting to strings.
+        Parameters
+        ----------
+        reso : str
+        parsed : datetime
+        use_lhs : bool, default True
+        use_rhs : bool, default True
 
         Returns
         -------
-        ndarray[str]
+        slice or ndarray[intp]
         """"""
-        raise AbstractMethodError(self)
-
-    def _formatter(self, boxed=False):
-        # TODO: Remove Datetime & DatetimeTZ formatters.
-        return ""'{}'"".format
+        self._validate_partial_date_slice(reso)
 
-    # ----------------------------------------------------------------
-    # Array-Like / EA-Interface Methods
+        t1, t2 = self._parsed_string_to_bounds(reso, parsed)
+        i8vals = self.asi8
+        unbox = self._data._unbox_scalar
 
-    @property
-    def nbytes(self):
-        return self._data.nbytes
+        if self.is_monotonic:
 
-    def __array__(self, dtype=None) -> np.ndarray:
-        # used for Timedelta/DatetimeArray, overwritten by PeriodArray
-        if is_object_dtype(dtype):
-            return np.array(list(self), dtype=object)
-        return self._data
+            if len(self) and (
+                (use_lhs and t1 < self[0] and t2 < self[0])
+                or ((use_rhs and t1 > self[-1] and t2 > self[-1]))
+            ):
+                # we are out of range
+                raise KeyError
 
-    @property
-    def size(self) -> int:
-        """"""The number of elements in this array.""""""
-        return np.prod(self.shape)
+            # TODO: does this depend on being monotonic _increasing_?
 
-    def __len__(self) -> int:
-        return len(self._data)
-
-    def __getitem__(self, key):
-        """"""
-        This getitem defers to the underlying array, which by-definition can
-        only handle list-likes, slices, and integer scalars
-        """"""
+            # a monotonic (sorted) series can be sliced
+            # Use asi8.searchsorted to avoid re-validating Periods/Timestamps
+            left = i8vals.searchsorted(unbox(t1), side=""left"") if use_lhs else None
+            right = i8vals.searchsorted(unbox(t2), side=""right"") if use_rhs else None
+            return slice(left, right)
 
-        if com.is_bool_indexer(key):
-            # first convert to boolean, because check_array_indexer doesn't
-            # allow object dtype
-            if is_object_dtype(key):
-                key = np.asarray(key, dtype=bool)
-
-            key = check_array_indexer(self, key)
-            key = lib.maybe_booleans_to_slice(key.view(np.uint8))
-        elif isinstance(key, list) and len(key) == 1 and isinstance(key[0], slice):
-            # see https://github.com/pandas-dev/pandas/issues/31299, need to allow
-            # this for now (would otherwise raise in check_array_indexer)
-            pass
         else:
-            key = check_array_indexer(self, key)
+            lhs_mask = (i8vals >= unbox(t1)) if use_lhs else True
+            rhs_mask = (i8vals <= unbox(t2)) if use_rhs else True
 
-        freq = self._get_getitem_freq(key)
-        result = self._data[key]
-        if lib.is_scalar(result):
-            return self._box_func(result)
-        return self._simple_new(result, dtype=self.dtype, freq=freq)
+            # try to find the dates
+            return (lhs_mask & rhs_mask).nonzero()[0]
 
-    def _get_getitem_freq(self, key):
+    # --------------------------------------------------------------------
+    # Arithmetic Methods
+
+    def _get_addsub_freq(self, other) -> Optional[DateOffset]:
         """"""
-        Find the `freq` attribute to assign to the result of a __getitem__ lookup.
+        Find the freq we expect the result of an addition/subtraction operation
+        to have.
         """"""
-        is_period = is_period_dtype(self.dtype)
-        if is_period:
-            freq = self.freq
-        else:
-            freq = None
-            if isinstance(key, slice):
-                if self.freq is not None and key.step is not None:
-                    freq = key.step * self.freq
-                else:
-                    freq = self.freq
-            elif key is Ellipsis:
-                # GH#21282 indexing with Ellipsis is similar to a full slice,
-                #  should preserve `freq` attribute
-                freq = self.freq
-        return freq
-
-    def __setitem__(
-        self,
-        key: Union[int, Sequence[int], Sequence[bool], slice],
-        value: Union[NaTType, Any, Sequence[Any]],
-    ) -> None:
-        # I'm fudging the types a bit here. ""Any"" above really depends
-        # on type(self). For PeriodArray, it's Period (or stuff coercible
-        # to a period in from_sequence). For DatetimeArray, it's Timestamp...
-        # I don't know if mypy can do that, possibly with Generics.
-        # https://mypy.readthedocs.io/en/latest/generics.html
-        if is_list_like(value):
-            is_slice = isinstance(key, slice)
-
-            if lib.is_scalar(key):
-                raise ValueError(""setting an array element with a sequence."")
-
-            if not is_slice:
-                key = cast(Sequence, key)
-                if len(key) != len(value) and not com.is_bool_indexer(key):
-                    msg = (
-                        f""shape mismatch: value array of length '{len(key)}' ""
-                        ""does not match indexing result of length ""
-                        f""'{len(value)}'.""
-                    )
-                    raise ValueError(msg)
-                elif not len(key):
-                    return
-
-        value = self._validate_setitem_value(value)
-        key = check_array_indexer(self, key)
-        self._data[key] = value
-        self._maybe_clear_freq()
-
-    def _maybe_clear_freq(self):
-        # inplace operations like __setitem__ may invalidate the freq of
-        # DatetimeArray and TimedeltaArray
-        pass
-
-    def astype(self, dtype, copy=True):
-        # Some notes on cases we don't have to handle here in the base class:
-        #   1. PeriodArray.astype handles period -> period
-        #   2. DatetimeArray.astype handles conversion between tz.
-        #   3. DatetimeArray.astype handles datetime -> period
-        dtype = pandas_dtype(dtype)
-
-        if is_object_dtype(dtype):
-            return self._box_values(self.asi8.ravel()).reshape(self.shape)
-        elif is_string_dtype(dtype) and not is_categorical_dtype(dtype):
-            return self._format_native_types()
-        elif is_integer_dtype(dtype):
-            # we deliberately ignore int32 vs. int64 here.
-            # See https://github.com/pandas-dev/pandas/issues/24381 for more.
-            values = self.asi8
-
-            if is_unsigned_integer_dtype(dtype):
-                # Again, we ignore int32 vs. int64
-                values = values.view(""uint64"")
-
-            if copy:
-                values = values.copy()
-            return values
-        elif (
-            is_datetime_or_timedelta_dtype(dtype)
-            and not is_dtype_equal(self.dtype, dtype)
-        ) or is_float_dtype(dtype):
-            # disallow conversion between datetime/timedelta,
-            # and conversions for any datetimelike to float
-            msg = f""Cannot cast {type(self).__name__} to dtype {dtype}""
-            raise TypeError(msg)
-        elif is_categorical_dtype(dtype):
-            arr_cls = dtype.construct_array_type()
-            return arr_cls(self, dtype=dtype)
-        else:
-            return np.asarray(self, dtype=dtype)
-
-    def view(self, dtype=None):
-        if dtype is None or dtype is self.dtype:
-            return type(self)(self._data, dtype=self.dtype)
-        return self._data.view(dtype=dtype)
-
-    # ------------------------------------------------------------------
-    # ExtensionArray Interface
-
-    def unique(self):
-        result = unique1d(self.asi8)
-        return type(self)(result, dtype=self.dtype)
-
-    @classmethod
-    def _concat_same_type(cls, to_concat, axis: int = 0):
-
-        # do not pass tz to set because tzlocal cannot be hashed
-        dtypes = {str(x.dtype) for x in to_concat}
-        if len(dtypes) != 1:
-            raise ValueError(""to_concat must have the same dtype (tz)"", dtypes)
-
-        obj = to_concat[0]
-        dtype = obj.dtype
-
-        i8values = [x.asi8 for x in to_concat]
-        values = np.concatenate(i8values, axis=axis)
-
-        new_freq = None
-        if is_period_dtype(dtype):
-            new_freq = obj.freq
-        elif axis == 0:
-            # GH 3232: If the concat result is evenly spaced, we can retain the
-            # original frequency
-            to_concat = [x for x in to_concat if len(x)]
-
-            if obj.freq is not None and all(x.freq == obj.freq for x in to_concat):
-                pairs = zip(to_concat[:-1], to_concat[1:])
-                if all(pair[0][-1] + obj.freq == pair[1][0] for pair in pairs):
-                    new_freq = obj.freq
-
-        return cls._simple_new(values, dtype=dtype, freq=new_freq)
-
-    def copy(self):
-        values = self.asi8.copy()
-        return type(self)._simple_new(values, dtype=self.dtype, freq=self.freq)
-
-    def _values_for_factorize(self):
-        return self.asi8, iNaT
-
-    @classmethod
-    def _from_factorized(cls, values, original):
-        return cls(values, dtype=original.dtype)
-
-    def _values_for_argsort(self):
-        return self._data
-
-    @Appender(ExtensionArray.shift.__doc__)
-    def shift(self, periods=1, fill_value=None, axis=0):
-        if not self.size or periods == 0:
-            return self.copy()
-
-        fill_value = self._validate_shift_value(fill_value)
-        new_values = shift(self._data, periods, axis, fill_value)
+        if is_period_dtype(self.dtype):
+            # Only used for ops that stay PeriodDtype
+            return self.freq
+        elif self.freq is None:
+            return None
+        elif lib.is_scalar(other) and isna(other):
+            return None
 
-        return type(self)._simple_new(new_values, dtype=self.dtype)
+        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
+            new_freq = None
+            if isinstance(self.freq, Tick):
+                new_freq = self.freq
+            return new_freq
 
-    # ------------------------------------------------------------------
-    # Validation Methods
-    # TODO: try to de-duplicate these, ensure identical behavior
+        elif isinstance(other, DateOffset):
+            # otherwise just DatetimeArray
+            return None  # TODO: Should we infer if it matches self.freq * n?
+        elif isinstance(other, (datetime, np.datetime64)):
+            return self.freq
 
-    def _validate_fill_value(self, fill_value):
-        """"""
-        If a fill_value is passed to `take` convert it to an i8 representation,
-        raising ValueError if this is not possible.
+        elif is_timedelta64_dtype(other):
+            return None  # TODO: shouldnt we be able to do self.freq + other.freq?
+        elif is_object_dtype(other):
+            return None  # TODO: is this quite right?  sometimes we unpack singletons
+        elif is_datetime64_any_dtype(other):
+            return None  # TODO: shouldnt we be able to do self.freq + other.freq?
+        else:
+            raise NotImplementedError
+
+    __add__ = _make_wrapped_arith_op_with_freq(""__add__"")
+    __sub__ = _make_wrapped_arith_op_with_freq(""__sub__"")
+    __radd__ = make_wrapped_arith_op(""__radd__"")
+    __rsub__ = make_wrapped_arith_op(""__rsub__"")
+    __pow__ = make_wrapped_arith_op(""__pow__"")
+    __rpow__ = make_wrapped_arith_op(""__rpow__"")
+    __mul__ = make_wrapped_arith_op(""__mul__"")
+    __rmul__ = make_wrapped_arith_op(""__rmul__"")
+    __floordiv__ = make_wrapped_arith_op(""__floordiv__"")
+    __rfloordiv__ = make_wrapped_arith_op(""__rfloordiv__"")
+    __mod__ = make_wrapped_arith_op(""__mod__"")
+    __rmod__ = make_wrapped_arith_op(""__rmod__"")
+    __divmod__ = make_wrapped_arith_op(""__divmod__"")
+    __rdivmod__ = make_wrapped_arith_op(""__rdivmod__"")
+    __truediv__ = make_wrapped_arith_op(""__truediv__"")
+    __rtruediv__ = make_wrapped_arith_op(""__rtruediv__"")
+
+    def isin(self, values, level=None):
+        """"""
+        Compute boolean array of whether each index value is found in the
+        passed set of values.
 
         Parameters
         ----------
-        fill_value : object
+        values : set or sequence of values
 
         Returns
         -------
-        fill_value : np.int64
-
-        Raises
-        ------
-        ValueError
+        is_contained : ndarray (boolean dtype)
         """"""
-        if is_valid_nat_for_dtype(fill_value, self.dtype):
-            fill_value = iNaT
-        elif isinstance(fill_value, self._recognized_scalars):
-            self._check_compatible_with(fill_value)
-            fill_value = self._scalar_type(fill_value)
-            fill_value = self._unbox_scalar(fill_value)
-        else:
-            raise ValueError(
-                f""'fill_value' should be a {self._scalar_type}. ""
-                f""Got '{str(fill_value)}'.""
-            )
-        return fill_value
-
-    def _validate_shift_value(self, fill_value):
-        # TODO(2.0): once this deprecation is enforced, used _validate_fill_value
-        if is_valid_nat_for_dtype(fill_value, self.dtype):
-            fill_value = NaT
-        elif not isinstance(fill_value, self._recognized_scalars):
-            # only warn if we're not going to raise
-            if self._scalar_type is Period and lib.is_integer(fill_value):
-                # kludge for #31971 since Period(integer) tries to cast to str
-                new_fill = Period._from_ordinal(fill_value, freq=self.freq)
-            else:
-                new_fill = self._scalar_type(fill_value)
-
-            # stacklevel here is chosen to be correct when called from
-            #  DataFrame.shift or Series.shift
-            warnings.warn(
-                f""Passing {type(fill_value)} to shift is deprecated and ""
-                ""will raise in a future version, pass ""
-                f""{self._scalar_type.__name__} instead."",
-                FutureWarning,
-                stacklevel=10,
-            )
-            fill_value = new_fill
-
-        fill_value = self._unbox_scalar(fill_value)
-        return fill_value
+        if level is not None:
+            self._validate_index_level(level)
 
-    def _validate_searchsorted_value(self, value):
-        if isinstance(value, str):
+        if not isinstance(values, type(self)):
             try:
-                value = self._scalar_from_string(value)
-            except ValueError as err:
-                raise TypeError(
-                    ""searchsorted requires compatible dtype or scalar""
-                ) from err
-
-        elif is_valid_nat_for_dtype(value, self.dtype):
-            value = NaT
-
-        elif isinstance(value, self._recognized_scalars):
-            value = self._scalar_type(value)
-
-        elif is_list_like(value) and not isinstance(value, type(self)):
-            value = array(value)
-
-            if not type(self)._is_recognized_dtype(value):
-                raise TypeError(
-                    ""searchsorted requires compatible dtype or scalar, ""
-                    f""not {type(value).__name__}""
-                )
-
-        if not (isinstance(value, (self._scalar_type, type(self))) or (value is NaT)):
-            raise TypeError(f""Unexpected type for 'value': {type(value)}"")
-
-        if isinstance(value, type(self)):
-            self._check_compatible_with(value)
-            value = value.asi8
-        else:
-            value = self._unbox_scalar(value)
-
-        return value
-
-    def _validate_setitem_value(self, value):
-        if lib.is_scalar(value) and not isna(value):
-            value = com.maybe_box_datetimelike(value)
-
-        if is_list_like(value):
-            value = type(self)._from_sequence(value, dtype=self.dtype)
-            self._check_compatible_with(value, setitem=True)
-            value = value.asi8
-        elif isinstance(value, self._scalar_type):
-            self._check_compatible_with(value, setitem=True)
-            value = self._unbox_scalar(value)
-        elif is_valid_nat_for_dtype(value, self.dtype):
-            value = iNaT
-        else:
-            msg = (
-                f""'value' should be a '{self._scalar_type.__name__}', 'NaT', ""
-                f""or array of those. Got '{type(value).__name__}' instead.""
-            )
-            raise TypeError(msg)
-
-        return value
-
-    def _validate_insert_value(self, value):
-        if isinstance(value, self._recognized_scalars):
-            value = self._scalar_type(value)
-        elif is_valid_nat_for_dtype(value, self.dtype):
-            # GH#18295
-            value = NaT
-        elif lib.is_scalar(value) and isna(value):
-            raise TypeError(
-                f""cannot insert {type(self).__name__} with incompatible label""
-            )
-
-        return value
+                values = type(self)(values)
+            except ValueError:
+                return self.astype(object).isin(values)
 
-    def _validate_where_value(self, other):
-        if is_valid_nat_for_dtype(other, self.dtype):
-            other = NaT
-        elif isinstance(other, self._recognized_scalars):
-            other = self._scalar_type(other)
-            self._check_compatible_with(other, setitem=True)
-        elif not is_list_like(other):
-            raise TypeError(f""Where requires matching dtype, not {type(other)}"")
+        return algorithms.isin(self.asi8, values.asi8)
 
-        else:
-            # Do type inference if necessary up front
-            # e.g. we passed PeriodIndex.values and got an ndarray of Periods
-            other = array(other)
-            other = extract_array(other, extract_numpy=True)
-
-            if is_categorical_dtype(other.dtype):
-                # e.g. we have a Categorical holding self.dtype
-                if is_dtype_equal(other.categories.dtype, self.dtype):
-                    other = other._internal_get_values()
-
-            if not type(self)._is_recognized_dtype(other.dtype):
-                raise TypeError(f""Where requires matching dtype, not {other.dtype}"")
-            self._check_compatible_with(other, setitem=True)
-
-        if lib.is_scalar(other):
-            other = self._unbox_scalar(other)
-        else:
-            other = other.view(""i8"")
+    @Appender(Index.where.__doc__)
+    def where(self, cond, other=None):
+        values = self.view(""i8"")
 
-        return other
+        try:
+            other = self._data._validate_where_value(other)
+        except (TypeError, ValueError) as err:
+            # Includes tzawareness mismatch and IncompatibleFrequencyError
+            oth = getattr(other, ""dtype"", other)
+            raise TypeError(f""Where requires matching dtype, not {oth}"") from err
 
-    # ------------------------------------------------------------------
-    # Additional array methods
-    #  These are not part of the EA API, but we implement them because
-    #  pandas assumes they're there.
+        result = np.where(cond, values, other).astype(""i8"")
+        arr = type(self._data)._simple_new(result, dtype=self.dtype)
+        return type(self)._simple_new(arr, name=self.name)
 
-    def searchsorted(self, value, side=""left"", sorter=None):
+    def _summary(self, name=None) -> str:
         """"""
-        Find indices where elements should be inserted to maintain order.
-
-        Find the indices into a sorted array `self` such that, if the
-        corresponding elements in `value` were inserted before the indices,
-        the order of `self` would be preserved.
+        Return a summarized representation.
 
         Parameters
         ----------
-        value : array_like
-            Values to insert into `self`.
-        side : {'left', 'right'}, optional
-            If 'left', the index of the first suitable location found is given.
-            If 'right', return the last such index.  If there is no suitable
-            index, return either 0 or N (where N is the length of `self`).
-        sorter : 1-D array_like, optional
-            Optional array of integer indices that sort `self` into ascending
-            order. They are typically the result of ``np.argsort``.
+        name : str
+            Name to use in the summary representation.
 
         Returns
         -------
-        indices : array of ints
-            Array of insertion points with the same shape as `value`.
+        str
+            Summarized representation of the index.
         """"""
-        value = self._validate_searchsorted_value(value)
+        formatter = self._formatter_func
+        if len(self) > 0:
+            index_summary = f"", {formatter(self[0])} to {formatter(self[-1])}""
+        else:
+            index_summary = """"
 
-        # TODO: Use datetime64 semantics for sorting, xref GH#29844
-        return self.asi8.searchsorted(value, side=side, sorter=sorter)
+        if name is None:
+            name = type(self).__name__
+        result = f""{name}: {len(self)} entries{index_summary}""
+        if self.freq:
+            result += f""\nFreq: {self.freqstr}""
 
-    def repeat(self, repeats, *args, **kwargs):
-        """"""
-        Repeat elements of an array.
+        # display as values, not quoted
+        result = result.replace(""'"", """")
+        return result
 
-        See Also
-        --------
-        numpy.ndarray.repeat
+    def shift(self, periods=1, freq=None):
         """"""
-        nv.validate_repeat(args, kwargs)
-        values = self._data.repeat(repeats)
-        return type(self)(values.view(""i8""), dtype=self.dtype)
+        Shift index by desired number of time frequency increments.
 
-    def value_counts(self, dropna=False):
-        """"""
-        Return a Series containing counts of unique values.
+        This method is for shifting the values of datetime-like indexes
+        by a specified time increment a given number of times.
 
         Parameters
         ----------
-        dropna : bool, default True
-            Don't include counts of NaT values.
+        periods : int, default 1
+            Number of periods (or increments) to shift by,
+            can be positive or negative.
 
-        Returns
-        -------
-        Series
-        """"""
-        from pandas import Series, Index
+            .. versionchanged:: 0.24.0
 
-        if dropna:
-            values = self[~self.isna()]._data
-        else:
-            values = self._data
-
-        cls = type(self)
+        freq : pandas.DateOffset, pandas.Timedelta or string, optional
+            Frequency increment to shift by.
+            If None, the index is shifted by its own `freq` attribute.
+            Offset aliases are valid strings, e.g., 'D', 'W', 'M' etc.
 
-        result = value_counts(values, sort=False, dropna=dropna)
-        index = Index(
-            cls(result.index.view(""i8""), dtype=self.dtype), name=result.index.name
-        )
-        return Series(result._values, index=index, name=result.name)
+        Returns
+        -------
+        pandas.DatetimeIndex
+            Shifted index.
 
-    def map(self, mapper):
-        # TODO(GH-23179): Add ExtensionArray.map
-        # Need to figure out if we want ExtensionArray.map first.
-        # If so, then we can refactor IndexOpsMixin._map_values to
-        # a standalone function and call from here..
-        # Else, just rewrite _map_infer_values to do the right thing.
-        from pandas import Index
+        See Also
+        --------
+        Index.shift : Shift values of Index.
+        PeriodIndex.shift : Shift values of PeriodIndex.
+        """"""
+        arr = self._data.view()
+        arr._freq = self.freq
+        result = arr._time_shift(periods, freq=freq)
+        return type(self)(result, name=self.name)
 
-        return Index(self).map(mapper).array
+    # --------------------------------------------------------------------
+    # List-like Methods
 
-    # ------------------------------------------------------------------
-    # Null Handling
+    def delete(self, loc):
+        new_i8s = np.delete(self.asi8, loc)
 
-    def isna(self):
-        return self._isnan
+        freq = None
+        if is_period_dtype(self):
+            freq = self.freq
+        elif is_integer(loc):
+            if loc in (0, -len(self), -1, len(self) - 1):
+                freq = self.freq
+        else:
+            if is_list_like(loc):
+                loc = lib.maybe_indices_to_slice(ensure_int64(np.array(loc)), len(self))
+            if isinstance(loc, slice) and loc.step in (1, None):
+                if loc.start in (0, None) or loc.stop in (len(self), None):
+                    freq = self.freq
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _isnan(self):
-        """"""
-        return if each value is nan
-        """"""
-        return self.asi8 == iNaT
+        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
+        return type(self)._simple_new(arr, name=self.name)
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _hasnans(self):
-        """"""
-        return if I have any nans; enables various perf speedups
-        """"""
-        return bool(self._isnan.any())
+    # --------------------------------------------------------------------
+    # Join/Set Methods
 
-    def _maybe_mask_results(self, result, fill_value=iNaT, convert=None):
-        """"""
-        Parameters
-        ----------
-        result : a ndarray
-        fill_value : object, default iNaT
-        convert : str, dtype or None
+    def _wrap_joined_index(self, joined: np.ndarray, other):
+        assert other.dtype == self.dtype, (other.dtype, self.dtype)
+        name = get_op_result_name(self, other)
 
-        Returns
-        -------
-        result : ndarray with values replace by the fill_value
+        if is_period_dtype(self.dtype):
+            freq = self.freq
+        else:
+            self = cast(DatetimeTimedeltaMixin, self)
+            freq = self.freq if self._can_fast_union(other) else None
+        new_data = type(self._data)._simple_new(joined, dtype=self.dtype, freq=freq)
 
-        mask the result if needed, convert to the provided dtype if its not
-        None
+        return type(self)._simple_new(new_data, name=name)
 
-        This is an internal routine.
-        """"""
-        if self._hasnans:
-            if convert:
-                result = result.astype(convert)
-            if fill_value is None:
-                fill_value = np.nan
-            result[self._isnan] = fill_value
-        return result
 
-    def fillna(self, value=None, method=None, limit=None):
-        # TODO(GH-20300): remove this
-        # Just overriding to ensure that we avoid an astype(object).
-        # Either 20300 or a `_values_for_fillna` would avoid this duplication.
-        if isinstance(value, ABCSeries):
-            value = value.array
-
-        value, method = validate_fillna_kwargs(value, method)
-
-        mask = self.isna()
-
-        if is_array_like(value):
-            if len(value) != len(self):
-                raise ValueError(
-                    f""Length of 'value' does not match. Got ({len(value)}) ""
-                    f"" expected {len(self)}""
-                )
-            value = value[mask]
-
-        if mask.any():
-            if method is not None:
-                if method == ""pad"":
-                    func = missing.pad_1d
-                else:
-                    func = missing.backfill_1d
-
-                values = self._data
-                if not is_period_dtype(self):
-                    # For PeriodArray self._data is i8, which gets copied
-                    #  by `func`.  Otherwise we need to make a copy manually
-                    # to avoid modifying `self` in-place.
-                    values = values.copy()
-
-                new_values = func(values, limit=limit, mask=mask)
-                if is_datetime64tz_dtype(self):
-                    # we need to pass int64 values to the constructor to avoid
-                    #  re-localizing incorrectly
-                    new_values = new_values.view(""i8"")
-                new_values = type(self)(new_values, dtype=self.dtype)
-            else:
-                # fill with value
-                new_values = self.copy()
-                new_values[mask] = value
-        else:
-            new_values = self.copy()
-        return new_values
+class DatetimeTimedeltaMixin(DatetimeIndexOpsMixin, Int64Index):
+    """"""
+    Mixin class for methods shared by DatetimeIndex and TimedeltaIndex,
+    but not PeriodIndex
+    """"""
 
-    # ------------------------------------------------------------------
-    # Frequency Properties/Methods
+    # Compat for frequency inference, see GH#23789
+    _is_monotonic_increasing = Index.is_monotonic_increasing
+    _is_monotonic_decreasing = Index.is_monotonic_decreasing
+    _is_unique = Index.is_unique
+    _freq = lib.no_default
 
     @property
     def freq(self):
         """"""
-        Return the frequency object if it is set, otherwise None.
+        In limited circumstances, our freq may differ from that of our _data.
         """"""
-        return self._freq
-
-    @freq.setter
-    def freq(self, value):
-        if value is not None:
-            value = frequencies.to_offset(value)
-            self._validate_frequency(self, value)
-
-        self._freq = value
+        if self._freq is not lib.no_default:
+            return self._freq
+        return self._data.freq
 
     @property
     def freqstr(self):
@@ -1102,708 +675,321 @@ class DatetimeLikeArrayMixin(
             return None
         return self.freq.freqstr
 
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def inferred_freq(self):
-        """"""
-        Tryies to return a string representing a frequency guess,
-        generated by infer_freq.  Returns None if it can't autodetect the
-        frequency.
-        """"""
-        if self.ndim != 1:
-            return None
-        try:
-            return frequencies.infer_freq(self)
-        except ValueError:
-            return None
-
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def _resolution(self):
-        return frequencies.Resolution.get_reso_from_freq(self.freqstr)
-
-    @property  # NB: override with cache_readonly in immutable subclasses
-    def resolution(self):
-        """"""
-        Returns day, hour, minute, second, millisecond or microsecond
-        """"""
-        return frequencies.Resolution.get_str(self._resolution)
-
-    @classmethod
-    def _validate_frequency(cls, index, freq, **kwargs):
-        """"""
-        Validate that a frequency is compatible with the values of a given
-        Datetime Array/Index or Timedelta Array/Index
-
-        Parameters
-        ----------
-        index : DatetimeIndex or TimedeltaIndex
-            The index on which to determine if the given frequency is valid
-        freq : DateOffset
-            The frequency to validate
-        """"""
-        if is_period_dtype(cls):
-            # Frequency validation is not meaningful for Period Array/Index
-            return None
-
-        inferred = index.inferred_freq
-        if index.size == 0 or inferred == freq.freqstr:
-            return None
-
-        try:
-            on_freq = cls._generate_range(
-                start=index[0], end=None, periods=len(index), freq=freq, **kwargs
-            )
-            if not np.array_equal(index.asi8, on_freq.asi8):
-                raise ValueError
-        except ValueError as e:
-            if ""non-fixed"" in str(e):
-                # non-fixed frequencies are not meaningful for timedelta64;
-                #  we retain that error message
-                raise e
-            # GH#11587 the main way this is reached is if the `np.array_equal`
-            #  check above is False.  This can also be reached if index[0]
-            #  is `NaT`, in which case the call to `cls._generate_range` will
-            #  raise a ValueError, which we re-raise with a more targeted
-            #  message.
-            raise ValueError(
-                f""Inferred frequency {inferred} from passed values ""
-                f""does not conform to passed frequency {freq.freqstr}""
-            ) from e
-
-    # monotonicity/uniqueness properties are called via frequencies.infer_freq,
-    #  see GH#23789
-
-    @property
-    def _is_monotonic_increasing(self):
-        return algos.is_monotonic(self.asi8, timelike=True)[0]
-
-    @property
-    def _is_monotonic_decreasing(self):
-        return algos.is_monotonic(self.asi8, timelike=True)[1]
-
-    @property
-    def _is_unique(self):
-        return len(unique1d(self.asi8)) == len(self)
-
-    # ------------------------------------------------------------------
-    # Arithmetic Methods
-    _create_comparison_method = classmethod(_datetimelike_array_cmp)
-
-    # pow is invalid for all three subclasses; TimedeltaArray will override
-    #  the multiplication and division ops
-    __pow__ = make_invalid_op(""__pow__"")
-    __rpow__ = make_invalid_op(""__rpow__"")
-    __mul__ = make_invalid_op(""__mul__"")
-    __rmul__ = make_invalid_op(""__rmul__"")
-    __truediv__ = make_invalid_op(""__truediv__"")
-    __rtruediv__ = make_invalid_op(""__rtruediv__"")
-    __floordiv__ = make_invalid_op(""__floordiv__"")
-    __rfloordiv__ = make_invalid_op(""__rfloordiv__"")
-    __mod__ = make_invalid_op(""__mod__"")
-    __rmod__ = make_invalid_op(""__rmod__"")
-    __divmod__ = make_invalid_op(""__divmod__"")
-    __rdivmod__ = make_invalid_op(""__rdivmod__"")
-
-    def _add_datetimelike_scalar(self, other):
-        # Overridden by TimedeltaArray
-        raise TypeError(f""cannot add {type(self).__name__} and {type(other).__name__}"")
-
-    _add_datetime_arraylike = _add_datetimelike_scalar
-
-    def _sub_datetimelike_scalar(self, other):
-        # Overridden by DatetimeArray
-        assert other is not NaT
-        raise TypeError(f""cannot subtract a datelike from a {type(self).__name__}"")
-
-    _sub_datetime_arraylike = _sub_datetimelike_scalar
-
-    def _sub_period(self, other):
-        # Overridden by PeriodArray
-        raise TypeError(f""cannot subtract Period from a {type(self).__name__}"")
-
-    def _add_offset(self, offset):
-        raise AbstractMethodError(self)
-
-    def _add_timedeltalike_scalar(self, other):
-        """"""
-        Add a delta of a timedeltalike
-
-        Returns
-        -------
-        Same type as self
-        """"""
-        if isna(other):
-            # i.e np.timedelta64(""NaT""), not recognized by delta_to_nanoseconds
-            new_values = np.empty(self.shape, dtype=""i8"")
-            new_values[:] = iNaT
-            return type(self)(new_values, dtype=self.dtype)
-
-        inc = delta_to_nanoseconds(other)
-        new_values = checked_add_with_arr(self.asi8, inc, arr_mask=self._isnan).view(
-            ""i8""
-        )
-        new_values = self._maybe_mask_results(new_values)
-
-        new_freq = None
-        if isinstance(self.freq, Tick) or is_period_dtype(self.dtype):
-            # adding a scalar preserves freq
-            new_freq = self.freq
-
-        return type(self)(new_values, dtype=self.dtype, freq=new_freq)
+    def _with_freq(self, freq):
+        index = self.copy(deep=False)
+        if freq is None:
+            # Even if we _can_ have a freq, we might want to set it to None
+            index._freq = None
+        elif len(self) == 0 and isinstance(freq, DateOffset):
+            # Always valid.  In the TimedeltaArray case, we assume this
+            #  is a Tick offset.
+            index._freq = freq
+        else:
+            assert freq == ""infer"", freq
+            freq = to_offset(self.inferred_freq)
+            index._freq = freq
 
-    def _add_timedelta_arraylike(self, other):
-        """"""
-        Add a delta of a TimedeltaIndex
+        return index
 
-        Returns
-        -------
-        Same type as self
-        """"""
-        # overridden by PeriodArray
+    def _shallow_copy(self, values=None, name: Label = lib.no_default):
+        name = self.name if name is lib.no_default else name
+        cache = self._cache.copy() if values is None else {}
 
-        if len(self) != len(other):
-            raise ValueError(""cannot add indices of unequal length"")
+        if values is None:
+            values = self._data
 
-        if isinstance(other, np.ndarray):
-            # ndarray[timedelta64]; wrap in TimedeltaIndex for op
-            from pandas.core.arrays import TimedeltaArray
+        if isinstance(values, np.ndarray):
+            # TODO: We would rather not get here
+            values = type(self._data)(values, dtype=self.dtype)
 
-            other = TimedeltaArray._from_sequence(other)
+        result = type(self)._simple_new(values, name=name)
+        result._cache = cache
+        return result
 
-        self_i8 = self.asi8
-        other_i8 = other.asi8
-        new_values = checked_add_with_arr(
-            self_i8, other_i8, arr_mask=self._isnan, b_mask=other._isnan
-        )
-        if self._hasnans or other._hasnans:
-            mask = (self._isnan) | (other._isnan)
-            new_values[mask] = iNaT
+    # --------------------------------------------------------------------
+    # Set Operation Methods
 
-        return type(self)(new_values, dtype=self.dtype)
+    @Appender(Index.difference.__doc__)
+    def difference(self, other, sort=None):
+        new_idx = super().difference(other, sort=sort)._with_freq(None)
+        return new_idx
 
-    def _add_nat(self):
+    def intersection(self, other, sort=False):
         """"""
-        Add pd.NaT to self
-        """"""
-        if is_period_dtype(self):
-            raise TypeError(
-                f""Cannot add {type(self).__name__} and {type(NaT).__name__}""
-            )
+        Specialized intersection for DatetimeIndex/TimedeltaIndex.
 
-        # GH#19124 pd.NaT is treated like a timedelta for both timedelta
-        # and datetime dtypes
-        result = np.zeros(self.shape, dtype=np.int64)
-        result.fill(iNaT)
-        return type(self)(result, dtype=self.dtype, freq=None)
-
-    def _sub_nat(self):
-        """"""
-        Subtract pd.NaT from self
-        """"""
-        # GH#19124 Timedelta - datetime is not in general well-defined.
-        # We make an exception for pd.NaT, which in this case quacks
-        # like a timedelta.
-        # For datetime64 dtypes by convention we treat NaT as a datetime, so
-        # this subtraction returns a timedelta64 dtype.
-        # For period dtype, timedelta64 is a close-enough return dtype.
-        result = np.zeros(self.shape, dtype=np.int64)
-        result.fill(iNaT)
-        return result.view(""timedelta64[ns]"")
-
-    def _sub_period_array(self, other):
-        """"""
-        Subtract a Period Array/Index from self.  This is only valid if self
-        is itself a Period Array/Index, raises otherwise.  Both objects must
-        have the same frequency.
+        May be much faster than Index.intersection
 
         Parameters
         ----------
-        other : PeriodIndex or PeriodArray
-
-        Returns
-        -------
-        result : np.ndarray[object]
-            Array of DateOffset objects; nulls represented by NaT.
-        """"""
-        if not is_period_dtype(self):
-            raise TypeError(
-                f""cannot subtract {other.dtype}-dtype from {type(self).__name__}""
-            )
+        other : Same type as self or array-like
+        sort : False or None, default False
+            Sort the resulting index if possible.
 
-        if self.freq != other.freq:
-            msg = DIFFERENT_FREQ.format(
-                cls=type(self).__name__, own_freq=self.freqstr, other_freq=other.freqstr
-            )
-            raise IncompatibleFrequency(msg)
+            .. versionadded:: 0.24.0
 
-        new_values = checked_add_with_arr(
-            self.asi8, -other.asi8, arr_mask=self._isnan, b_mask=other._isnan
-        )
+            .. versionchanged:: 0.24.1
 
-        new_values = np.array([self.freq.base * x for x in new_values])
-        if self._hasnans or other._hasnans:
-            mask = (self._isnan) | (other._isnan)
-            new_values[mask] = NaT
-        return new_values
+               Changed the default to ``False`` to match the behaviour
+               from before 0.24.0.
 
-    def _addsub_object_array(self, other: np.ndarray, op):
-        """"""
-        Add or subtract array-like of DateOffset objects
+            .. versionchanged:: 0.25.0
 
-        Parameters
-        ----------
-        other : np.ndarray[object]
-        op : {operator.add, operator.sub}
+               The `sort` keyword is added
 
         Returns
         -------
-        result : same class as self
+        y : Index or same type as self
         """"""
-        assert op in [operator.add, operator.sub]
-        if len(other) == 1:
-            return op(self, other[0])
-
-        warnings.warn(
-            ""Adding/subtracting array of DateOffsets to ""
-            f""{type(self).__name__} not vectorized"",
-            PerformanceWarning,
-        )
+        self._validate_sort_keyword(sort)
+        self._assert_can_do_setop(other)
 
-        # Caller is responsible for broadcasting if necessary
-        assert self.shape == other.shape, (self.shape, other.shape)
+        if self.equals(other):
+            return self._get_reconciled_name_object(other)
 
-        res_values = op(self.astype(""O""), np.array(other))
-        result = array(res_values.ravel())
-        result = extract_array(result, extract_numpy=True).reshape(self.shape)
-        return result
-
-    def _time_shift(self, periods, freq=None):
-        """"""
-        Shift each value by `periods`.
-
-        Note this is different from ExtensionArray.shift, which
-        shifts the *position* of each element, padding the end with
-        missing values.
+        if len(self) == 0:
+            return self.copy()
+        if len(other) == 0:
+            return other.copy()
+
+        if not isinstance(other, type(self)):
+            result = Index.intersection(self, other, sort=sort)
+            if isinstance(result, type(self)):
+                if result.freq is None:
+                    result = result._with_freq(""infer"")
+            return result
 
-        Parameters
-        ----------
-        periods : int
-            Number of periods to shift by.
-        freq : pandas.DateOffset, pandas.Timedelta, or str
-            Frequency increment to shift by.
-        """"""
-        if freq is not None and freq != self.freq:
-            if isinstance(freq, str):
-                freq = frequencies.to_offset(freq)
-            offset = periods * freq
-            result = self + offset
+        elif (
+            other.freq is None
+            or self.freq is None
+            or other.freq != self.freq
+            or not other.freq.is_anchored()
+            or (not self.is_monotonic or not other.is_monotonic)
+        ):
+            result = Index.intersection(self, other, sort=sort)
+            result = result._with_freq(""infer"")
             return result
 
-        if periods == 0:
-            # immutable so OK
-            return self.copy()
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        else:
+            left, right = other, self
 
-        if self.freq is None:
-            raise NullFrequencyError(""Cannot shift with no freq"")
+        # after sorting, the intersection always starts with the right index
+        # and ends with the index of which the last elements is smallest
+        end = min(left[-1], right[-1])
+        start = right[0]
 
-        start = self[0] + periods * self.freq
-        end = self[-1] + periods * self.freq
+        if end < start:
+            return type(self)(data=[], dtype=self.dtype, freq=self.freq)
+        else:
+            lslice = slice(*left.slice_locs(start, end))
+            left_chunk = left._values[lslice]
+            return self._shallow_copy(left_chunk)
 
-        # Note: in the DatetimeTZ case, _generate_range will infer the
-        #  appropriate timezone from `start` and `end`, so tz does not need
-        #  to be passed explicitly.
-        return self._generate_range(start=start, end=end, periods=None, freq=self.freq)
+    def _can_fast_union(self, other) -> bool:
+        if not isinstance(other, type(self)):
+            return False
 
-    @unpack_zerodim_and_defer(""__add__"")
-    def __add__(self, other):
+        freq = self.freq
 
-        # scalar others
-        if other is NaT:
-            result = self._add_nat()
-        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
-            result = self._add_timedeltalike_scalar(other)
-        elif isinstance(other, DateOffset):
-            # specifically _not_ a Tick
-            result = self._add_offset(other)
-        elif isinstance(other, (datetime, np.datetime64)):
-            result = self._add_datetimelike_scalar(other)
-        elif lib.is_integer(other):
-            # This check must come after the check for np.timedelta64
-            # as is_integer returns True for these
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._time_shift(other)
-
-        # array-like others
-        elif is_timedelta64_dtype(other):
-            # TimedeltaIndex, ndarray[timedelta64]
-            result = self._add_timedelta_arraylike(other)
-        elif is_object_dtype(other):
-            # e.g. Array/Index of DateOffset objects
-            result = self._addsub_object_array(other, operator.add)
-        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
-            # DatetimeIndex, ndarray[datetime64]
-            return self._add_datetime_arraylike(other)
-        elif is_integer_dtype(other):
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._addsub_int_array(other, operator.add)
-        else:
-            # Includes Categorical, other ExtensionArrays
-            # For PeriodDtype, if self is a TimedeltaArray and other is a
-            #  PeriodArray with  a timedelta-like (i.e. Tick) freq, this
-            #  operation is valid.  Defer to the PeriodArray implementation.
-            #  In remaining cases, this will end up raising TypeError.
-            return NotImplemented
+        if freq is None or freq != other.freq:
+            return False
 
-        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
-            from pandas.core.arrays import TimedeltaArray
+        if not self.is_monotonic or not other.is_monotonic:
+            return False
 
-            return TimedeltaArray(result)
-        return result
+        if len(self) == 0 or len(other) == 0:
+            return True
 
-    def __radd__(self, other):
-        # alias for __add__
-        return self.__add__(other)
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        else:
+            left, right = other, self
 
-    @unpack_zerodim_and_defer(""__sub__"")
-    def __sub__(self, other):
+        right_start = right[0]
+        left_end = left[-1]
 
-        # scalar others
-        if other is NaT:
-            result = self._sub_nat()
-        elif isinstance(other, (Tick, timedelta, np.timedelta64)):
-            result = self._add_timedeltalike_scalar(-other)
-        elif isinstance(other, DateOffset):
-            # specifically _not_ a Tick
-            result = self._add_offset(-other)
-        elif isinstance(other, (datetime, np.datetime64)):
-            result = self._sub_datetimelike_scalar(other)
-        elif lib.is_integer(other):
-            # This check must come after the check for np.timedelta64
-            # as is_integer returns True for these
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._time_shift(-other)
-
-        elif isinstance(other, Period):
-            result = self._sub_period(other)
-
-        # array-like others
-        elif is_timedelta64_dtype(other):
-            # TimedeltaIndex, ndarray[timedelta64]
-            result = self._add_timedelta_arraylike(-other)
-        elif is_object_dtype(other):
-            # e.g. Array/Index of DateOffset objects
-            result = self._addsub_object_array(other, operator.sub)
-        elif is_datetime64_dtype(other) or is_datetime64tz_dtype(other):
-            # DatetimeIndex, ndarray[datetime64]
-            result = self._sub_datetime_arraylike(other)
-        elif is_period_dtype(other):
-            # PeriodIndex
-            result = self._sub_period_array(other)
-        elif is_integer_dtype(other):
-            if not is_period_dtype(self):
-                raise integer_op_not_supported(self)
-            result = self._addsub_int_array(other, operator.sub)
+        # Only need to ""adjoin"", not overlap
+        try:
+            return (right_start == left_end + freq) or right_start in left
+        except ValueError:
+            # if we are comparing a freq that does not propagate timezones
+            # this will raise
+            return False
+
+    def _fast_union(self, other, sort=None):
+        if len(other) == 0:
+            return self.view(type(self))
+
+        if len(self) == 0:
+            return other.view(type(self))
+
+        # to make our life easier, ""sort"" the two ranges
+        if self[0] <= other[0]:
+            left, right = self, other
+        elif sort is False:
+            # TDIs are not in the ""correct"" order and we don't want
+            #  to sort but want to remove overlaps
+            left, right = self, other
+            left_start = left[0]
+            loc = right.searchsorted(left_start, side=""left"")
+            right_chunk = right._values[:loc]
+            dates = concat_compat((left._values, right_chunk))
+            # TODO: can we infer that it has self.freq?
+            result = self._shallow_copy(dates)._with_freq(""infer"")
+            return result
         else:
-            # Includes ExtensionArrays, float_dtype
-            return NotImplemented
+            left, right = other, self
+
+        left_end = left[-1]
+        right_end = right[-1]
+
+        # concatenate
+        if left_end < right_end:
+            loc = right.searchsorted(left_end, side=""right"")
+            right_chunk = right._values[loc:]
+            dates = concat_compat([left._values, right_chunk])
+            # TODO: can we infer that it has self.freq?
+            result = self._shallow_copy(dates)._with_freq(""infer"")
+            return result
+        else:
+            return left
 
-        if is_timedelta64_dtype(result) and isinstance(result, np.ndarray):
-            from pandas.core.arrays import TimedeltaArray
+    def _union(self, other, sort):
+        if not len(other) or self.equals(other) or not len(self):
+            return super()._union(other, sort=sort)
 
-            return TimedeltaArray(result)
-        return result
+        # We are called by `union`, which is responsible for this validation
+        assert isinstance(other, type(self))
 
-    def __rsub__(self, other):
-        if is_datetime64_any_dtype(other) and is_timedelta64_dtype(self.dtype):
-            # ndarray[datetime64] cannot be subtracted from self, so
-            # we need to wrap in DatetimeArray/Index and flip the operation
-            if lib.is_scalar(other):
-                # i.e. np.datetime64 object
-                return Timestamp(other) - self
-            if not isinstance(other, DatetimeLikeArrayMixin):
-                # Avoid down-casting DatetimeIndex
-                from pandas.core.arrays import DatetimeArray
-
-                other = DatetimeArray(other)
-            return other - self
-        elif (
-            is_datetime64_any_dtype(self.dtype)
-            and hasattr(other, ""dtype"")
-            and not is_datetime64_any_dtype(other.dtype)
-        ):
-            # GH#19959 datetime - datetime is well-defined as timedelta,
-            # but any other type - datetime is not well-defined.
-            raise TypeError(
-                f""cannot subtract {type(self).__name__} from {type(other).__name__}""
-            )
-        elif is_period_dtype(self.dtype) and is_timedelta64_dtype(other):
-            # TODO: Can we simplify/generalize these cases at all?
-            raise TypeError(f""cannot subtract {type(self).__name__} from {other.dtype}"")
-        elif is_timedelta64_dtype(self.dtype):
-            if lib.is_integer(other) or is_integer_dtype(other):
-                # need to subtract before negating, since that flips freq
-                # -self flips self.freq, messing up results
-                return -(self - other)
-
-            return (-self) + other
-
-        return -(self - other)
-
-    def __iadd__(self, other):
-        result = self + other
-        self[:] = result[:]
-
-        if not is_period_dtype(self):
-            # restore freq, which is invalidated by setitem
-            self._freq = result._freq
-        return self
-
-    def __isub__(self, other):
-        result = self - other
-        self[:] = result[:]
-
-        if not is_period_dtype(self):
-            # restore freq, which is invalidated by setitem
-            self._freq = result._freq
-        return self
-
-    # --------------------------------------------------------------
-    # Reductions
-
-    def _reduce(self, name, axis=0, skipna=True, **kwargs):
-        op = getattr(self, name, None)
-        if op:
-            return op(skipna=skipna, **kwargs)
+        this, other = self._maybe_utc_convert(other)
+
+        if this._can_fast_union(other):
+            result = this._fast_union(other, sort=sort)
+            if result.freq is None:
+                result = result._with_freq(""infer"")
+            return result
         else:
-            return super()._reduce(name, skipna, **kwargs)
+            i8self = Int64Index._simple_new(self.asi8, name=self.name)
+            i8other = Int64Index._simple_new(other.asi8, name=other.name)
+            i8result = i8self._union(i8other, sort=sort)
+            result = type(self)(i8result, dtype=self.dtype, freq=""infer"")
+            return result
 
-    def min(self, axis=None, skipna=True, *args, **kwargs):
-        """"""
-        Return the minimum value of the Array or minimum along
-        an axis.
+    # --------------------------------------------------------------------
+    # Join Methods
+    _join_precedence = 10
 
-        See Also
-        --------
-        numpy.ndarray.min
-        Index.min : Return the minimum value in an Index.
-        Series.min : Return the minimum value in a Series.
+    _inner_indexer = _join_i8_wrapper(libjoin.inner_join_indexer)
+    _outer_indexer = _join_i8_wrapper(libjoin.outer_join_indexer)
+    _left_indexer = _join_i8_wrapper(libjoin.left_join_indexer)
+    _left_indexer_unique = _join_i8_wrapper(
+        libjoin.left_join_indexer_unique, with_indexers=False
+    )
+
+    def join(
+        self, other, how: str = ""left"", level=None, return_indexers=False, sort=False
+    ):
         """"""
-        nv.validate_min(args, kwargs)
-        nv.validate_minmax_axis(axis)
+        See Index.join
+        """"""
+        if self._is_convertible_to_index_for_join(other):
+            try:
+                other = type(self)(other)
+            except (TypeError, ValueError):
+                pass
 
-        result = nanops.nanmin(self.asi8, skipna=skipna, mask=self.isna())
-        if isna(result):
-            # Period._from_ordinal does not handle np.nan gracefully
-            return NaT
-        return self._box_func(result)
+        this, other = self._maybe_utc_convert(other)
+        return Index.join(
+            this,
+            other,
+            how=how,
+            level=level,
+            return_indexers=return_indexers,
+            sort=sort,
+        )
 
-    def max(self, axis=None, skipna=True, *args, **kwargs):
-        """"""
-        Return the maximum value of the Array or maximum along
-        an axis.
+    def _maybe_utc_convert(self, other):
+        this = self
+        if not hasattr(self, ""tz""):
+            return this, other
 
-        See Also
-        --------
-        numpy.ndarray.max
-        Index.max : Return the maximum value in an Index.
-        Series.max : Return the maximum value in a Series.
-        """"""
-        # TODO: skipna is broken with max.
-        # See https://github.com/pandas-dev/pandas/issues/24265
-        nv.validate_max(args, kwargs)
-        nv.validate_minmax_axis(axis)
+        if isinstance(other, type(self)):
+            if self.tz is not None:
+                if other.tz is None:
+                    raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
+            elif other.tz is not None:
+                raise TypeError(""Cannot join tz-naive with tz-aware DatetimeIndex"")
 
-        mask = self.isna()
-        if skipna:
-            values = self[~mask].asi8
-        elif mask.any():
-            return NaT
-        else:
-            values = self.asi8
+            if not timezones.tz_compare(self.tz, other.tz):
+                this = self.tz_convert(""UTC"")
+                other = other.tz_convert(""UTC"")
+        return this, other
 
-        if not len(values):
-            # short-circuit for empty max / min
-            return NaT
+    @classmethod
+    def _is_convertible_to_index_for_join(cls, other: Index) -> bool:
+        """"""
+        return a boolean whether I can attempt conversion to a
+        DatetimeIndex/TimedeltaIndex
+        """"""
+        if isinstance(other, cls):
+            return False
+        elif len(other) > 0 and other.inferred_type not in (
+            ""floating"",
+            ""mixed-integer"",
+            ""integer"",
+            ""integer-na"",
+            ""mixed-integer-float"",
+            ""mixed"",
+        ):
+            return True
+        return False
 
-        result = nanops.nanmax(values, skipna=skipna)
-        # Don't have to worry about NA `result`, since no NA went in.
-        return self._box_func(result)
+    # --------------------------------------------------------------------
+    # List-Like Methods
 
-    def mean(self, skipna=True):
+    def insert(self, loc, item):
         """"""
-        Return the mean value of the Array.
-
-        .. versionadded:: 0.25.0
+        Make new Index inserting new item at location
 
         Parameters
         ----------
-        skipna : bool, default True
-            Whether to ignore any NaT elements.
+        loc : int
+        item : object
+            if not either a Python datetime or a numpy integer-like, returned
+            Index dtype will be object rather than datetime.
 
         Returns
         -------
-        scalar
-            Timestamp or Timedelta.
-
-        See Also
-        --------
-        numpy.ndarray.mean : Returns the average of array elements along a given axis.
-        Series.mean : Return the mean value in a Series.
-
-        Notes
-        -----
-        mean is only defined for Datetime and Timedelta dtypes, not for Period.
-        """"""
-        if is_period_dtype(self):
-            # See discussion in GH#24757
-            raise TypeError(
-                f""mean is not implemented for {type(self).__name__} since the ""
-                ""meaning is ambiguous.  An alternative is ""
-                ""obj.to_timestamp(how='start').mean()""
-            )
-
-        mask = self.isna()
-        if skipna:
-            values = self[~mask]
-        elif mask.any():
-            return NaT
-        else:
-            values = self
-
-        if not len(values):
-            # short-circuit for empty max / min
-            return NaT
-
-        result = nanops.nanmean(values.view(""i8""), skipna=skipna)
-        # Don't have to worry about NA `result`, since no NA went in.
-        return self._box_func(result)
-
-
-DatetimeLikeArrayMixin._add_comparison_ops()
-
-# -------------------------------------------------------------------
-# Shared Constructor Helpers
-
-
-def validate_periods(periods):
-    """"""
-    If a `periods` argument is passed to the Datetime/Timedelta Array/Index
-    constructor, cast it to an integer.
-
-    Parameters
-    ----------
-    periods : None, float, int
-
-    Returns
-    -------
-    periods : None or int
-
-    Raises
-    ------
-    TypeError
-        if periods is None, float, or int
-    """"""
-    if periods is not None:
-        if lib.is_float(periods):
-            periods = int(periods)
-        elif not lib.is_integer(periods):
-            raise TypeError(f""periods must be a number, got {periods}"")
-    return periods
-
-
-def validate_endpoints(closed):
-    """"""
-    Check that the `closed` argument is among [None, ""left"", ""right""]
-
-    Parameters
-    ----------
-    closed : {None, ""left"", ""right""}
-
-    Returns
-    -------
-    left_closed : bool
-    right_closed : bool
-
-    Raises
-    ------
-    ValueError : if argument is not among valid values
-    """"""
-    left_closed = False
-    right_closed = False
-
-    if closed is None:
-        left_closed = True
-        right_closed = True
-    elif closed == ""left"":
-        left_closed = True
-    elif closed == ""right"":
-        right_closed = True
-    else:
-        raise ValueError(""Closed has to be either 'left', 'right' or None"")
-
-    return left_closed, right_closed
-
-
-def validate_inferred_freq(freq, inferred_freq, freq_infer):
-    """"""
-    If the user passes a freq and another freq is inferred from passed data,
-    require that they match.
-
-    Parameters
-    ----------
-    freq : DateOffset or None
-    inferred_freq : DateOffset or None
-    freq_infer : bool
-
-    Returns
-    -------
-    freq : DateOffset or None
-    freq_infer : bool
-
-    Notes
-    -----
-    We assume at this point that `maybe_infer_freq` has been called, so
-    `freq` is either a DateOffset object or None.
-    """"""
-    if inferred_freq is not None:
-        if freq is not None and freq != inferred_freq:
-            raise ValueError(
-                f""Inferred frequency {inferred_freq} from passed ""
-                ""values does not conform to passed frequency ""
-                f""{freq.freqstr}""
-            )
-        elif freq is None:
-            freq = inferred_freq
-        freq_infer = False
-
-    return freq, freq_infer
+        new_index : Index
+        """"""
+        if isinstance(item, str):
+            # TODO: Why are strings special?
+            # TODO: Should we attempt _scalar_from_string?
+            return self.astype(object).insert(loc, item)
+
+        item = self._data._validate_insert_value(item)
+
+        freq = None
+        # check freq can be preserved on edge cases
+        if self.freq is not None:
+            if self.size:
+                if item is NaT:
+                    pass
+                elif (loc == 0 or loc == -len(self)) and item + self.freq == self[0]:
+                    freq = self.freq
+                elif (loc == len(self)) and item - self.freq == self[-1]:
+                    freq = self.freq
+            else:
+                # Adding a single item to an empty index may preserve freq
+                if self.freq.is_on_offset(item):
+                    freq = self.freq
 
+        item = self._data._unbox_scalar(item)
 
-def maybe_infer_freq(freq):
-    """"""
-    Comparing a DateOffset to the string ""infer"" raises, so we need to
-    be careful about comparisons.  Make a dummy variable `freq_infer` to
-    signify the case where the given freq is ""infer"" and set freq to None
-    to avoid comparison trouble later on.
-
-    Parameters
-    ----------
-    freq : {DateOffset, None, str}
-
-    Returns
-    -------
-    freq : {DateOffset, None}
-    freq_infer : bool
-        Whether we should inherit the freq of passed data.
-    """"""
-    freq_infer = False
-    if not isinstance(freq, DateOffset):
-        # if a passed freq is None, don't infer automatically
-        if freq != ""infer"":
-            freq = frequencies.to_offset(freq)
-        else:
-            freq_infer = True
-            freq = None
-    return freq, freq_infer
+        new_i8s = np.concatenate([self[:loc].asi8, [item], self[loc:].asi8])
+        arr = type(self._data)._simple_new(new_i8s, dtype=self.dtype, freq=freq)
+        return type(self)._simple_new(arr, name=self.name)
diff --git a/pandas/tests/indexing/test_coercion.py b/pandas/tests/indexing/test_coercion.py
index 1e362827e..5b73118cf 100644
--- a/pandas/tests/indexing/test_coercion.py
+++ b/pandas/tests/indexing/test_coercion.py
@@ -448,7 +448,7 @@ class TestInsertIndexCoercion(CoercionBase):
             with pytest.raises(TypeError, match=msg):
                 obj.insert(1, pd.Timestamp(""2012-01-01"", tz=""Asia/Tokyo""))
 
-        msg = ""cannot insert DatetimeIndex with incompatible label""
+        msg = ""cannot insert DatetimeArray with incompatible label""
         with pytest.raises(TypeError, match=msg):
             obj.insert(1, 1)
 
@@ -465,12 +465,12 @@ class TestInsertIndexCoercion(CoercionBase):
         )
 
         # ToDo: must coerce to object
-        msg = ""cannot insert TimedeltaIndex with incompatible label""
+        msg = ""cannot insert TimedeltaArray with incompatible label""
         with pytest.raises(TypeError, match=msg):
             obj.insert(1, pd.Timestamp(""2012-01-01""))
 
         # ToDo: must coerce to object
-        msg = ""cannot insert TimedeltaIndex with incompatible label""
+        msg = ""cannot insert TimedeltaArray with incompatible label""
         with pytest.raises(TypeError, match=msg):
             obj.insert(1, 1)
 
diff --git a/pandas/tests/indexing/test_partial.py b/pandas/tests/indexing/test_partial.py
index 6c259db33..ee87a3f9f 100644
--- a/pandas/tests/indexing/test_partial.py
+++ b/pandas/tests/indexing/test_partial.py
@@ -335,7 +335,7 @@ class TestPartialSetting:
         df = orig.copy()
 
         # don't allow not string inserts
-        msg = ""cannot insert DatetimeIndex with incompatible label""
+        msg = ""cannot insert DatetimeArray with incompatible label""
 
         with pytest.raises(TypeError, match=msg):
             df.loc[100.0, :] = df.iloc[0]
"
"pandas","116","9333e3d","c4fa6a52f7737aecda08f6b0f2d6c27476298ae1","pandas/core/reshape/merge.py","pandas/core/reshape/merge.py","diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py","pandas/tests/reshape/merge/test_merge_asof.py","","diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py
index fdd31b3b7..d671fff56 100644
--- a/pandas/core/reshape/merge.py
+++ b/pandas/core/reshape/merge.py
@@ -1027,7 +1027,7 @@ class _MergeOperation:
                     )
                 ]
             else:
-                left_keys = [self.left.index.values]
+                left_keys = [self.left.index._values]
 
         if left_drop:
             self.left = self.left._drop_labels_or_levels(left_drop)
"
"pandas","100","8806ed7","2b1b3da4c68fdaf9637d12706c5ba3de1a9b20de","pandas/core/generic.py","pandas/core/generic.py","diff --git a/pandas/core/generic.py b/pandas/core/generic.py","pandas/tests/frame/methods/test_pct_change.py","","diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 85bbf9b55..e54de78db 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -9857,11 +9857,11 @@ class NDFrame(PandasObject, SelectionMixin):
             data = self.fillna(method=fill_method, limit=limit, axis=axis)
 
         rs = data.div(data.shift(periods=periods, freq=freq, axis=axis, **kwargs)) - 1
-        rs = rs.loc[~rs.index.duplicated()]
-        rs = rs.reindex_like(data)
-        if freq is None:
-            mask = isna(com.values_from_object(data))
-            np.putmask(rs.values, mask, np.nan)
+        if freq is not None:
+            # Shift method is implemented differently when freq is not None
+            # We want to restore the original index
+            rs = rs.loc[~rs.index.duplicated()]
+            rs = rs.reindex_like(data)
         return rs
 
     def _agg_by_level(self, name, axis=0, level=0, skipna=True, **kwargs):
"
"pandas","66","f5409cb","d84f9eb32aea33a8f790e8e365cf226eddd5c7a7","pandas/core/generic.py;pandas/core/internals/managers.py","pandas/core/generic.py;pandas/core/internals/managers.py","diff --git a/pandas/core/generic.py b/pandas/core/generic.py;diff --git a/pandas/core/internals/managers.py b/pandas/core/internals/managers.py","pandas/tests/series/indexing/test_xs.py","","diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 3bb584d4d..62a4878f1 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -3442,15 +3442,14 @@ class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):
                 new_index = self.index[loc]
 
         if is_scalar(loc):
-            new_values = self._data.fast_xs(loc)
+            # In this case loc should be an integer
+            if self.ndim == 1:
+                # if we encounter an array-like and we only have 1 dim
+                # that means that their are list/ndarrays inside the Series!
+                # so just return them (GH 6394)
+                return self._values[loc]
 
-            # may need to box a datelike-scalar
-            #
-            # if we encounter an array-like and we only have 1 dim
-            # that means that their are list/ndarrays inside the Series!
-            # so just return them (GH 6394)
-            if not is_list_like(new_values) or self.ndim == 1:
-                return com.maybe_box_datetimelike(new_values)
+            new_values = self._data.fast_xs(loc)
 
             result = self._constructor_sliced(
                 new_values,
diff --git a/pandas/core/internals/managers.py b/pandas/core/internals/managers.py
index 526863d2e..08ae0b021 100644
--- a/pandas/core/internals/managers.py
+++ b/pandas/core/internals/managers.py
@@ -1565,7 +1565,7 @@ class SingleBlockManager(BlockManager):
         fast path for getting a cross-section
         return a view of the data
         """"""
-        return self._block.values[loc]
+        raise NotImplementedError(""Use series._values[loc] instead"")
 
     def concat(self, to_concat, new_axis) -> ""SingleBlockManager"":
         """"""
"
"pandas","57","267d2d8","84444538a88721c5ee74de8836b716d3c1adc4b8","pandas/_testing.py;pandas/core/arrays/categorical.py","pandas/_testing.py;pandas/core/arrays/categorical.py","diff --git a/pandas/_testing.py b/pandas/_testing.py;diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py","pandas/tests/arrays/categorical/test_replace.py","","diff --git a/pandas/_testing.py b/pandas/_testing.py
index b19905f1c..7ebf2c282 100644
--- a/pandas/_testing.py
+++ b/pandas/_testing.py
@@ -1074,6 +1074,7 @@ def assert_series_equal(
     check_exact=False,
     check_datetimelike_compat=False,
     check_categorical=True,
+    check_category_order=True,
     obj=""Series"",
 ):
     """"""
@@ -1108,6 +1109,10 @@ def assert_series_equal(
         Compare datetime-like which is comparable ignoring dtype.
     check_categorical : bool, default True
         Whether to compare internal Categorical exactly.
+    check_category_order : bool, default True
+        Whether to compare category order of internal Categoricals
+
+        .. versionadded:: 1.0.2
     obj : str, default 'Series'
         Specify object name being compared, internally used to show appropriate
         assertion message.
@@ -1210,7 +1215,12 @@ def assert_series_equal(
 
     if check_categorical:
         if is_categorical_dtype(left) or is_categorical_dtype(right):
-            assert_categorical_equal(left.values, right.values, obj=f""{obj} category"")
+            assert_categorical_equal(
+                left.values,
+                right.values,
+                obj=f""{obj} category"",
+                check_category_order=check_category_order,
+            )
 
 
 # This could be refactored to use the NDFrame.equals method
diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index 19602010f..d469b5748 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -2441,18 +2441,30 @@ class Categorical(ExtensionArray, PandasObject):
         """"""
         inplace = validate_bool_kwarg(inplace, ""inplace"")
         cat = self if inplace else self.copy()
-        if to_replace in cat.categories:
-            if isna(value):
-                cat.remove_categories(to_replace, inplace=True)
-            else:
+
+        # build a dict of (to replace -> value) pairs
+        if is_list_like(to_replace):
+            # if to_replace is list-like and value is scalar
+            replace_dict = {replace_value: value for replace_value in to_replace}
+        else:
+            # if both to_replace and value are scalar
+            replace_dict = {to_replace: value}
+
+        # other cases, like if both to_replace and value are list-like or if
+        # to_replace is a dict, are handled separately in NDFrame
+        for replace_value, new_value in replace_dict.items():
+            if replace_value in cat.categories:
+                if isna(new_value):
+                    cat.remove_categories(replace_value, inplace=True)
+                    continue
                 categories = cat.categories.tolist()
-                index = categories.index(to_replace)
-                if value in cat.categories:
-                    value_index = categories.index(value)
+                index = categories.index(replace_value)
+                if new_value in cat.categories:
+                    value_index = categories.index(new_value)
                     cat._codes[cat._codes == index] = value_index
-                    cat.remove_categories(to_replace, inplace=True)
+                    cat.remove_categories(replace_value, inplace=True)
                 else:
-                    categories[index] = value
+                    categories[index] = new_value
                     cat.rename_categories(categories, inplace=True)
         if not inplace:
             return cat
"
"pandas","163","61819ab","f669f94a186ea444cc771985a915e90eecf218a9","pandas/core/window.py","pandas/core/window.py","diff --git a/pandas/core/window.py b/pandas/core/window.py","pandas/tests/window/test_rolling.py","","diff --git a/pandas/core/window.py b/pandas/core/window.py
index a7425bc14..3e3f17369 100644
--- a/pandas/core/window.py
+++ b/pandas/core/window.py
@@ -246,8 +246,10 @@ class _Window(PandasObject, SelectionMixin):
             except (ValueError, TypeError):
                 raise TypeError(""cannot handle this type -> {0}"".format(values.dtype))
 
-        # Always convert inf to nan
-        values[np.isinf(values)] = np.NaN
+        # Convert inf to nan for C funcs
+        inf = np.isinf(values)
+        if inf.any():
+            values = np.where(inf, np.nan, values)
 
         return values
 
"
"pandas","121","ad4c4d5","958756af5cb40658e975a70d29089b68aea93040","pandas/core/internals/blocks.py;pandas/core/internals/managers.py","pandas/core/internals/blocks.py;pandas/core/internals/managers.py","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py;diff --git a/pandas/core/internals/managers.py b/pandas/core/internals/managers.py","pandas/tests/frame/test_replace.py","","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index 7ace80415..5edb4d93e 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -2825,6 +2825,8 @@ class ObjectBlock(Block):
             if convert:
                 block = [b.convert(numeric=False, copy=True) for b in block]
             return block
+        if convert:
+            return [self.convert(numeric=False, copy=True)]
         return self
 
 
diff --git a/pandas/core/internals/managers.py b/pandas/core/internals/managers.py
index 0e6ba8a2c..c36dd9463 100644
--- a/pandas/core/internals/managers.py
+++ b/pandas/core/internals/managers.py
@@ -629,7 +629,7 @@ class BlockManager(PandasObject):
                         convert=convert,
                         regex=regex,
                     )
-                    if m.any():
+                    if m.any() or convert:
                         new_rb = _extend_blocks(result, new_rb)
                     else:
                         new_rb.append(b)
"
"pandas","75","2038d7a","9a211aae9f710db23c9113aea0251e2758904755","pandas/core/indexes/period.py","pandas/core/indexes/period.py","diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py","pandas/tests/indexes/period/test_indexing.py","","diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py
index 2a40f4a6f..fe6c1ba80 100644
--- a/pandas/core/indexes/period.py
+++ b/pandas/core/indexes/period.py
@@ -486,7 +486,7 @@ class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):
             try:
                 loc = self._get_string_slice(key)
                 return series[loc]
-            except (TypeError, ValueError):
+            except (TypeError, ValueError, OverflowError):
                 pass
 
             asdt, reso = parse_time_string(key, self.freq)
@@ -567,18 +567,34 @@ class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):
         """"""
 
         if isinstance(key, str):
+
             try:
-                return self._get_string_slice(key)
-            except (TypeError, KeyError, ValueError, OverflowError):
+                loc = self._get_string_slice(key)
+                return loc
+            except (TypeError, ValueError):
                 pass
 
             try:
                 asdt, reso = parse_time_string(key, self.freq)
-                key = asdt
             except DateParseError:
                 # A string with invalid format
                 raise KeyError(f""Cannot interpret '{key}' as period"")
 
+            grp = resolution.Resolution.get_freq_group(reso)
+            freqn = resolution.get_freq_group(self.freq)
+
+            # _get_string_slice will handle cases where grp < freqn
+            assert grp >= freqn
+
+            if grp == freqn:
+                key = Period(asdt, freq=self.freq)
+                loc = self.get_loc(key, method=method, tolerance=tolerance)
+                return loc
+            elif method is None:
+                raise KeyError(key)
+            else:
+                key = asdt
+
         elif is_integer(key):
             # Period constructor will cast to string, which we dont want
             raise KeyError(key)
"
"pandas","143","c13c13b","df918becf698741861da0e9b7e810d477b0eb194","pandas/core/indexes/range.py","pandas/core/indexes/range.py","diff --git a/pandas/core/indexes/range.py b/pandas/core/indexes/range.py","pandas/tests/frame/test_indexing.py;pandas/tests/indexes/test_range.py","","diff --git a/pandas/core/indexes/range.py b/pandas/core/indexes/range.py
index 8783351cc..43445a0d5 100644
--- a/pandas/core/indexes/range.py
+++ b/pandas/core/indexes/range.py
@@ -380,8 +380,10 @@ class RangeIndex(Int64Index):
 
     @Appender(_index_shared_docs[""get_indexer""])
     def get_indexer(self, target, method=None, limit=None, tolerance=None):
-        if not (method is None and tolerance is None and is_list_like(target)):
-            return super().get_indexer(target, method=method, tolerance=tolerance)
+        if com.any_not_none(method, tolerance, limit) or not is_list_like(target):
+            return super().get_indexer(
+                target, method=method, tolerance=tolerance, limit=limit
+            )
 
         if self.step > 0:
             start, stop, step = self.start, self.stop, self.step
"
"pandas","145","3bd222d","f08a1e62e31fc11e7e5bd7bec72b7e6d86473423","pandas/core/ops/__init__.py","pandas/core/ops/__init__.py","diff --git a/pandas/core/ops/__init__.py b/pandas/core/ops/__init__.py","pandas/tests/frame/test_arithmetic.py","","diff --git a/pandas/core/ops/__init__.py b/pandas/core/ops/__init__.py
index ca4f35514..f53b5045a 100644
--- a/pandas/core/ops/__init__.py
+++ b/pandas/core/ops/__init__.py
@@ -498,8 +498,19 @@ def dispatch_to_series(left, right, func, str_rep=None, axis=None):
         # in which case we specifically want to operate row-by-row
         assert right.index.equals(left.columns)
 
-        def column_op(a, b):
-            return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}
+        if right.dtype == ""timedelta64[ns]"":
+            # ensure we treat NaT values as the correct dtype
+            # Note: we do not do this unconditionally as it may be lossy or
+            #  expensive for EA dtypes.
+            right = np.asarray(right)
+
+            def column_op(a, b):
+                return {i: func(a.iloc[:, i], b[i]) for i in range(len(a.columns))}
+
+        else:
+
+            def column_op(a, b):
+                return {i: func(a.iloc[:, i], b.iloc[i]) for i in range(len(a.columns))}
 
     elif isinstance(right, ABCSeries):
         assert right.index.equals(left.index)  # Handle other cases later
"
"pandas","112","ebbe2a2","8a354b7630f74739212725c38cbaa9b069191a88","pandas/core/indexes/interval.py;pandas/tests/indexes/interval/test_indexing.py","pandas/core/indexes/interval.py;pandas/tests/indexes/interval/test_indexing.py","diff --git a/pandas/core/indexes/interval.py b/pandas/core/indexes/interval.py;diff --git a/pandas/tests/indexes/interval/test_indexing.py b/pandas/tests/indexes/interval/test_indexing.py","pandas/tests/frame/test_analytics.py","","diff --git a/pandas/core/indexes/interval.py b/pandas/core/indexes/interval.py
index 2555caac2..b1f67eeab 100644
--- a/pandas/core/indexes/interval.py
+++ b/pandas/core/indexes/interval.py
@@ -19,6 +19,7 @@ from pandas.core.dtypes.cast import (
 )
 from pandas.core.dtypes.common import (
     ensure_platform_int,
+    is_categorical,
     is_datetime64tz_dtype,
     is_datetime_or_timedelta_dtype,
     is_dtype_equal,
@@ -36,6 +37,7 @@ from pandas.core.dtypes.generic import ABCSeries
 from pandas.core.dtypes.missing import isna
 
 from pandas._typing import AnyArrayLike
+from pandas.core.algorithms import take_1d
 from pandas.core.arrays.interval import IntervalArray, _interval_shared_docs
 import pandas.core.common as com
 import pandas.core.indexes.base as ibase
@@ -958,6 +960,10 @@ class IntervalIndex(IntervalMixin, Index):
             left_indexer = self.left.get_indexer(target_as_index.left)
             right_indexer = self.right.get_indexer(target_as_index.right)
             indexer = np.where(left_indexer == right_indexer, left_indexer, -1)
+        elif is_categorical(target_as_index):
+            # get an indexer for unique categories then propogate to codes via take_1d
+            categories_indexer = self.get_indexer(target_as_index.categories)
+            indexer = take_1d(categories_indexer, target_as_index.codes, fill_value=-1)
         elif not is_object_dtype(target_as_index):
             # homogeneous scalar index: use IntervalTree
             target_as_index = self._maybe_convert_i8(target_as_index)
diff --git a/pandas/tests/indexes/interval/test_indexing.py b/pandas/tests/indexes/interval/test_indexing.py
index 05d8aee2a..15ea9a6b6 100644
--- a/pandas/tests/indexes/interval/test_indexing.py
+++ b/pandas/tests/indexes/interval/test_indexing.py
@@ -3,7 +3,14 @@ import re
 import numpy as np
 import pytest
 
-from pandas import Interval, IntervalIndex, Timedelta, date_range, timedelta_range
+from pandas import (
+    CategoricalIndex,
+    Interval,
+    IntervalIndex,
+    Timedelta,
+    date_range,
+    timedelta_range,
+)
 from pandas.core.indexes.base import InvalidIndexError
 import pandas.util.testing as tm
 
@@ -231,6 +238,25 @@ class TestGetIndexer:
         expected = np.array([0] * size, dtype=""intp"")
         tm.assert_numpy_array_equal(result, expected)
 
+    @pytest.mark.parametrize(
+        ""target"",
+        [
+            IntervalIndex.from_tuples([(7, 8), (1, 2), (3, 4), (0, 1)]),
+            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4), np.nan]),
+            IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)], closed=""both""),
+            [-1, 0, 0.5, 1, 2, 2.5, np.nan],
+            [""foo"", ""foo"", ""bar"", ""baz""],
+        ],
+    )
+    def test_get_indexer_categorical(self, target, ordered_fixture):
+        # GH 30063: categorical and non-categorical results should be consistent
+        index = IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)])
+        categorical_target = CategoricalIndex(target, ordered=ordered_fixture)
+
+        result = index.get_indexer(categorical_target)
+        expected = index.get_indexer(target)
+        tm.assert_numpy_array_equal(result, expected)
+
     @pytest.mark.parametrize(
         ""tuples, closed"",
         [
"
"pandas","28","40fd73a","ef9b9387c88cf12b20dd8656dfedfc236e0f3352","pandas/core/strings.py","pandas/core/strings.py","diff --git a/pandas/core/strings.py b/pandas/core/strings.py","pandas/tests/test_strings.py","","diff --git a/pandas/core/strings.py b/pandas/core/strings.py
index 52d9a8148..76b851d8a 100644
--- a/pandas/core/strings.py
+++ b/pandas/core/strings.py
@@ -2297,7 +2297,7 @@ class StringMethods(NoNewAttributesMixin):
         if isinstance(others, ABCSeries):
             return [others]
         elif isinstance(others, ABCIndexClass):
-            return [Series(others._values, index=others)]
+            return [Series(others._values, index=idx)]
         elif isinstance(others, ABCDataFrame):
             return [others[x] for x in others]
         elif isinstance(others, np.ndarray) and others.ndim == 2:
"
"pandas","154","3f5b5c4","e0c63b4cfaa821dfe310f4a8a1f84929ced5f5bd","pandas/core/groupby/groupby.py","pandas/core/groupby/groupby.py","diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py","pandas/tests/groupby/test_groupby.py","","diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py
index 55def024c..e010e615e 100644
--- a/pandas/core/groupby/groupby.py
+++ b/pandas/core/groupby/groupby.py
@@ -2263,26 +2263,28 @@ class GroupBy(_GroupBy):
         base_func = getattr(libgroupby, how)
 
         for name, obj in self._iterate_slices():
+            values = obj._data._values
+
             if aggregate:
                 result_sz = ngroups
             else:
-                result_sz = len(obj.values)
+                result_sz = len(values)
 
             if not cython_dtype:
-                cython_dtype = obj.values.dtype
+                cython_dtype = values.dtype
 
             result = np.zeros(result_sz, dtype=cython_dtype)
             func = partial(base_func, result, labels)
             inferences = None
 
             if needs_values:
-                vals = obj.values
+                vals = values
                 if pre_processing:
                     vals, inferences = pre_processing(vals)
                 func = partial(func, vals)
 
             if needs_mask:
-                mask = isna(obj.values).view(np.uint8)
+                mask = isna(values).view(np.uint8)
                 func = partial(func, mask)
 
             if needs_ngroups:
@@ -2291,7 +2293,7 @@ class GroupBy(_GroupBy):
             func(**kwargs)  # Call func to modify indexer values in place
 
             if result_is_index:
-                result = algorithms.take_nd(obj.values, result)
+                result = algorithms.take_nd(values, result)
 
             if post_processing:
                 result = post_processing(result, inferences)
"
"pandas","146","5ebb1e4","74cba561ece511e24abb5145225bf98a929ca6c9","pandas/core/dtypes/missing.py;pandas/core/indexes/base.py","pandas/core/dtypes/missing.py;pandas/core/indexes/base.py","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py;diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py","pandas/tests/dtypes/test_missing.py","","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py
index 6dd032b92..cd87fbef0 100644
--- a/pandas/core/dtypes/missing.py
+++ b/pandas/core/dtypes/missing.py
@@ -445,8 +445,14 @@ def array_equivalent(left, right, strict_nan=False):
                 if not isinstance(right_value, float) or not np.isnan(right_value):
                     return False
             else:
-                if np.any(left_value != right_value):
-                    return False
+                try:
+                    if np.any(left_value != right_value):
+                        return False
+                except TypeError as err:
+                    if ""Cannot compare tz-naive"" in str(err):
+                        # tzawareness compat failure, see GH#28507
+                        return False
+                    raise
         return True
 
     # NaNs can occur in float and complex arrays.
diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index 6ef9d78ff..c7e9dd5f0 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -4324,12 +4324,9 @@ class Index(IndexOpsMixin, PandasObject):
             # if other is not object, use other's logic for coercion
             return other.equals(self)
 
-        try:
-            return array_equivalent(
-                com.values_from_object(self), com.values_from_object(other)
-            )
-        except Exception:
-            return False
+        return array_equivalent(
+            com.values_from_object(self), com.values_from_object(other)
+        )
 
     def identical(self, other):
         """"""
"
"pandas","64","31c1856","d0c84ce57d23a409169daf7232ec7681e42363fe","pandas/io/formats/excel.py","pandas/io/formats/excel.py","diff --git a/pandas/io/formats/excel.py b/pandas/io/formats/excel.py","pandas/tests/io/excel/test_writers.py","","diff --git a/pandas/io/formats/excel.py b/pandas/io/formats/excel.py
index 14e795385..28a069bc9 100644
--- a/pandas/io/formats/excel.py
+++ b/pandas/io/formats/excel.py
@@ -403,7 +403,7 @@ class ExcelFormatter:
                 # Deprecated in GH#17295, enforced in 1.0.0
                 raise KeyError(""Not all names specified in 'columns' are found"")
 
-            self.df = df
+            self.df = df.reindex(columns=cols)
 
         self.columns = self.df.columns
         self.float_format = float_format
"
"pandas","20","ea09d50","92afd5c2c08216e4e9c80eb6b92b1660f91846e0","pandas/tseries/offsets.py","pandas/tseries/offsets.py","diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py","pandas/tests/tseries/offsets/test_yqm_offsets.py","","diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py
index 8ba10f56f..b419d5ccc 100644
--- a/pandas/tseries/offsets.py
+++ b/pandas/tseries/offsets.py
@@ -1147,9 +1147,7 @@ class MonthOffset(SingleConstructorOffset):
     @apply_index_wraps
     def apply_index(self, i):
         shifted = liboffsets.shift_months(i.asi8, self.n, self._day_opt)
-        # TODO: going through __new__ raises on call to _validate_frequency;
-        #  are we passing incorrect freq?
-        return type(i)._simple_new(shifted, freq=i.freq, dtype=i.dtype)
+        return type(i)._simple_new(shifted, dtype=i.dtype)
 
 
 class MonthEnd(MonthOffset):
@@ -1868,11 +1866,7 @@ class QuarterOffset(DateOffset):
         shifted = liboffsets.shift_quarters(
             dtindex.asi8, self.n, self.startingMonth, self._day_opt
         )
-        # TODO: going through __new__ raises on call to _validate_frequency;
-        #  are we passing incorrect freq?
-        return type(dtindex)._simple_new(
-            shifted, freq=dtindex.freq, dtype=dtindex.dtype
-        )
+        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)
 
 
 class BQuarterEnd(QuarterOffset):
@@ -1954,11 +1948,7 @@ class YearOffset(DateOffset):
         shifted = liboffsets.shift_quarters(
             dtindex.asi8, self.n, self.month, self._day_opt, modby=12
         )
-        # TODO: going through __new__ raises on call to _validate_frequency;
-        #  are we passing incorrect freq?
-        return type(dtindex)._simple_new(
-            shifted, freq=dtindex.freq, dtype=dtindex.dtype
-        )
+        return type(dtindex)._simple_new(shifted, dtype=dtindex.dtype)
 
     def is_on_offset(self, dt: datetime) -> bool:
         if self.normalize and not _is_normalized(dt):
"
"pandas","90","a474a01","1c3d64bae7c07b5ae1be337e0ebd751385b7ce27","pandas/_testing.py;pandas/io/pickle.py","pandas/_testing.py;pandas/io/pickle.py","diff --git a/pandas/_testing.py b/pandas/_testing.py;diff --git a/pandas/io/pickle.py b/pandas/io/pickle.py","pandas/tests/io/test_pickle.py","","diff --git a/pandas/_testing.py b/pandas/_testing.py
index 2050a18cb..0b81fb0f7 100644
--- a/pandas/_testing.py
+++ b/pandas/_testing.py
@@ -8,7 +8,7 @@ import os
 from shutil import rmtree
 import string
 import tempfile
-from typing import List, Optional, Union, cast
+from typing import Any, List, Optional, Union, cast
 import warnings
 import zipfile
 
@@ -22,7 +22,7 @@ from pandas._config.localization import (  # noqa:F401
 )
 
 import pandas._libs.testing as _testing
-from pandas._typing import FrameOrSeries
+from pandas._typing import FilePathOrBuffer, FrameOrSeries
 from pandas.compat import _get_lzma_file, _import_lzma
 
 from pandas.core.dtypes.common import (
@@ -101,15 +101,17 @@ def reset_display_options():
     pd.reset_option(""^display."", silent=True)
 
 
-def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOrSeries:
+def round_trip_pickle(
+    obj: Any, path: Optional[FilePathOrBuffer] = None
+) -> FrameOrSeries:
     """"""
     Pickle an object and then read it again.
 
     Parameters
     ----------
-    obj : pandas object
+    obj : any object
         The object to pickle and then re-read.
-    path : str, default None
+    path : str, path object or file-like object, default None
         The path where the pickled object is written and then read.
 
     Returns
@@ -117,11 +119,12 @@ def round_trip_pickle(obj: FrameOrSeries, path: Optional[str] = None) -> FrameOr
     pandas object
         The original object that was pickled and then re-read.
     """"""
-    if path is None:
-        path = f""__{rands(10)}__.pickle""
-    with ensure_clean(path) as path:
-        pd.to_pickle(obj, path)
-        return pd.read_pickle(path)
+    _path = path
+    if _path is None:
+        _path = f""__{rands(10)}__.pickle""
+    with ensure_clean(_path) as path:
+        pd.to_pickle(obj, _path)
+        return pd.read_pickle(_path)
 
 
 def round_trip_pathlib(writer, reader, path: Optional[str] = None):
diff --git a/pandas/io/pickle.py b/pandas/io/pickle.py
index 6ce52da21..e51f24b55 100644
--- a/pandas/io/pickle.py
+++ b/pandas/io/pickle.py
@@ -1,13 +1,20 @@
 """""" pickle compat """"""
 import pickle
+from typing import Any, Optional
 import warnings
 
+from pandas._typing import FilePathOrBuffer
 from pandas.compat import pickle_compat as pc
 
-from pandas.io.common import get_handle, stringify_path
+from pandas.io.common import get_filepath_or_buffer, get_handle
 
 
-def to_pickle(obj, path, compression=""infer"", protocol=pickle.HIGHEST_PROTOCOL):
+def to_pickle(
+    obj: Any,
+    filepath_or_buffer: FilePathOrBuffer,
+    compression: Optional[str] = ""infer"",
+    protocol: int = pickle.HIGHEST_PROTOCOL,
+):
     """"""
     Pickle (serialize) object to file.
 
@@ -15,11 +22,17 @@ def to_pickle(obj, path, compression=""infer"", protocol=pickle.HIGHEST_PROTOCOL):
     ----------
     obj : any object
         Any python object.
-    path : str
-        File path where the pickled object will be stored.
+    filepath_or_buffer : str, path object or file-like object
+        File path, URL, or buffer where the pickled object will be stored.
+
+        .. versionchanged:: 1.0.0
+           Accept URL. URL has to be of S3 or GCS.
+
     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'
-        A string representing the compression to use in the output file. By
-        default, infers from the file extension in specified path.
+        If 'infer' and 'path_or_url' is path-like, then detect compression from
+        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no
+        compression) If 'infer' and 'path_or_url' is not path-like, then use
+        None (= no decompression).
     protocol : int
         Int which indicates which protocol should be used by the pickler,
         default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible
@@ -63,8 +76,12 @@ def to_pickle(obj, path, compression=""infer"", protocol=pickle.HIGHEST_PROTOCOL):
     >>> import os
     >>> os.remove(""./dummy.pkl"")
     """"""
-    path = stringify_path(path)
-    f, fh = get_handle(path, ""wb"", compression=compression, is_text=False)
+    fp_or_buf, _, compression, should_close = get_filepath_or_buffer(
+        filepath_or_buffer, compression=compression, mode=""wb""
+    )
+    if not isinstance(fp_or_buf, str) and compression == ""infer"":
+        compression = None
+    f, fh = get_handle(fp_or_buf, ""wb"", compression=compression, is_text=False)
     if protocol < 0:
         protocol = pickle.HIGHEST_PROTOCOL
     try:
@@ -73,9 +90,16 @@ def to_pickle(obj, path, compression=""infer"", protocol=pickle.HIGHEST_PROTOCOL):
         f.close()
         for _f in fh:
             _f.close()
+        if should_close:
+            try:
+                fp_or_buf.close()
+            except ValueError:
+                pass
 
 
-def read_pickle(path, compression=""infer""):
+def read_pickle(
+    filepath_or_buffer: FilePathOrBuffer, compression: Optional[str] = ""infer""
+):
     """"""
     Load pickled pandas object (or any object) from file.
 
@@ -86,13 +110,17 @@ def read_pickle(path, compression=""infer""):
 
     Parameters
     ----------
-    path : str
-        File path where the pickled object will be loaded.
+    filepath_or_buffer : str, path object or file-like object
+        File path, URL, or buffer where the pickled object will be loaded from.
+
+        .. versionchanged:: 1.0.0
+           Accept URL. URL is not limited to S3 and GCS.
+
     compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'
-        For on-the-fly decompression of on-disk data. If 'infer', then use
-        gzip, bz2, xz or zip if path ends in '.gz', '.bz2', '.xz',
-        or '.zip' respectively, and no decompression otherwise.
-        Set to None for no decompression.
+        If 'infer' and 'path_or_url' is path-like, then detect compression from
+        the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no
+        compression) If 'infer' and 'path_or_url' is not path-like, then use
+        None (= no decompression).
 
     Returns
     -------
@@ -134,8 +162,12 @@ def read_pickle(path, compression=""infer""):
     >>> import os
     >>> os.remove(""./dummy.pkl"")
     """"""
-    path = stringify_path(path)
-    f, fh = get_handle(path, ""rb"", compression=compression, is_text=False)
+    fp_or_buf, _, compression, should_close = get_filepath_or_buffer(
+        filepath_or_buffer, compression=compression
+    )
+    if not isinstance(fp_or_buf, str) and compression == ""infer"":
+        compression = None
+    f, fh = get_handle(fp_or_buf, ""rb"", compression=compression, is_text=False)
 
     # 1) try standard library Pickle
     # 2) try pickle_compat (older pandas version) to handle subclass changes
@@ -159,3 +191,8 @@ def read_pickle(path, compression=""infer""):
         f.close()
         for _f in fh:
             _f.close()
+        if should_close:
+            try:
+                fp_or_buf.close()
+            except ValueError:
+                pass
"
"pandas","37","c69f7d8","845c50c9e2ce611c773422ae9db7097fc3e5fba5","pandas/core/arrays/string_.py","pandas/core/arrays/string_.py","diff --git a/pandas/core/arrays/string_.py b/pandas/core/arrays/string_.py","pandas/tests/arrays/string_/test_string.py","","diff --git a/pandas/core/arrays/string_.py b/pandas/core/arrays/string_.py
index f82790ac4..dbca8e74f 100644
--- a/pandas/core/arrays/string_.py
+++ b/pandas/core/arrays/string_.py
@@ -13,7 +13,8 @@ from pandas.core.dtypes.inference import is_array_like
 
 from pandas import compat
 from pandas.core import ops
-from pandas.core.arrays import PandasArray
+from pandas.core.arrays import IntegerArray, PandasArray
+from pandas.core.arrays.integer import _IntegerDtype
 from pandas.core.construction import extract_array
 from pandas.core.indexers import check_array_indexer
 from pandas.core.missing import isna
@@ -271,6 +272,13 @@ class StringArray(PandasArray):
             if copy:
                 return self.copy()
             return self
+        elif isinstance(dtype, _IntegerDtype):
+            arr = self._ndarray.copy()
+            mask = self.isna()
+            arr[mask] = 0
+            values = arr.astype(dtype.numpy_dtype)
+            return IntegerArray(values, mask, copy=False)
+
         return super().astype(dtype, copy)
 
     def _reduce(self, name, skipna=True, **kwargs):
"
"pandas","27","6658d89","13dc13f12c0fca943979cde065b7484bb0e40d66","pandas/core/arrays/datetimes.py","pandas/core/arrays/datetimes.py","diff --git a/pandas/core/arrays/datetimes.py b/pandas/core/arrays/datetimes.py","pandas/tests/indexes/datetimes/test_to_period.py","","diff --git a/pandas/core/arrays/datetimes.py b/pandas/core/arrays/datetimes.py
index d6af11a44..2ccc0ff2f 100644
--- a/pandas/core/arrays/datetimes.py
+++ b/pandas/core/arrays/datetimes.py
@@ -18,6 +18,7 @@ from pandas._libs.tslibs import (
     timezones,
     tzconversion,
 )
+import pandas._libs.tslibs.frequencies as libfrequencies
 from pandas.errors import PerformanceWarning
 
 from pandas.core.dtypes.common import (
@@ -1097,7 +1098,14 @@ default 'raise'
                     ""You must pass a freq argument as current index has none.""
                 )
 
-            freq = get_period_alias(freq)
+            res = get_period_alias(freq)
+
+            #  https://github.com/pandas-dev/pandas/issues/33358
+            if res is None:
+                base, stride = libfrequencies._base_and_stride(freq)
+                res = f""{stride}{base}""
+
+            freq = res
 
         return PeriodArray._from_datetime64(self._data, freq, tz=self.tz)
 
"
"pandas","56","9e69040","dafec63f2e138d0451dae5b37edea2e83f9adc8a","pandas/core/frame.py","pandas/core/frame.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py","pandas/tests/indexing/test_scalar.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 9fe1ec7b7..b3da22d10 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -2622,8 +2622,8 @@ class DataFrame(NDFrame):
         scalar
         """"""
         if takeable:
-            series = self._iget_item_cache(col)
-            return com.maybe_box_datetimelike(series._values[index])
+            series = self._ixs(col, axis=1)
+            return series._values[index]
 
         series = self._get_item_cache(col)
         engine = self.index._engine
"
"pandas","78","f5aa542","bd6b395a1e8fb7d099fa17a0e24f8fe3b628822c","pandas/core/frame.py","pandas/core/frame.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py","pandas/tests/frame/test_subclass.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 3b69dae78..439f78969 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -8013,7 +8013,7 @@ Wild         185.0
                     result = coerce_to_dtypes(result, self.dtypes)
 
         if constructor is not None:
-            result = Series(result, index=labels)
+            result = self._constructor_sliced(result, index=labels)
         return result
 
     def nunique(self, axis=0, dropna=True) -> Series:
"
"pandas","132","221c8a7","bd8f07fb29d2ac819f4c8e8e1b8e6d40f8b0f40c","pandas/core/nanops.py;pandas/core/series.py","pandas/core/nanops.py;pandas/core/series.py","diff --git a/pandas/core/nanops.py b/pandas/core/nanops.py;diff --git a/pandas/core/series.py b/pandas/core/series.py","pandas/tests/reductions/test_reductions.py","","diff --git a/pandas/core/nanops.py b/pandas/core/nanops.py
index b9267db76..f6555f390 100644
--- a/pandas/core/nanops.py
+++ b/pandas/core/nanops.py
@@ -705,11 +705,14 @@ def nanstd(values, axis=None, skipna=True, ddof=1, mask=None):
     >>> nanops.nanstd(s)
     1.0
     """"""
+    orig_dtype = values.dtype
+    values, mask, dtype, dtype_max, fill_value = _get_values(values, skipna, mask=mask)
+
     result = np.sqrt(nanvar(values, axis=axis, skipna=skipna, ddof=ddof, mask=mask))
-    return _wrap_results(result, values.dtype)
+    return _wrap_results(result, orig_dtype)
 
 
-@disallow(""M8"")
+@disallow(""M8"", ""m8"")
 @bottleneck_switch(ddof=1)
 def nanvar(values, axis=None, skipna=True, ddof=1, mask=None):
     """"""
diff --git a/pandas/core/series.py b/pandas/core/series.py
index 3e9d3d5c0..4c7fb4d41 100644
--- a/pandas/core/series.py
+++ b/pandas/core/series.py
@@ -3988,6 +3988,7 @@ class Series(base.IndexOpsMixin, generic.NDFrame):
         If we have an ndarray as a value, then simply perform the operation,
         otherwise delegate to the object.
         """"""
+
         delegate = self._values
 
         if axis is not None:
"
"pandas","45","74c5306","74f6579941fbe71cf7c033f53977047ac872e469","pandas/core/construction.py","pandas/core/construction.py","diff --git a/pandas/core/construction.py b/pandas/core/construction.py","pandas/tests/frame/test_constructors.py","","diff --git a/pandas/core/construction.py b/pandas/core/construction.py
index e2d8fba8d..c9754ff58 100644
--- a/pandas/core/construction.py
+++ b/pandas/core/construction.py
@@ -5,6 +5,7 @@ and Index.__new__.
 These should not depend on core.internals.
 """"""
 
+from collections import abc
 from typing import TYPE_CHECKING, Any, Optional, Sequence, Union, cast
 
 import numpy as np
@@ -446,6 +447,8 @@ def sanitize_array(
         # GH#16804
         arr = np.arange(data.start, data.stop, data.step, dtype=""int64"")
         subarr = _try_cast(arr, dtype, copy, raise_cast_failure)
+    elif isinstance(data, abc.Set):
+        raise TypeError(""Set type is unordered"")
     else:
         subarr = _try_cast(data, dtype, copy, raise_cast_failure)
 
"
"pandas","76","4da554f","47922d3b00edfc264f73b1484589734bbd077c11","pandas/io/json/_json.py","pandas/io/json/_json.py","diff --git a/pandas/io/json/_json.py b/pandas/io/json/_json.py","pandas/tests/io/json/test_pandas.py","","diff --git a/pandas/io/json/_json.py b/pandas/io/json/_json.py
index ae6ae70cb..204807b55 100644
--- a/pandas/io/json/_json.py
+++ b/pandas/io/json/_json.py
@@ -942,7 +942,7 @@ class Parser:
                 if (new_data == data).all():
                     data = new_data
                     result = True
-            except (TypeError, ValueError):
+            except (TypeError, ValueError, OverflowError):
                 pass
 
         # coerce ints to 64
"
"pandas","141","b298696","411dd249e755d7e281603fe3e0ab9e0e48383df9","pandas/core/indexes/range.py","pandas/core/indexes/range.py","diff --git a/pandas/core/indexes/range.py b/pandas/core/indexes/range.py","pandas/tests/indexes/test_range.py","","diff --git a/pandas/core/indexes/range.py b/pandas/core/indexes/range.py
index 43445a0d5..6e2d500f4 100644
--- a/pandas/core/indexes/range.py
+++ b/pandas/core/indexes/range.py
@@ -388,8 +388,9 @@ class RangeIndex(Int64Index):
         if self.step > 0:
             start, stop, step = self.start, self.stop, self.step
         else:
-            # Work on reversed range for simplicity:
-            start, stop, step = (self.stop - self.step, self.start + 1, -self.step)
+            # GH 28678: work on reversed range for simplicity
+            reverse = self._range[::-1]
+            start, stop, step = reverse.start, reverse.stop, reverse.step
 
         target_array = np.asarray(target)
         if not (is_integer_dtype(target_array) and target_array.ndim == 1):
"
"pandas","12","9bd296c","8aa707298428801199280b2b994632080591700a","pandas/core/frame.py","pandas/core/frame.py","diff --git a/pandas/core/frame.py b/pandas/core/frame.py","pandas/tests/frame/methods/test_cov_corr.py","","diff --git a/pandas/core/frame.py b/pandas/core/frame.py
index 5810e86f2..4b4801f4e 100644
--- a/pandas/core/frame.py
+++ b/pandas/core/frame.py
@@ -84,7 +84,6 @@ from pandas.core.dtypes.cast import (
     validate_numeric_casting,
 )
 from pandas.core.dtypes.common import (
-    ensure_float64,
     ensure_int64,
     ensure_platform_int,
     infer_dtype_from_object,
@@ -7871,16 +7870,16 @@ Wild         185.0
         numeric_df = self._get_numeric_data()
         cols = numeric_df.columns
         idx = cols.copy()
-        mat = numeric_df.values
+        mat = numeric_df.astype(float, copy=False).to_numpy()
 
         if method == ""pearson"":
-            correl = libalgos.nancorr(ensure_float64(mat), minp=min_periods)
+            correl = libalgos.nancorr(mat, minp=min_periods)
         elif method == ""spearman"":
-            correl = libalgos.nancorr_spearman(ensure_float64(mat), minp=min_periods)
+            correl = libalgos.nancorr_spearman(mat, minp=min_periods)
         elif method == ""kendall"" or callable(method):
             if min_periods is None:
                 min_periods = 1
-            mat = ensure_float64(mat).T
+            mat = mat.T
             corrf = nanops.get_corr_func(method)
             K = len(cols)
             correl = np.empty((K, K), dtype=float)
@@ -8006,19 +8005,19 @@ Wild         185.0
         numeric_df = self._get_numeric_data()
         cols = numeric_df.columns
         idx = cols.copy()
-        mat = numeric_df.values
+        mat = numeric_df.astype(float, copy=False).to_numpy()
 
         if notna(mat).all():
             if min_periods is not None and min_periods > len(mat):
-                baseCov = np.empty((mat.shape[1], mat.shape[1]))
-                baseCov.fill(np.nan)
+                base_cov = np.empty((mat.shape[1], mat.shape[1]))
+                base_cov.fill(np.nan)
             else:
-                baseCov = np.cov(mat.T)
-            baseCov = baseCov.reshape((len(cols), len(cols)))
+                base_cov = np.cov(mat.T)
+            base_cov = base_cov.reshape((len(cols), len(cols)))
         else:
-            baseCov = libalgos.nancorr(ensure_float64(mat), cov=True, minp=min_periods)
+            base_cov = libalgos.nancorr(mat, cov=True, minp=min_periods)
 
-        return self._constructor(baseCov, index=idx, columns=cols)
+        return self._constructor(base_cov, index=idx, columns=cols)
 
     def corrwith(self, other, axis=0, drop=False, method=""pearson"") -> Series:
         """"""
"
"pandas","29","6e3537d","4334482c348c3adc69683c8332295e22092c1b57","pandas/core/arrays/interval.py","pandas/core/arrays/interval.py","diff --git a/pandas/core/arrays/interval.py b/pandas/core/arrays/interval.py","pandas/tests/arrays/interval/test_interval.py;pandas/tests/series/methods/test_convert_dtypes.py","","diff --git a/pandas/core/arrays/interval.py b/pandas/core/arrays/interval.py
index 22ce5a6f8..220b70ff7 100644
--- a/pandas/core/arrays/interval.py
+++ b/pandas/core/arrays/interval.py
@@ -542,19 +542,19 @@ class IntervalArray(IntervalMixin, ExtensionArray):
                 msg = f""'value' should be an interval type, got {type(value)} instead.""
                 raise TypeError(msg) from err
 
+        if needs_float_conversion:
+            raise ValueError(""Cannot set float NaN to integer-backed IntervalArray"")
+
         key = check_array_indexer(self, key)
+
         # Need to ensure that left and right are updated atomically, so we're
         # forced to copy, update the copy, and swap in the new values.
         left = self.left.copy(deep=True)
-        if needs_float_conversion:
-            left = left.astype(""float"")
-        left.values[key] = value_left
+        left._values[key] = value_left
         self._left = left
 
         right = self.right.copy(deep=True)
-        if needs_float_conversion:
-            right = right.astype(""float"")
-        right.values[key] = value_right
+        right._values[key] = value_right
         self._right = right
 
     def __eq__(self, other):
"
"pandas","83","964400d","7ffcf9d6753e7de2c5318e8e0ecdc63586d502f3","pandas/core/indexes/api.py;pandas/core/reshape/concat.py","pandas/core/indexes/api.py;pandas/core/reshape/concat.py","diff --git a/pandas/core/indexes/api.py b/pandas/core/indexes/api.py;diff --git a/pandas/core/reshape/concat.py b/pandas/core/reshape/concat.py","pandas/tests/reshape/test_concat.py","","diff --git a/pandas/core/indexes/api.py b/pandas/core/indexes/api.py
index 4072d06b9..0a23d38ac 100644
--- a/pandas/core/indexes/api.py
+++ b/pandas/core/indexes/api.py
@@ -63,7 +63,7 @@ __all__ = [
 
 
 def get_objs_combined_axis(
-    objs, intersect: bool = False, axis=0, sort: bool = True
+    objs, intersect: bool = False, axis=0, sort: bool = True, copy: bool = False
 ) -> Index:
     """"""
     Extract combined index: return intersection or union (depending on the
@@ -81,13 +81,15 @@ def get_objs_combined_axis(
         The axis to extract indexes from.
     sort : bool, default True
         Whether the result index should come out sorted or not.
+    copy : bool, default False
+        If True, return a copy of the combined index.
 
     Returns
     -------
     Index
     """"""
     obs_idxes = [obj._get_axis(axis) for obj in objs]
-    return _get_combined_index(obs_idxes, intersect=intersect, sort=sort)
+    return _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)
 
 
 def _get_distinct_objs(objs: List[Index]) -> List[Index]:
@@ -105,7 +107,10 @@ def _get_distinct_objs(objs: List[Index]) -> List[Index]:
 
 
 def _get_combined_index(
-    indexes: List[Index], intersect: bool = False, sort: bool = False
+    indexes: List[Index],
+    intersect: bool = False,
+    sort: bool = False,
+    copy: bool = False,
 ) -> Index:
     """"""
     Return the union or intersection of indexes.
@@ -119,6 +124,8 @@ def _get_combined_index(
         calculate the union.
     sort : bool, default False
         Whether the result index should come out sorted or not.
+    copy : bool, default False
+        If True, return a copy of the combined index.
 
     Returns
     -------
@@ -143,6 +150,11 @@ def _get_combined_index(
             index = index.sort_values()
         except TypeError:
             pass
+
+    # GH 29879
+    if copy:
+        index = index.copy()
+
     return index
 
 
diff --git a/pandas/core/reshape/concat.py b/pandas/core/reshape/concat.py
index 9528de36a..b42497b50 100644
--- a/pandas/core/reshape/concat.py
+++ b/pandas/core/reshape/concat.py
@@ -517,7 +517,11 @@ class _Concatenator:
     def _get_comb_axis(self, i: int) -> Index:
         data_axis = self.objs[0]._get_block_manager_axis(i)
         return get_objs_combined_axis(
-            self.objs, axis=data_axis, intersect=self.intersect, sort=self.sort
+            self.objs,
+            axis=data_axis,
+            intersect=self.intersect,
+            sort=self.sort,
+            copy=self.copy,
         )
 
     def _get_concat_axis(self) -> Index:
"
"pandas","34","02a134b","cf9ec7854ecb80709804178e769425f02ddf8c64","pandas/core/resample.py","pandas/core/resample.py","diff --git a/pandas/core/resample.py b/pandas/core/resample.py","pandas/tests/resample/test_datetime_index.py","","diff --git a/pandas/core/resample.py b/pandas/core/resample.py
index 9e3318db3..2e1dcf8da 100644
--- a/pandas/core/resample.py
+++ b/pandas/core/resample.py
@@ -1422,13 +1422,15 @@ class TimeGrouper(Grouper):
         # because replace() will swallow the nanosecond part
         # thus last bin maybe slightly before the end if the end contains
         # nanosecond part and lead to `Values falls after last bin` error
+        # GH 25758: If DST lands at midnight (e.g. 'America/Havana'), user feedback
+        # has noted that ambiguous=True provides the most sensible result
         binner = labels = date_range(
             freq=self.freq,
             start=first,
             end=last,
             tz=ax.tz,
             name=ax.name,
-            ambiguous=""infer"",
+            ambiguous=True,
             nonexistent=""shift_forward"",
         )
 
"
"pandas","147","6acfc75","773f341c8cc5a481a5a222508718034457ed1ebc","pandas/core/dtypes/dtypes.py","pandas/core/dtypes/dtypes.py","diff --git a/pandas/core/dtypes/dtypes.py b/pandas/core/dtypes/dtypes.py","pandas/tests/dtypes/test_dtypes.py","","diff --git a/pandas/core/dtypes/dtypes.py b/pandas/core/dtypes/dtypes.py
index aa7e6801b..fcdb89dd8 100644
--- a/pandas/core/dtypes/dtypes.py
+++ b/pandas/core/dtypes/dtypes.py
@@ -685,7 +685,7 @@ class DatetimeTZDtype(PandasExtensionDtype):
             tz = timezones.tz_standardize(tz)
         elif tz is not None:
             raise pytz.UnknownTimeZoneError(tz)
-        elif tz is None:
+        if tz is None:
             raise TypeError(""A 'tz' is required."")
 
         self._unit = unit
@@ -737,14 +737,17 @@ class DatetimeTZDtype(PandasExtensionDtype):
         """"""
         if isinstance(string, str):
             msg = ""Could not construct DatetimeTZDtype from '{}'""
-            try:
-                match = cls._match.match(string)
-                if match:
-                    d = match.groupdict()
+            match = cls._match.match(string)
+            if match:
+                d = match.groupdict()
+                try:
                     return cls(unit=d[""unit""], tz=d[""tz""])
-            except Exception:
-                # TODO(py3): Change this pass to `raise TypeError(msg) from e`
-                pass
+                except (KeyError, TypeError, ValueError) as err:
+                    # KeyError if maybe_get_tz tries and fails to get a
+                    #  pytz timezone (actually pytz.UnknownTimeZoneError).
+                    # TypeError if we pass a nonsense tz;
+                    # ValueError if we pass a unit other than ""ns""
+                    raise TypeError(msg.format(string)) from err
             raise TypeError(msg.format(string))
 
         raise TypeError(""Could not construct DatetimeTZDtype"")
"
"pandas","134","da1401b","b1eb97bdfe17f477600eef19e82d65480457bbf5","pandas/tests/groupby/test_categorical.py;pandas/tseries/holiday.py","pandas/tests/groupby/test_categorical.py;pandas/tseries/holiday.py","diff --git a/pandas/tests/groupby/test_categorical.py b/pandas/tests/groupby/test_categorical.py;diff --git a/pandas/tseries/holiday.py b/pandas/tseries/holiday.py","pandas/tests/tseries/holiday/test_calendar.py","","diff --git a/pandas/tests/groupby/test_categorical.py b/pandas/tests/groupby/test_categorical.py
index 5391cb5ce..0e30b104b 100644
--- a/pandas/tests/groupby/test_categorical.py
+++ b/pandas/tests/groupby/test_categorical.py
@@ -784,7 +784,8 @@ def test_categorical_no_compress():
 
 def test_sort():
 
-    # http://stackoverflow.com/questions/23814368/sorting-pandas-categorical-labels-after-groupby  # noqa: E501
+    # http://stackoverflow.com/questions/23814368/sorting-pandas-
+    #        categorical-labels-after-groupby
     # This should result in a properly sorted Series so that the plot
     # has a sorted x axis
     # self.cat.groupby(['value_group'])['value_group'].count().plot(kind='bar')
diff --git a/pandas/tseries/holiday.py b/pandas/tseries/holiday.py
index 1654163d2..eb8600031 100644
--- a/pandas/tseries/holiday.py
+++ b/pandas/tseries/holiday.py
@@ -346,7 +346,7 @@ class AbstractHolidayCalendar(metaclass=HolidayCalendarMetaClass):
 
     rules = []  # type: List[Holiday]
     start_date = Timestamp(datetime(1970, 1, 1))
-    end_date = Timestamp(datetime(2030, 12, 31))
+    end_date = Timestamp(datetime(2200, 12, 31))
     _cache = None
 
     def __init__(self, name=None, rules=None):
"
"pandas","92","f2b213c","511a2847f4330c54d079d04b3cac4febe0fe9915","pandas/core/generic.py;pandas/core/indexes/period.py;pandas/core/resample.py;pandas/tests/indexes/period/test_period.py;pandas/tests/indexes/period/test_tools.py","pandas/core/generic.py;pandas/core/indexes/period.py;pandas/core/resample.py;pandas/tests/indexes/period/test_period.py;pandas/tests/indexes/period/test_tools.py","diff --git a/pandas/core/generic.py b/pandas/core/generic.py;diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py;diff --git a/pandas/core/resample.py b/pandas/core/resample.py;diff --git a/pandas/tests/indexes/period/test_period.py b/pandas/tests/indexes/period/test_period.py;diff --git a/pandas/tests/indexes/period/test_tools.py b/pandas/tests/indexes/period/test_tools.py","pandas/tests/frame/methods/test_asof.py","","diff --git a/pandas/core/generic.py b/pandas/core/generic.py
index 24c794cd7..63bb04371 100644
--- a/pandas/core/generic.py
+++ b/pandas/core/generic.py
@@ -6972,8 +6972,7 @@ class NDFrame(PandasObject, SelectionMixin, indexing.IndexingMixin):
         if not is_list:
             start = self.index[0]
             if isinstance(self.index, PeriodIndex):
-                where = Period(where, freq=self.index.freq).ordinal
-                start = start.ordinal
+                where = Period(where, freq=self.index.freq)
 
             if where < start:
                 if not is_series:
diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py
index a8cd8ab49..d34ac1a54 100644
--- a/pandas/core/indexes/period.py
+++ b/pandas/core/indexes/period.py
@@ -469,17 +469,19 @@ class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):
     @Substitution(klass=""PeriodIndex"")
     @Appender(_shared_docs[""searchsorted""])
     def searchsorted(self, value, side=""left"", sorter=None):
-        if isinstance(value, Period):
-            if value.freq != self.freq:
-                raise raise_on_incompatible(self, value)
-            value = value.ordinal
+        if isinstance(value, Period) or value is NaT:
+            self._data._check_compatible_with(value)
         elif isinstance(value, str):
             try:
-                value = Period(value, freq=self.freq).ordinal
+                value = Period(value, freq=self.freq)
             except DateParseError:
                 raise KeyError(f""Cannot interpret '{value}' as period"")
+        elif not isinstance(value, PeriodArray):
+            raise TypeError(
+                ""PeriodIndex.searchsorted requires either a Period or PeriodArray""
+            )
 
-        return self._ndarray_values.searchsorted(value, side=side, sorter=sorter)
+        return self._data.searchsorted(value, side=side, sorter=sorter)
 
     @property
     def is_full(self) -> bool:
@@ -703,8 +705,7 @@ class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):
 
         t1, t2 = self._parsed_string_to_bounds(reso, parsed)
         return slice(
-            self.searchsorted(t1.ordinal, side=""left""),
-            self.searchsorted(t2.ordinal, side=""right""),
+            self.searchsorted(t1, side=""left""), self.searchsorted(t2, side=""right"")
         )
 
     def _convert_tolerance(self, tolerance, target):
diff --git a/pandas/core/resample.py b/pandas/core/resample.py
index 056ba73ed..0e43880df 100644
--- a/pandas/core/resample.py
+++ b/pandas/core/resample.py
@@ -1586,7 +1586,10 @@ class TimeGrouper(Grouper):
         rng += freq_mult
         # adjust bin edge indexes to account for base
         rng -= bin_shift
-        bins = memb.searchsorted(rng, side=""left"")
+
+        # Wrap in PeriodArray for PeriodArray.searchsorted
+        prng = type(memb._data)(rng, dtype=memb.dtype)
+        bins = memb.searchsorted(prng, side=""left"")
 
         if nat_count > 0:
             # NaT handling as in pandas._lib.lib.generate_bins_dt64()
diff --git a/pandas/tests/indexes/period/test_period.py b/pandas/tests/indexes/period/test_period.py
index 4eacf4038..16fa0b0c2 100644
--- a/pandas/tests/indexes/period/test_period.py
+++ b/pandas/tests/indexes/period/test_period.py
@@ -451,7 +451,7 @@ class TestPeriodIndex(DatetimeLike):
         idx = PeriodIndex([2000, 2007, 2007, 2009, 2009], freq=""A-JUN"")
         ts = Series(np.random.randn(len(idx)), index=idx)
 
-        result = ts[2007]
+        result = ts[""2007""]
         expected = ts[1:3]
         tm.assert_series_equal(result, expected)
         result[:] = 1
@@ -461,7 +461,7 @@ class TestPeriodIndex(DatetimeLike):
         idx = PeriodIndex([2000, 2007, 2007, 2009, 2007], freq=""A-JUN"")
         ts = Series(np.random.randn(len(idx)), index=idx)
 
-        result = ts[2007]
+        result = ts[""2007""]
         expected = ts[idx == ""2007""]
         tm.assert_series_equal(result, expected)
 
diff --git a/pandas/tests/indexes/period/test_tools.py b/pandas/tests/indexes/period/test_tools.py
index 2135b8a99..28ab14af7 100644
--- a/pandas/tests/indexes/period/test_tools.py
+++ b/pandas/tests/indexes/period/test_tools.py
@@ -231,14 +231,43 @@ class TestPeriodIndex:
         p2 = pd.Period(""2014-01-04"", freq=freq)
         assert pidx.searchsorted(p2) == 3
 
-        msg = ""Input has different freq=H from PeriodIndex""
+        assert pidx.searchsorted(pd.NaT) == 0
+
+        msg = ""Input has different freq=H from PeriodArray""
         with pytest.raises(IncompatibleFrequency, match=msg):
             pidx.searchsorted(pd.Period(""2014-01-01"", freq=""H""))
 
-        msg = ""Input has different freq=5D from PeriodIndex""
+        msg = ""Input has different freq=5D from PeriodArray""
         with pytest.raises(IncompatibleFrequency, match=msg):
             pidx.searchsorted(pd.Period(""2014-01-01"", freq=""5D""))
 
+    def test_searchsorted_invalid(self):
+        pidx = pd.PeriodIndex(
+            [""2014-01-01"", ""2014-01-02"", ""2014-01-03"", ""2014-01-04"", ""2014-01-05""],
+            freq=""D"",
+        )
+
+        other = np.array([0, 1], dtype=np.int64)
+
+        msg = ""requires either a Period or PeriodArray""
+        with pytest.raises(TypeError, match=msg):
+            pidx.searchsorted(other)
+
+        with pytest.raises(TypeError, match=msg):
+            pidx.searchsorted(other.astype(""timedelta64[ns]""))
+
+        with pytest.raises(TypeError, match=msg):
+            pidx.searchsorted(np.timedelta64(4))
+
+        with pytest.raises(TypeError, match=msg):
+            pidx.searchsorted(np.timedelta64(""NaT"", ""ms""))
+
+        with pytest.raises(TypeError, match=msg):
+            pidx.searchsorted(np.datetime64(4, ""ns""))
+
+        with pytest.raises(TypeError, match=msg):
+            pidx.searchsorted(np.datetime64(""NaT"", ""ns""))
+
 
 class TestPeriodIndexConversion:
     def test_tolist(self):
"
"pandas","4","cca710b","2250ddfaff92abaff20a5bcd78315f5d4bd44981","pandas/core/indexes/base.py","pandas/core/indexes/base.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py","pandas/tests/indexes/multi/test_join.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index 191fe5bee..b8a9827b5 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -3528,7 +3528,10 @@ class Index(IndexOpsMixin, PandasObject):
 
             multi_join_idx = multi_join_idx.remove_unused_levels()
 
-            return multi_join_idx, lidx, ridx
+            if return_indexers:
+                return multi_join_idx, lidx, ridx
+            else:
+                return multi_join_idx
 
         jl = list(overlap)[0]
 
"
"pandas","124","deceebe","5a0f7e9e03976020ba52a7473f90cb1c8a4354c0","pandas/core/strings.py","pandas/core/strings.py","diff --git a/pandas/core/strings.py b/pandas/core/strings.py","pandas/tests/test_strings.py","","diff --git a/pandas/core/strings.py b/pandas/core/strings.py
index a6e0c1252..55ce44d73 100644
--- a/pandas/core/strings.py
+++ b/pandas/core/strings.py
@@ -3401,59 +3401,69 @@ class StringMethods(NoNewAttributesMixin):
     _doc_args[""istitle""] = dict(type=""titlecase"", method=""istitle"")
     _doc_args[""isnumeric""] = dict(type=""numeric"", method=""isnumeric"")
     _doc_args[""isdecimal""] = dict(type=""decimal"", method=""isdecimal"")
+    # force _noarg_wrapper return type with dtype=bool (GH 29624)
     isalnum = _noarg_wrapper(
         lambda x: x.isalnum(),
         name=""isalnum"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""isalnum""],
         returns_string=False,
+        dtype=bool,
     )
     isalpha = _noarg_wrapper(
         lambda x: x.isalpha(),
         name=""isalpha"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""isalpha""],
         returns_string=False,
+        dtype=bool,
     )
     isdigit = _noarg_wrapper(
         lambda x: x.isdigit(),
         name=""isdigit"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""isdigit""],
         returns_string=False,
+        dtype=bool,
     )
     isspace = _noarg_wrapper(
         lambda x: x.isspace(),
         name=""isspace"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""isspace""],
         returns_string=False,
+        dtype=bool,
     )
     islower = _noarg_wrapper(
         lambda x: x.islower(),
         name=""islower"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""islower""],
         returns_string=False,
+        dtype=bool,
     )
     isupper = _noarg_wrapper(
         lambda x: x.isupper(),
         name=""isupper"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""isupper""],
         returns_string=False,
+        dtype=bool,
     )
     istitle = _noarg_wrapper(
         lambda x: x.istitle(),
         name=""istitle"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""istitle""],
         returns_string=False,
+        dtype=bool,
     )
     isnumeric = _noarg_wrapper(
         lambda x: x.isnumeric(),
         name=""isnumeric"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""isnumeric""],
         returns_string=False,
+        dtype=bool,
     )
     isdecimal = _noarg_wrapper(
         lambda x: x.isdecimal(),
         name=""isdecimal"",
         docstring=_shared_docs[""ismethods""] % _doc_args[""isdecimal""],
         returns_string=False,
+        dtype=bool,
     )
 
     @classmethod
"
"pandas","26","13dc13f","70ca24680d3e51fa4b957366e8093b3cc755462d","pandas/core/arrays/categorical.py","pandas/core/arrays/categorical.py","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py","pandas/tests/arrays/categorical/test_analytics.py","","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index c9b8db28e..b3fb34598 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -2143,7 +2143,7 @@ class Categorical(ExtensionArray, PandasObject):
 
         good = self._codes != -1
         if not good.all():
-            if skipna:
+            if skipna and good.any():
                 pointer = self._codes[good].min()
             else:
                 return np.nan
@@ -2178,7 +2178,7 @@ class Categorical(ExtensionArray, PandasObject):
 
         good = self._codes != -1
         if not good.all():
-            if skipna:
+            if skipna and good.any():
                 pointer = self._codes[good].max()
             else:
                 return np.nan
"
"pandas","72","a9b61a9","0c50950f2a7e32887eff6be5979f09772091e1de","pandas/core/internals/blocks.py","pandas/core/internals/blocks.py","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py","pandas/tests/frame/indexing/test_categorical.py","","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index a93211edf..43edc246d 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -876,7 +876,11 @@ class Block(PandasObject):
 
         # length checking
         check_setitem_lengths(indexer, value, values)
-
+        exact_match = (
+            len(arr_value.shape)
+            and arr_value.shape[0] == values.shape[0]
+            and arr_value.size == values.size
+        )
         if is_empty_indexer(indexer, arr_value):
             # GH#8669 empty indexers
             pass
@@ -886,14 +890,21 @@ class Block(PandasObject):
             #  be e.g. a list; see GH#6043
             values[indexer] = value
 
-        # if we are an exact match (ex-broadcasting),
-        # then use the resultant dtype
         elif (
-            len(arr_value.shape)
-            and arr_value.shape[0] == values.shape[0]
-            and arr_value.size == values.size
+            exact_match
+            and is_categorical_dtype(arr_value.dtype)
+            and not is_categorical_dtype(values)
         ):
+            # GH25495 - If the current dtype is not categorical,
+            # we need to create a new categorical block
             values[indexer] = value
+            return self.make_block(Categorical(self.values, dtype=arr_value.dtype))
+
+        # if we are an exact match (ex-broadcasting),
+        # then use the resultant dtype
+        elif exact_match:
+            values[indexer] = value
+
             try:
                 values = values.astype(arr_value.dtype)
             except ValueError:
"
"pandas","142","7721f31","65815e6f33e25991e3d40a53c581ffb3c7daf70f","pandas/core/algorithms.py;pandas/tests/series/test_timeseries.py","pandas/core/algorithms.py;pandas/tests/series/test_timeseries.py","diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py;diff --git a/pandas/tests/series/test_timeseries.py b/pandas/tests/series/test_timeseries.py","pandas/tests/series/test_analytics.py","","diff --git a/pandas/core/algorithms.py b/pandas/core/algorithms.py
index 002bbcc63..5a479667f 100644
--- a/pandas/core/algorithms.py
+++ b/pandas/core/algorithms.py
@@ -1910,6 +1910,7 @@ def diff(arr, n: int, axis: int = 0):
     dtype = arr.dtype
 
     is_timedelta = False
+    is_bool = False
     if needs_i8_conversion(arr):
         dtype = np.float64
         arr = arr.view(""i8"")
@@ -1918,6 +1919,7 @@ def diff(arr, n: int, axis: int = 0):
 
     elif is_bool_dtype(dtype):
         dtype = np.object_
+        is_bool = True
 
     elif is_integer_dtype(dtype):
         dtype = np.float64
@@ -1959,6 +1961,8 @@ def diff(arr, n: int, axis: int = 0):
             result = res - lag
             result[mask] = na
             out_arr[res_indexer] = result
+        elif is_bool:
+            out_arr[res_indexer] = arr[res_indexer] ^ arr[lag_indexer]
         else:
             out_arr[res_indexer] = arr[res_indexer] - arr[lag_indexer]
 
diff --git a/pandas/tests/series/test_timeseries.py b/pandas/tests/series/test_timeseries.py
index d0ca5d82c..fbe3f929c 100644
--- a/pandas/tests/series/test_timeseries.py
+++ b/pandas/tests/series/test_timeseries.py
@@ -355,48 +355,6 @@ class TestTimeSeries(TestData):
         )
         tm.assert_index_equal(expected.index, result.index)
 
-    def test_diff(self):
-        # Just run the function
-        self.ts.diff()
-
-        # int dtype
-        a = 10000000000000000
-        b = a + 1
-        s = Series([a, b])
-
-        rs = s.diff()
-        assert rs[1] == 1
-
-        # neg n
-        rs = self.ts.diff(-1)
-        xp = self.ts - self.ts.shift(-1)
-        assert_series_equal(rs, xp)
-
-        # 0
-        rs = self.ts.diff(0)
-        xp = self.ts - self.ts
-        assert_series_equal(rs, xp)
-
-        # datetime diff (GH3100)
-        s = Series(date_range(""20130102"", periods=5))
-        rs = s - s.shift(1)
-        xp = s.diff()
-        assert_series_equal(rs, xp)
-
-        # timedelta diff
-        nrs = rs - rs.shift(1)
-        nxp = xp.diff()
-        assert_series_equal(nrs, nxp)
-
-        # with tz
-        s = Series(
-            date_range(""2000-01-01 09:00:00"", periods=5, tz=""US/Eastern""), name=""foo""
-        )
-        result = s.diff()
-        assert_series_equal(
-            result, Series(TimedeltaIndex([""NaT""] + [""1 days""] * 4), name=""foo"")
-        )
-
     def test_pct_change(self):
         rs = self.ts.pct_change(fill_method=None)
         assert_series_equal(rs, self.ts / self.ts.shift(1) - 1)
"
"pandas","96","5e488a0","6d67cf9f02dd22cc870fd407f569197512700203","pandas/tseries/offsets.py","pandas/tseries/offsets.py","diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py","pandas/tests/indexes/datetimes/test_date_range.py","","diff --git a/pandas/tseries/offsets.py b/pandas/tseries/offsets.py
index f20d385ff..8bb98a271 100644
--- a/pandas/tseries/offsets.py
+++ b/pandas/tseries/offsets.py
@@ -896,7 +896,15 @@ class BusinessHourMixin(BusinessMixin):
 
             # adjust by business days first
             if bd != 0:
-                skip_bd = BusinessDay(n=bd)
+                if isinstance(self, _CustomMixin):  # GH 30593
+                    skip_bd = CustomBusinessDay(
+                        n=bd,
+                        weekmask=self.weekmask,
+                        holidays=self.holidays,
+                        calendar=self.calendar,
+                    )
+                else:
+                    skip_bd = BusinessDay(n=bd)
                 # midnight business hour may not on BusinessDay
                 if not self.next_bday.is_on_offset(other):
                     prev_open = self._prev_opening_time(other)
"
"pandas","167","6af6d51","226398224d260d908a1f3d0f23c16fa9ffc8f9b0","pandas/core/indexes/base.py;pandas/core/indexes/datetimes.py;pandas/core/indexes/period.py;pandas/core/indexing.py","pandas/core/indexes/base.py;pandas/core/indexes/datetimes.py;pandas/core/indexes/period.py;pandas/core/indexing.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py;diff --git a/pandas/core/indexes/datetimes.py b/pandas/core/indexes/datetimes.py;diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py;diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py","pandas/tests/indexes/datetimes/test_partial_slicing.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index d0de99525..5fc477ed3 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -243,6 +243,9 @@ class Index(IndexOpsMixin, PandasObject):
     _infer_as_myclass = False
 
     _engine_type = libindex.ObjectEngine
+    # whether we support partial string indexing. Overridden
+    # in DatetimeIndex and PeriodIndex
+    _supports_partial_string_indexing = False
 
     _accessors = {""str""}
 
diff --git a/pandas/core/indexes/datetimes.py b/pandas/core/indexes/datetimes.py
index 0f7f580e2..67de7b019 100644
--- a/pandas/core/indexes/datetimes.py
+++ b/pandas/core/indexes/datetimes.py
@@ -238,6 +238,7 @@ class DatetimeIndex(DatetimeIndexOpsMixin, Int64Index, DatetimeDelegateMixin):
     )
 
     _engine_type = libindex.DatetimeEngine
+    _supports_partial_string_indexing = True
 
     _tz = None
     _freq = None
diff --git a/pandas/core/indexes/period.py b/pandas/core/indexes/period.py
index 19fe1eb89..f6b3d1076 100644
--- a/pandas/core/indexes/period.py
+++ b/pandas/core/indexes/period.py
@@ -173,6 +173,7 @@ class PeriodIndex(DatetimeIndexOpsMixin, Int64Index, PeriodDelegateMixin):
     _data = None
 
     _engine_type = libindex.PeriodEngine
+    _supports_partial_string_indexing = True
 
     # ------------------------------------------------------------------------
     # Index Constructors
diff --git a/pandas/core/indexing.py b/pandas/core/indexing.py
index df89dbe6d..e308ae037 100755
--- a/pandas/core/indexing.py
+++ b/pandas/core/indexing.py
@@ -1704,6 +1704,11 @@ class _LocIndexer(_LocationIndexer):
             if isinstance(ax, MultiIndex):
                 return False
 
+            if isinstance(k, str) and ax._supports_partial_string_indexing:
+                # partial string indexing, df.loc['2000', 'A']
+                # should not be considered scalar
+                return False
+
             if not ax.is_unique:
                 return False
 
@@ -1719,7 +1724,10 @@ class _LocIndexer(_LocationIndexer):
         """"""Translate any partial string timestamp matches in key, returning the
         new key (GH 10331)""""""
         if isinstance(labels, MultiIndex):
-            if isinstance(key, str) and labels.levels[0].is_all_dates:
+            if (
+                isinstance(key, str)
+                and labels.levels[0]._supports_partial_string_indexing
+            ):
                 # Convert key '2016-01-01' to
                 # ('2016-01-01'[, slice(None, None, None)]+)
                 key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))
@@ -1729,7 +1737,10 @@ class _LocIndexer(_LocationIndexer):
                 # (..., slice('2016-01-01', '2016-01-01', None), ...)
                 new_key = []
                 for i, component in enumerate(key):
-                    if isinstance(component, str) and labels.levels[i].is_all_dates:
+                    if (
+                        isinstance(component, str)
+                        and labels.levels[i]._supports_partial_string_indexing
+                    ):
                         new_key.append(slice(component, component, None))
                     else:
                         new_key.append(component)
@@ -2334,7 +2345,7 @@ def convert_to_index_sliceable(obj, key):
 
         # We might have a datetimelike string that we can translate to a
         # slice here via partial string indexing
-        if idx.is_all_dates:
+        if idx._supports_partial_string_indexing:
             try:
                 return idx._get_string_slice(key)
             except (KeyError, ValueError, NotImplementedError):
"
"pandas","137","a1b2c4b","48f1a67469c91c38e78ebb2648061fe73dd79e6b","pandas/core/arrays/categorical.py","pandas/core/arrays/categorical.py","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py","pandas/tests/extension/test_categorical.py;pandas/tests/reshape/merge/test_merge.py","","diff --git a/pandas/core/arrays/categorical.py b/pandas/core/arrays/categorical.py
index 43e52cb01..bab1127e6 100644
--- a/pandas/core/arrays/categorical.py
+++ b/pandas/core/arrays/categorical.py
@@ -57,7 +57,7 @@ from pandas.core.algorithms import (
 )
 from pandas.core.base import NoNewAttributesMixin, PandasObject, _shared_docs
 import pandas.core.common as com
-from pandas.core.construction import extract_array, sanitize_array
+from pandas.core.construction import array, extract_array, sanitize_array
 from pandas.core.missing import interpolate_2d
 from pandas.core.sorting import nargsort
 
@@ -520,6 +520,8 @@ class Categorical(ExtensionArray, PandasObject):
             if dtype == self.dtype:
                 return self
             return self._set_dtype(dtype)
+        if is_extension_array_dtype(dtype):
+            return array(self, dtype=dtype, copy=copy)  # type: ignore # GH 28770
         if is_integer_dtype(dtype) and self.isna().any():
             msg = ""Cannot convert float NaN to integer""
             raise ValueError(msg)
"
"pandas","91","5c12d4f","cb9a1c7d0319c34a97247973ca96af53ead8033a","pandas/core/indexes/timedeltas.py","pandas/core/indexes/timedeltas.py","diff --git a/pandas/core/indexes/timedeltas.py b/pandas/core/indexes/timedeltas.py","pandas/tests/arrays/test_timedeltas.py","","diff --git a/pandas/core/indexes/timedeltas.py b/pandas/core/indexes/timedeltas.py
index d1d8db074..a94e19d99 100644
--- a/pandas/core/indexes/timedeltas.py
+++ b/pandas/core/indexes/timedeltas.py
@@ -357,11 +357,25 @@ class TimedeltaIndex(
     @Appender(_shared_docs[""searchsorted""])
     def searchsorted(self, value, side=""left"", sorter=None):
         if isinstance(value, (np.ndarray, Index)):
-            value = np.array(value, dtype=_TD_DTYPE, copy=False)
-        else:
-            value = Timedelta(value).asm8.view(_TD_DTYPE)
+            if not type(self._data)._is_recognized_dtype(value):
+                raise TypeError(
+                    ""searchsorted requires compatible dtype or scalar, ""
+                    f""not {type(value).__name__}""
+                )
+            value = type(self._data)(value)
+            self._data._check_compatible_with(value)
+
+        elif isinstance(value, self._data._recognized_scalars):
+            self._data._check_compatible_with(value)
+            value = self._data._scalar_type(value)
+
+        elif not isinstance(value, TimedeltaArray):
+            raise TypeError(
+                ""searchsorted requires compatible dtype or scalar, ""
+                f""not {type(value).__name__}""
+            )
 
-        return self.values.searchsorted(value, side=side, sorter=sorter)
+        return self._data.searchsorted(value, side=side, sorter=sorter)
 
     def is_type_compatible(self, typ) -> bool:
         return typ == self.inferred_type or typ == ""timedelta""
"
"pandas","5","4f4282f8a7c1033fc3efe9f4eab5368a70ae5041","0babe10f1f3f19bb6183b6fb4cc958583c6860c8","pandas/core/indexes/base.py;pandas/tests/indexes/multi/test_join.py","pandas/core/indexes/base.py;pandas/tests/indexes/multi/test_join.py","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py;diff --git a/pandas/tests/indexes/multi/test_join.py b/pandas/tests/indexes/multi/test_join.py","pandas/tests/groupby/test_function.py","","diff --git a/pandas/core/indexes/base.py b/pandas/core/indexes/base.py
index 191fe5bee..b8a9827b5 100644
--- a/pandas/core/indexes/base.py
+++ b/pandas/core/indexes/base.py
@@ -3528,7 +3528,10 @@ class Index(IndexOpsMixin, PandasObject):
 
             multi_join_idx = multi_join_idx.remove_unused_levels()
 
-            return multi_join_idx, lidx, ridx
+            if return_indexers:
+                return multi_join_idx, lidx, ridx
+            else:
+                return multi_join_idx
 
         jl = list(overlap)[0]
 
diff --git a/pandas/tests/indexes/multi/test_join.py b/pandas/tests/indexes/multi/test_join.py
index 062fb92c4..6be9ec463 100644
--- a/pandas/tests/indexes/multi/test_join.py
+++ b/pandas/tests/indexes/multi/test_join.py
@@ -96,10 +96,20 @@ def test_join_multi_wrong_order():
     midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[""a"", ""b""])
     midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[""b"", ""a""])
 
-    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=False)
+    join_idx, lidx, ridx = midx1.join(midx2, return_indexers=True)
 
     exp_ridx = np.array([-1, -1, -1, -1], dtype=np.intp)
 
     tm.assert_index_equal(midx1, join_idx)
     assert lidx is None
     tm.assert_numpy_array_equal(ridx, exp_ridx)
+
+
+def test_join_multi_return_indexers():
+    # GH 34074
+
+    midx1 = pd.MultiIndex.from_product([[1, 2], [3, 4], [5, 6]], names=[""a"", ""b"", ""c""])
+    midx2 = pd.MultiIndex.from_product([[1, 2], [3, 4]], names=[""a"", ""b""])
+
+    result = midx1.join(midx2, return_indexers=False)
+    tm.assert_index_equal(result, midx1)
"
"pandas","67","c3e32d7","1996b17599731b889895b0e1edf758588c068fbb","pandas/core/internals/blocks.py","pandas/core/internals/blocks.py","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py","pandas/tests/frame/indexing/test_indexing.py","","diff --git a/pandas/core/internals/blocks.py b/pandas/core/internals/blocks.py
index 9e31ccebd..cb03fbe17 100644
--- a/pandas/core/internals/blocks.py
+++ b/pandas/core/internals/blocks.py
@@ -7,7 +7,7 @@ import warnings
 
 import numpy as np
 
-from pandas._libs import NaT, algos as libalgos, lib, tslib, writers
+from pandas._libs import NaT, Timestamp, algos as libalgos, lib, tslib, writers
 from pandas._libs.index import convert_scalar
 import pandas._libs.internals as libinternals
 from pandas._libs.tslibs import Timedelta, conversion
@@ -2158,6 +2158,16 @@ class DatetimeLikeBlockMixin:
         # Override to return DatetimeArray and TimedeltaArray
         return self.array_values()
 
+    def iget(self, key):
+        # GH#31649 we need to wrap scalars in Timestamp/Timedelta
+        # TODO: this can be removed if we ever have 2D EA
+        result = super().iget(key)
+        if isinstance(result, np.datetime64):
+            result = Timestamp(result)
+        elif isinstance(result, np.timedelta64):
+            result = Timedelta(result)
+        return result
+
 
 class DatetimeBlock(DatetimeLikeBlockMixin, Block):
     __slots__ = ()
"
"pandas","40","218cc30","8a5f2917e163e09e08af880819fdf44144b1a5fe","pandas/core/reshape/merge.py","pandas/core/reshape/merge.py","diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py","pandas/tests/reshape/merge/test_merge.py","","diff --git a/pandas/core/reshape/merge.py b/pandas/core/reshape/merge.py
index 3b3802875..6e024560e 100644
--- a/pandas/core/reshape/merge.py
+++ b/pandas/core/reshape/merge.py
@@ -6,14 +6,14 @@ import copy
 import datetime
 from functools import partial
 import string
-from typing import TYPE_CHECKING, Optional, Tuple, Union
+from typing import TYPE_CHECKING, Optional, Tuple, Union, cast
 import warnings
 
 import numpy as np
 
 from pandas._libs import Timedelta, hashtable as libhashtable, lib
 import pandas._libs.join as libjoin
-from pandas._typing import FrameOrSeries
+from pandas._typing import ArrayLike, FrameOrSeries
 from pandas.errors import MergeError
 from pandas.util._decorators import Appender, Substitution
 
@@ -24,6 +24,7 @@ from pandas.core.dtypes.common import (
     is_array_like,
     is_bool,
     is_bool_dtype,
+    is_categorical,
     is_categorical_dtype,
     is_datetime64tz_dtype,
     is_dtype_equal,
@@ -1271,7 +1272,7 @@ def _get_join_indexers(
 
     # get left & right join labels and num. of levels at each location
     mapped = (
-        _factorize_keys(left_keys[n], right_keys[n], sort=sort)
+        _factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how)
         for n in range(len(left_keys))
     )
     zipped = zip(*mapped)
@@ -1283,8 +1284,8 @@ def _get_join_indexers(
     # factorize keys to a dense i8 space
     # `count` is the num. of unique keys
     # set(lkey) | set(rkey) == range(count)
-    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort)
 
+    lkey, rkey, count = _factorize_keys(lkey, rkey, sort=sort, how=how)
     # preserve left frame order if how == 'left' and sort == False
     kwargs = copy.copy(kwargs)
     if how == ""left"":
@@ -1822,7 +1823,59 @@ def _right_outer_join(x, y, max_groups):
     return left_indexer, right_indexer
 
 
-def _factorize_keys(lk, rk, sort=True):
+def _factorize_keys(
+    lk: ArrayLike, rk: ArrayLike, sort: bool = True, how: str = ""inner""
+) -> Tuple[np.array, np.array, int]:
+    """"""
+    Encode left and right keys as enumerated types.
+
+    This is used to get the join indexers to be used when merging DataFrames.
+
+    Parameters
+    ----------
+    lk : array-like
+        Left key.
+    rk : array-like
+        Right key.
+    sort : bool, defaults to True
+        If True, the encoding is done such that the unique elements in the
+        keys are sorted.
+    how : {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’
+        Type of merge.
+
+    Returns
+    -------
+    array
+        Left (resp. right if called with `key='right'`) labels, as enumerated type.
+    array
+        Right (resp. left if called with `key='right'`) labels, as enumerated type.
+    int
+        Number of unique elements in union of left and right labels.
+
+    See Also
+    --------
+    merge : Merge DataFrame or named Series objects
+        with a database-style join.
+    algorithms.factorize : Encode the object as an enumerated type
+        or categorical variable.
+
+    Examples
+    --------
+    >>> lk = np.array([""a"", ""c"", ""b""])
+    >>> rk = np.array([""a"", ""c""])
+
+    Here, the unique values are `'a', 'b', 'c'`. With the default
+    `sort=True`, the encoding will be `{0: 'a', 1: 'b', 2: 'c'}`:
+
+    >>> pd.core.reshape.merge._factorize_keys(lk, rk)
+    (array([0, 2, 1]), array([0, 2]), 3)
+
+    With the `sort=False`, the encoding will correspond to the order
+    in which the unique elements first appear: `{0: 'a', 1: 'c', 2: 'b'}`:
+
+    >>> pd.core.reshape.merge._factorize_keys(lk, rk, sort=False)
+    (array([0, 1, 2]), array([0, 1]), 3)
+    """"""
     # Some pre-processing for non-ndarray lk / rk
     lk = extract_array(lk, extract_numpy=True)
     rk = extract_array(rk, extract_numpy=True)
@@ -1834,8 +1887,11 @@ def _factorize_keys(lk, rk, sort=True):
         rk, _ = rk._values_for_factorize()
 
     elif (
-        is_categorical_dtype(lk) and is_categorical_dtype(rk) and lk.is_dtype_equal(rk)
+        is_categorical_dtype(lk) and is_categorical_dtype(rk) and is_dtype_equal(lk, rk)
     ):
+        assert is_categorical(lk) and is_categorical(rk)
+        lk = cast(Categorical, lk)
+        rk = cast(Categorical, rk)
         if lk.categories.equals(rk.categories):
             # if we exactly match in categories, allow us to factorize on codes
             rk = rk.codes
@@ -1892,6 +1948,8 @@ def _factorize_keys(lk, rk, sort=True):
             np.putmask(rlab, rmask, count)
         count += 1
 
+    if how == ""right"":
+        return rlab, llab, count
     return llab, rlab, count
 
 
"
"pandas","24","91dcc3a","6367bd23b935a85f1bcd2ae762c7f08433d0efbd","pandas/core/arrays/datetimes.py;pandas/tests/series/test_arithmetic.py","pandas/core/arrays/datetimes.py;pandas/tests/series/test_arithmetic.py","diff --git a/pandas/core/arrays/datetimes.py b/pandas/core/arrays/datetimes.py;diff --git a/pandas/tests/series/test_arithmetic.py b/pandas/tests/series/test_arithmetic.py","pandas/tests/indexes/datetimes/test_timezones.py","","diff --git a/pandas/core/arrays/datetimes.py b/pandas/core/arrays/datetimes.py
index dd553011c..f777f52f5 100644
--- a/pandas/core/arrays/datetimes.py
+++ b/pandas/core/arrays/datetimes.py
@@ -886,7 +886,7 @@ default 'raise'
         DatetimeIndex(['2018-03-01 09:00:00-05:00',
                        '2018-03-02 09:00:00-05:00',
                        '2018-03-03 09:00:00-05:00'],
-                      dtype='datetime64[ns, US/Eastern]', freq='D')
+                      dtype='datetime64[ns, US/Eastern]', freq=None)
 
         With the ``tz=None``, we can remove the time zone information
         while keeping the local time (not converted to UTC):
@@ -894,7 +894,7 @@ default 'raise'
         >>> tz_aware.tz_localize(None)
         DatetimeIndex(['2018-03-01 09:00:00', '2018-03-02 09:00:00',
                        '2018-03-03 09:00:00'],
-                      dtype='datetime64[ns]', freq='D')
+                      dtype='datetime64[ns]', freq=None)
 
         Be careful with DST changes. When there is sequential data, pandas can
         infer the DST time:
@@ -973,7 +973,16 @@ default 'raise'
             )
         new_dates = new_dates.view(DT64NS_DTYPE)
         dtype = tz_to_dtype(tz)
-        return self._simple_new(new_dates, dtype=dtype, freq=self.freq)
+
+        freq = None
+        if timezones.is_utc(tz) or (len(self) == 1 and not isna(new_dates[0])):
+            # we can preserve freq
+            # TODO: Also for fixed-offsets
+            freq = self.freq
+        elif tz is None and self.tz is None:
+            # no-op
+            freq = self.freq
+        return self._simple_new(new_dates, dtype=dtype, freq=freq)
 
     # ----------------------------------------------------------------
     # Conversion Methods - Vectorized analogues of Timestamp methods
diff --git a/pandas/tests/series/test_arithmetic.py b/pandas/tests/series/test_arithmetic.py
index a63852405..16163ee76 100644
--- a/pandas/tests/series/test_arithmetic.py
+++ b/pandas/tests/series/test_arithmetic.py
@@ -378,6 +378,7 @@ class TestSeriesComparison:
 
         # datetime64tz dtype
         dti = dti.tz_localize(""US/Central"")
+        dti._set_freq(""infer"")  # freq not preserved by tz_localize
         ser = Series(dti).rename(names[1])
         result = op(ser, dti)
         assert result.name == names[2]
"
"pandas","36","cb41651","51f114b9882a5cf819efddb8be74524814f437e1","pandas/core/dtypes/missing.py","pandas/core/dtypes/missing.py","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py","pandas/tests/dtypes//test_missing.py","","diff --git a/pandas/core/dtypes/missing.py b/pandas/core/dtypes/missing.py
index d461db2d0..f7b061536 100644
--- a/pandas/core/dtypes/missing.py
+++ b/pandas/core/dtypes/missing.py
@@ -16,30 +16,24 @@ from pandas.core.dtypes.common import (
     ensure_object,
     is_bool_dtype,
     is_complex_dtype,
-    is_datetime64_dtype,
-    is_datetime64tz_dtype,
     is_datetimelike_v_numeric,
     is_dtype_equal,
     is_extension_array_dtype,
     is_float_dtype,
     is_integer_dtype,
     is_object_dtype,
-    is_period_dtype,
     is_scalar,
     is_string_dtype,
     is_string_like_dtype,
-    is_timedelta64_dtype,
     needs_i8_conversion,
     pandas_dtype,
 )
 from pandas.core.dtypes.generic import (
     ABCDataFrame,
-    ABCDatetimeArray,
     ABCExtensionArray,
     ABCIndexClass,
     ABCMultiIndex,
     ABCSeries,
-    ABCTimedeltaArray,
 )
 from pandas.core.dtypes.inference import is_list_like
 
@@ -139,17 +133,7 @@ def _isna_new(obj):
         raise NotImplementedError(""isna is not defined for MultiIndex"")
     elif isinstance(obj, type):
         return False
-    elif isinstance(
-        obj,
-        (
-            ABCSeries,
-            np.ndarray,
-            ABCIndexClass,
-            ABCExtensionArray,
-            ABCDatetimeArray,
-            ABCTimedeltaArray,
-        ),
-    ):
+    elif isinstance(obj, (ABCSeries, np.ndarray, ABCIndexClass, ABCExtensionArray)):
         return _isna_ndarraylike(obj)
     elif isinstance(obj, ABCDataFrame):
         return obj.isna()
@@ -158,7 +142,7 @@ def _isna_new(obj):
     elif hasattr(obj, ""__array__""):
         return _isna_ndarraylike(np.asarray(obj))
     else:
-        return obj is None
+        return False
 
 
 def _isna_old(obj):
@@ -189,7 +173,7 @@ def _isna_old(obj):
     elif hasattr(obj, ""__array__""):
         return _isna_ndarraylike_old(np.asarray(obj))
     else:
-        return obj is None
+        return False
 
 
 _isna = _isna_new
@@ -224,37 +208,14 @@ def _use_inf_as_na(key):
 
 
 def _isna_ndarraylike(obj):
-    is_extension = is_extension_array_dtype(obj)
-
-    if not is_extension:
-        # Avoid accessing `.values` on things like
-        # PeriodIndex, which may be expensive.
-        values = getattr(obj, ""_values"", obj)
-    else:
-        values = obj
-
+    is_extension = is_extension_array_dtype(obj.dtype)
+    values = getattr(obj, ""_values"", obj)
     dtype = values.dtype
 
     if is_extension:
-        if isinstance(obj, (ABCIndexClass, ABCSeries)):
-            values = obj._values
-        else:
-            values = obj
         result = values.isna()
-    elif isinstance(obj, ABCDatetimeArray):
-        return obj.isna()
     elif is_string_dtype(dtype):
-        # Working around NumPy ticket 1542
-        shape = values.shape
-
-        if is_string_like_dtype(dtype):
-            # object array of strings
-            result = np.zeros(values.shape, dtype=bool)
-        else:
-            # object array of non-strings
-            result = np.empty(shape, dtype=bool)
-            vec = libmissing.isnaobj(values.ravel())
-            result[...] = vec.reshape(shape)
+        result = _isna_string_dtype(values, dtype, old=False)
 
     elif needs_i8_conversion(dtype):
         # this is the NaT pattern
@@ -274,17 +235,9 @@ def _isna_ndarraylike_old(obj):
     dtype = values.dtype
 
     if is_string_dtype(dtype):
-        # Working around NumPy ticket 1542
-        shape = values.shape
-
-        if is_string_like_dtype(dtype):
-            result = np.zeros(values.shape, dtype=bool)
-        else:
-            result = np.empty(shape, dtype=bool)
-            vec = libmissing.isnaobj_old(values.ravel())
-            result[:] = vec.reshape(shape)
+        result = _isna_string_dtype(values, dtype, old=True)
 
-    elif is_datetime64_dtype(dtype):
+    elif needs_i8_conversion(dtype):
         # this is the NaT pattern
         result = values.view(""i8"") == iNaT
     else:
@@ -297,6 +250,24 @@ def _isna_ndarraylike_old(obj):
     return result
 
 
+def _isna_string_dtype(values: np.ndarray, dtype: np.dtype, old: bool) -> np.ndarray:
+    # Working around NumPy ticket 1542
+    shape = values.shape
+
+    if is_string_like_dtype(dtype):
+        result = np.zeros(values.shape, dtype=bool)
+    else:
+        result = np.empty(shape, dtype=bool)
+        if old:
+            vec = libmissing.isnaobj_old(values.ravel())
+        else:
+            vec = libmissing.isnaobj(values.ravel())
+
+        result[...] = vec.reshape(shape)
+
+    return result
+
+
 def notna(obj):
     """"""
     Detect non-missing values for an array-like object.
@@ -556,12 +527,7 @@ def na_value_for_dtype(dtype, compat: bool = True):
 
     if is_extension_array_dtype(dtype):
         return dtype.na_value
-    if (
-        is_datetime64_dtype(dtype)
-        or is_datetime64tz_dtype(dtype)
-        or is_timedelta64_dtype(dtype)
-        or is_period_dtype(dtype)
-    ):
+    if needs_i8_conversion(dtype):
         return NaT
     elif is_float_dtype(dtype):
         return np.nan
"
"pandas","104","f738581","8e9b3eee812b70197341c26c40200d8a1a77ed9c","pandas/core/groupby/groupby.py","pandas/core/groupby/groupby.py","diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py","pandas/tests/groupby/test_function.py","","diff --git a/pandas/core/groupby/groupby.py b/pandas/core/groupby/groupby.py
index 529d123d2..227547daf 100644
--- a/pandas/core/groupby/groupby.py
+++ b/pandas/core/groupby/groupby.py
@@ -1937,21 +1937,22 @@ class GroupBy(_GroupBy):
             #  >>> result.stack(0).loc[pd.IndexSlice[:, ..., q], :]
             #  but this hits https://github.com/pandas-dev/pandas/issues/10710
             #  which doesn't reorder the list-like `q` on the inner level.
-            order = np.roll(list(range(result.index.nlevels)), -1)
-            result = result.reorder_levels(order)
-            result = result.reindex(q, level=-1)
+            order = list(range(1, result.index.nlevels)) + [0]
+
+            # temporarily saves the index names
+            index_names = np.array(result.index.names)
 
-            # fix order.
-            hi = len(q) * self.ngroups
-            arr = np.arange(0, hi, self.ngroups)
-            arrays = []
+            # set index names to positions to avoid confusion
+            result.index.names = np.arange(len(index_names))
+
+            # place quantiles on the inside
+            result = result.reorder_levels(order)
 
-            for i in range(self.ngroups):
-                arr2 = arr + i
-                arrays.append(arr2)
+            # restore the index names in order
+            result.index.names = index_names[order]
 
-            indices = np.concatenate(arrays)
-            assert len(indices) == len(result)
+            # reorder rows to keep things sorted
+            indices = np.arange(len(result)).reshape([len(q), self.ngroups]).T.flatten()
             return result.take(indices)
 
     @Substitution(name=""groupby"")
"
"pandas","168","2de4fbb","1fa1ad91b29c5474cbb86cbcbcdcd50537cad0ae","pandas/core/groupby/grouper.py","pandas/core/groupby/grouper.py","diff --git a/pandas/core/groupby/grouper.py b/pandas/core/groupby/grouper.py","pandas/tests/groupby/test_groupby.py","","diff --git a/pandas/core/groupby/grouper.py b/pandas/core/groupby/grouper.py
index 1d88ebd26..5c32550af 100644
--- a/pandas/core/groupby/grouper.py
+++ b/pandas/core/groupby/grouper.py
@@ -606,10 +606,10 @@ def _get_grouper(
         elif is_in_axis(gpr):  # df.groupby('name')
             if gpr in obj:
                 if validate:
-                    obj._check_label_or_level_ambiguity(gpr)
+                    obj._check_label_or_level_ambiguity(gpr, axis=axis)
                 in_axis, name, gpr = True, gpr, obj[gpr]
                 exclusions.append(name)
-            elif obj._is_level_reference(gpr):
+            elif obj._is_level_reference(gpr, axis=axis):
                 in_axis, name, level, gpr = False, None, gpr, None
             else:
                 raise KeyError(gpr)
"
"pandas","86","55cfabb","f792d8c50ee456aa8aa2ae406d8e6b8843f45614","pandas/core/reshape/pivot.py","pandas/core/reshape/pivot.py","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py","pandas/tests/reshape/test_pivot.py","","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py
index 13df39cc0..930ff5f45 100644
--- a/pandas/core/reshape/pivot.py
+++ b/pandas/core/reshape/pivot.py
@@ -429,6 +429,9 @@ def _convert_by(by):
 @Substitution(""\ndata : DataFrame"")
 @Appender(_shared_docs[""pivot""], indents=1)
 def pivot(data: ""DataFrame"", index=None, columns=None, values=None) -> ""DataFrame"":
+    if columns is None:
+        raise TypeError(""pivot() missing 1 required argument: 'columns'"")
+
     if values is None:
         cols = [columns] if index is None else [index, columns]
         append = index is None
"
"pandas","87","641346c","a890239b7020dec714d9819b718d83f786bfda34","pandas/core/reshape/pivot.py","pandas/core/reshape/pivot.py","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py","pandas/tests/reshape/test_pivot.py","","diff --git a/pandas/core/reshape/pivot.py b/pandas/core/reshape/pivot.py
index 7109f2376..13df39cc0 100644
--- a/pandas/core/reshape/pivot.py
+++ b/pandas/core/reshape/pivot.py
@@ -581,6 +581,8 @@ def crosstab(
     from pandas import DataFrame
 
     df = DataFrame(data, index=common_idx)
+    original_df_cols = df.columns
+
     if values is None:
         df[""__dummy__""] = 0
         kwargs = {""aggfunc"": len, ""fill_value"": 0}
@@ -589,7 +591,7 @@ def crosstab(
         kwargs = {""aggfunc"": aggfunc}
 
     table = df.pivot_table(
-        ""__dummy__"",
+        [""__dummy__""],
         index=rownames,
         columns=colnames,
         margins=margins,
@@ -598,6 +600,12 @@ def crosstab(
         **kwargs,
     )
 
+    # GH18321, after pivoting, an extra top level of column index of `__dummy__` is
+    # created, and this extra level should not be included in the further steps
+    if not table.empty:
+        cols_diff = df.columns.difference(original_df_cols)[0]
+        table = table[cols_diff]
+
     # Post-process
     if normalize is not False:
         table = _normalize(
"
"pandas","46","e734449","0ed6d538c38010bcbd540cd6413ae8e4b749d9e6","pandas/core/indexes/multi.py;pandas/core/reshape/util.py;pandas/tests/reshape/test_util.py","pandas/core/indexes/multi.py;pandas/core/reshape/util.py;pandas/tests/reshape/test_util.py","diff --git a/pandas/core/indexes/multi.py b/pandas/core/indexes/multi.py;diff --git a/pandas/core/reshape/util.py b/pandas/core/reshape/util.py;diff --git a/pandas/tests/reshape/test_util.py b/pandas/tests/reshape/test_util.py","pandas/tests/reshape/test_pivot.py","","diff --git a/pandas/core/indexes/multi.py b/pandas/core/indexes/multi.py
index 122097f44..5bffc4ec5 100644
--- a/pandas/core/indexes/multi.py
+++ b/pandas/core/indexes/multi.py
@@ -565,6 +565,7 @@ class MultiIndex(Index):
         if names is lib.no_default:
             names = [getattr(it, ""name"", None) for it in iterables]
 
+        # codes are all ndarrays, so cartesian_product is lossless
         codes = cartesian_product(codes)
         return MultiIndex(levels, codes, sortorder=sortorder, names=names)
 
diff --git a/pandas/core/reshape/util.py b/pandas/core/reshape/util.py
index d8652c9b4..7abb14303 100644
--- a/pandas/core/reshape/util.py
+++ b/pandas/core/reshape/util.py
@@ -2,8 +2,6 @@ import numpy as np
 
 from pandas.core.dtypes.common import is_list_like
 
-import pandas.core.common as com
-
 
 def cartesian_product(X):
     """"""
@@ -51,9 +49,20 @@ def cartesian_product(X):
         # if any factor is empty, the cartesian product is empty
         b = np.zeros_like(cumprodX)
 
-    return [
-        np.tile(
-            np.repeat(np.asarray(com.values_from_object(x)), b[i]), np.product(a[i])
-        )
-        for i, x in enumerate(X)
-    ]
+    return [_tile_compat(np.repeat(x, b[i]), np.product(a[i])) for i, x in enumerate(X)]
+
+
+def _tile_compat(arr, num: int):
+    """"""
+    Index compat for np.tile.
+
+    Notes
+    -----
+    Does not support multi-dimensional `num`.
+    """"""
+    if isinstance(arr, np.ndarray):
+        return np.tile(arr, num)
+
+    # Otherwise we have an Index
+    taker = np.tile(np.arange(len(arr)), num)
+    return arr.take(taker)
diff --git a/pandas/tests/reshape/test_util.py b/pandas/tests/reshape/test_util.py
index cd518dda4..9d074b5ad 100644
--- a/pandas/tests/reshape/test_util.py
+++ b/pandas/tests/reshape/test_util.py
@@ -25,6 +25,22 @@ class TestCartesianProduct:
         tm.assert_index_equal(result1, expected1)
         tm.assert_index_equal(result2, expected2)
 
+    def test_tzaware_retained(self):
+        x = date_range(""2000-01-01"", periods=2, tz=""US/Pacific"")
+        y = np.array([3, 4])
+        result1, result2 = cartesian_product([x, y])
+
+        expected = x.repeat(2)
+        tm.assert_index_equal(result1, expected)
+
+    def test_tzaware_retained_categorical(self):
+        x = date_range(""2000-01-01"", periods=2, tz=""US/Pacific"").astype(""category"")
+        y = np.array([3, 4])
+        result1, result2 = cartesian_product([x, y])
+
+        expected = x.repeat(2)
+        tm.assert_index_equal(result1, expected)
+
     def test_empty(self):
         # product of empty factors
         X = [[], [0, 1], []]
"
"keras","22","54386efa549f850dff13f79fc3af67799a4e5d4f","ee02d256611b17d11e37b86bd4f618d7f2a37d84","keras/engine/input_layer.py","keras/engine/input_layer.py","diff --git a/keras/engine/input_layer.py b/keras/engine/input_layer.py","tests/keras/layers/core_test.py","","diff --git a/keras/engine/input_layer.py b/keras/engine/input_layer.py
index bc149168..632bf39e 100644
--- a/keras/engine/input_layer.py
+++ b/keras/engine/input_layer.py
@@ -42,6 +42,7 @@ class InputLayer(Layer):
         self.trainable = False
         self.built = True
         self.sparse = sparse
+        self.supports_masking = True
 
         if input_shape and batch_input_shape:
             raise ValueError('Only provide the input_shape OR '
"
"keras","18","9400be98783135a1d42dd238f4e6c3aa048eceea","244546c2fe5165b6770eb456afd5fac8878473c5","keras/backend/tensorflow_backend.py","keras/backend/tensorflow_backend.py","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py","tests/keras/backend/backend_test.py","","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py
index 99e00949..0b0003d1 100644
--- a/keras/backend/tensorflow_backend.py
+++ b/keras/backend/tensorflow_backend.py
@@ -2545,7 +2545,10 @@ class Function(object):
         # (since the outputs of fetches are never returned).
         # This requires us to wrap fetches in `identity` ops.
         self.fetches = [tf.identity(x) for x in self.fetches]
-        self.session_kwargs = session_kwargs
+        # self.session_kwargs is used for _legacy_call
+        self.session_kwargs = session_kwargs.copy()
+        self.run_options = session_kwargs.pop('options', None)
+        self.run_metadata = session_kwargs.pop('run_metadata', None)
         if session_kwargs:
             raise ValueError('Some keys in session_kwargs are not '
                              'supported at this '
@@ -2593,6 +2596,9 @@ class Function(object):
             callable_opts.fetch.append(x.name)
         # Handle updates.
         callable_opts.target.append(self.updates_op.name)
+        # Handle run_options.
+        if self.run_options:
+            callable_opts.run_options.CopyFrom(self.run_options)
         # Create callable.
         callable_fn = session._make_callable_from_options(callable_opts)
         # Cache parameters corresponding to the generated callable, so that
@@ -2643,7 +2649,10 @@ class Function(object):
                                 feed_symbols,
                                 symbol_vals,
                                 session)
-        fetched = self._callable_fn(*array_vals)
+        if self.run_metadata:
+            fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)
+        else:
+            fetched = self._callable_fn(*array_vals)
         return fetched[:len(self.outputs)]
 
     def _legacy_call(self, inputs):
@@ -2673,6 +2682,16 @@ class Function(object):
                         'supported with sparse inputs.')
                 return self._legacy_call(inputs)
 
+            # callable generated by Session._make_callable_from_options accepts
+            # `run_metadata` keyword argument since TF 1.10
+            if (self.run_metadata and
+                    StrictVersion(tf.__version__.split('-')[0]) < StrictVersion('1.10.0')):
+                if py_any(is_tensor(x) for x in inputs):
+                    raise ValueError(
+                        'In order to feed symbolic tensors to a Keras model and set '
+                        '`run_metadata`, you need tensorflow 1.10 or higher.')
+                return self._legacy_call(inputs)
+
             return self._call(inputs)
         else:
             if py_any(is_tensor(x) for x in inputs):
"
"keras","14","98465b85d020f1326bcef7632f1261a9a7a84e92","02bc5010a04bb11c8e91835cc9775c8149dec754","keras/metrics.py","keras/metrics.py","diff --git a/keras/metrics.py b/keras/metrics.py","tests/keras/metrics_test.py","","diff --git a/keras/metrics.py b/keras/metrics.py
index a2667eaf..8e3df1f3 100644
--- a/keras/metrics.py
+++ b/keras/metrics.py
@@ -45,7 +45,8 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):
 
 
 def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):
-    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k),
+    # If the shape of y_true is (num_samples, 1), flatten to (num_samples,)
+    return K.mean(K.in_top_k(y_pred, K.cast(K.flatten(y_true), 'int32'), k),
                   axis=-1)
 
 
"
"keras","3","c0d1709cbae3d05efc6dd224230012bc120be8e5","c13d2723d01212d09dfdda39b0ad439803ec9230","keras/models.py","keras/models.py","diff --git a/keras/models.py b/keras/models.py","tests/keras/test_sequential_model.py","","diff --git a/keras/models.py b/keras/models.py
index a03b09ab..03c487dd 100644
--- a/keras/models.py
+++ b/keras/models.py
@@ -137,9 +137,12 @@ def _clone_functional_model(model, input_tensors=None):
                             kwargs['mask'] = computed_mask
                     output_tensors = to_list(
                         layer(computed_tensor, **kwargs))
-                    output_masks = to_list(
-                        layer.compute_mask(computed_tensor,
-                                           computed_mask))
+                    if layer.supports_masking:
+                        output_masks = to_list(
+                            layer.compute_mask(computed_tensor,
+                                               computed_mask))
+                    else:
+                        output_masks = [None] * len(output_tensors)
                     computed_tensors = [computed_tensor]
                     computed_masks = [computed_mask]
                 else:
@@ -150,9 +153,12 @@ def _clone_functional_model(model, input_tensors=None):
                             kwargs['mask'] = computed_masks
                     output_tensors = to_list(
                         layer(computed_tensors, **kwargs))
-                    output_masks = to_list(
-                        layer.compute_mask(computed_tensors,
-                                           computed_masks))
+                    if layer.supports_masking:
+                        output_masks = to_list(
+                            layer.compute_mask(computed_tensors,
+                                               computed_masks))
+                    else:
+                        output_masks = [None] * len(output_tensors)
                 # Update tensor_map.
                 for x, y, mask in zip(reference_output_tensors,
                                       output_tensors,
"
"keras","16","514aca20c6f076a86819d7180f36c3b2e8bcc33b","fe38f9dfc8c732a77ac03507b63c79b1d2acfba2","keras/engine/sequential.py","keras/engine/sequential.py","diff --git a/keras/engine/sequential.py b/keras/engine/sequential.py","tests/keras/test_sequential_model.py","","diff --git a/keras/engine/sequential.py b/keras/engine/sequential.py
index 262d9245..83a2b22b 100644
--- a/keras/engine/sequential.py
+++ b/keras/engine/sequential.py
@@ -85,6 +85,7 @@ class Sequential(Model):
 
     def __init__(self, layers=None, name=None):
         super(Sequential, self).__init__(name=name)
+        self._build_input_shape = None
 
         # Add to the model any layers passed to the constructor.
         if layers:
@@ -219,8 +220,7 @@ class Sequential(Model):
             for layer in self._layers:
                 x = layer(x)
             self.outputs = [x]
-            if self._layers:
-                self._layers[0].batch_input_shape = batch_shape
+            self._build_input_shape = input_shape
 
         if self.inputs:
             self._init_graph_network(self.inputs,
@@ -271,19 +271,31 @@ class Sequential(Model):
             return (proba > 0.5).astype('int32')
 
     def get_config(self):
-        config = []
+        layer_configs = []
         for layer in self.layers:
-            config.append({
+            layer_configs.append({
                 'class_name': layer.__class__.__name__,
                 'config': layer.get_config()
             })
-        return copy.deepcopy(config)
+        config = {
+            'name': self.name,
+            'layers': copy.deepcopy(layer_configs)
+        }
+        if self._build_input_shape:
+            config['build_input_shape'] = self._build_input_shape
+        return config
 
     @classmethod
     def from_config(cls, config, custom_objects=None):
-        model = cls()
-        for conf in config:
+        if 'name' in config:
+            name = config['name']
+            build_input_shape = config.get('build_input_shape')
+            layer_configs = config['layers']
+        model = cls(name=name)
+        for conf in layer_configs:
             layer = layer_module.deserialize(conf,
                                              custom_objects=custom_objects)
             model.add(layer)
+        if not model.inputs and build_input_shape:
+            model.build(build_input_shape)
         return model
"
"keras","2","2f55055a9f053b35fa721d3eb75dd07ea5a5f1e3","c24d16af155e20976bdf61e468ba760408e676ff","keras/backend/numpy_backend.py","keras/backend/numpy_backend.py","diff --git a/keras/backend/numpy_backend.py b/keras/backend/numpy_backend.py","tests/keras/backend/backend_test.py","","diff --git a/keras/backend/numpy_backend.py b/keras/backend/numpy_backend.py
index fe23567a..1e061955 100644
--- a/keras/backend/numpy_backend.py
+++ b/keras/backend/numpy_backend.py
@@ -316,6 +316,12 @@ def l2_normalize(x, axis=-1):
     return x / np.sqrt(y)
 
 
+def in_top_k(predictions, targets, k):
+    top_k = np.argsort(-predictions)[:, :k]
+    targets = targets.reshape(-1, 1)
+    return np.any(targets == top_k, axis=-1)
+
+
 def binary_crossentropy(target, output, from_logits=False):
     if not from_logits:
         output = np.clip(output, 1e-7, 1 - 1e-7)
"
"keras","38","53ec990d54130dd0a457dd235c93d39de32d571d","64f80d6077edd5f277a1181df94bf4510ea0517a","keras/layers/recurrent.py","keras/layers/recurrent.py","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py","tests/keras/layers/recurrent_test.py","","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py
index 59046d78..89983d8e 100644
--- a/keras/layers/recurrent.py
+++ b/keras/layers/recurrent.py
@@ -106,7 +106,7 @@ class StackedRNNCells(Layer):
                 output_dim = cell.state_size[0]
             else:
                 output_dim = cell.state_size
-            input_shape = (input_shape[0], input_shape[1], output_dim)
+            input_shape = (input_shape[0], output_dim)
         self.built = True
 
     def get_config(self):
"
"keras","8","87540a2a2f42e00c4a2ca7ca35d19f96e62e6cb0","d78c982b326adeed6ac25200dc6892ff8f518ca6","keras/engine/network.py","keras/engine/network.py","diff --git a/keras/engine/network.py b/keras/engine/network.py","tests/keras/engine/test_topology.py","","diff --git a/keras/engine/network.py b/keras/engine/network.py
index 7d36df7a..e5daf0bb 100644
--- a/keras/engine/network.py
+++ b/keras/engine/network.py
@@ -958,12 +958,28 @@ class Network(Layer):
         unprocessed_nodes = {}
 
         def add_unprocessed_node(layer, node_data):
+            """"""Add node to layer list
+
+            Args:
+                layer: layer object
+                node_data: Node data specifying layer call
+            """"""
             if layer not in unprocessed_nodes:
                 unprocessed_nodes[layer] = [node_data]
             else:
                 unprocessed_nodes[layer].append(node_data)
 
         def process_node(layer, node_data):
+            """"""Reconstruct node by linking to inbound layers
+
+            Args:
+                layer: Layer to process
+                node_data: List of layer configs
+
+            Raises:
+                ValueError: For incorrect layer config
+                LookupError: If layer required is not found
+            """"""
             input_tensors = []
             for input_data in node_data:
                 inbound_layer_name = input_data[0]
@@ -976,12 +992,14 @@ class Network(Layer):
                 else:
                     raise ValueError('Improperly formatted model config.')
                 inbound_layer = created_layers[inbound_layer_name]
+                # Raise an error if the corresponding layer node
+                # has not yet been created
                 if len(inbound_layer._inbound_nodes) <= inbound_node_index:
-                    add_unprocessed_node(layer, node_data)
-                    return
+                    raise LookupError
                 inbound_node = inbound_layer._inbound_nodes[inbound_node_index]
                 input_tensors.append(
                     inbound_node.output_tensors[inbound_tensor_index])
+
             # Call layer on its inputs, thus creating the node
             # and building the layer if needed.
             if input_tensors:
@@ -1017,6 +1035,7 @@ class Network(Layer):
         # First, we create all layers and enqueue nodes to be processed
         for layer_data in config['layers']:
             process_layer(layer_data)
+
         # Then we process nodes in order of layer depth.
         # Nodes that cannot yet be processed (if the inbound node
         # does not yet exist) are re-enqueued, and the process
@@ -1024,10 +1043,33 @@ class Network(Layer):
         while unprocessed_nodes:
             for layer_data in config['layers']:
                 layer = created_layers[layer_data['name']]
+
+                # Process all nodes in layer, if not yet processed
                 if layer in unprocessed_nodes:
-                    for node_data in unprocessed_nodes.pop(layer):
-                        process_node(layer, node_data)
+                    node_data_list = unprocessed_nodes[layer]
+
+                    # Process nodes in order
+                    node_index = 0
+                    while node_index < len(node_data_list):
+                        node_data = node_data_list[node_index]
+                        try:
+                            process_node(layer, node_data)
+
+                        # If the node does not have all inbound layers
+                        # available, stop processing and continue later
+                        except LookupError:
+                            break
+
+                        node_index += 1
+
+                    # If not all nodes processed then store unprocessed nodes
+                    if node_index < len(node_data_list):
+                        unprocessed_nodes[layer] = node_data_list[node_index:]
+                    # If all nodes processed remove the layer
+                    else:
+                        del unprocessed_nodes[layer]
 
+        # Create lits of input and output tensors and return new class
         name = config.get('name')
         input_tensors = []
         output_tensors = []
"
"keras","23","3dcd9c767ce6875fc8b69c74971ac8a552e23131","69c30a150f0b2caee7961ca1c0080960ef5ad6f6","keras/engine/sequential.py","keras/engine/sequential.py","diff --git a/keras/engine/sequential.py b/keras/engine/sequential.py","tests/keras/test_sequential_model.py","","diff --git a/keras/engine/sequential.py b/keras/engine/sequential.py
index 37d19d0b..6def870b 100644
--- a/keras/engine/sequential.py
+++ b/keras/engine/sequential.py
@@ -149,8 +149,6 @@ class Sequential(Model):
                     first_layer = layer.layers[0]
                     while isinstance(first_layer, (Model, Sequential)):
                         first_layer = first_layer.layers[0]
-                    batch_shape = first_layer.batch_input_shape
-                    dtype = first_layer.dtype
 
                 if hasattr(first_layer, 'batch_input_shape'):
                     batch_shape = first_layer.batch_input_shape
"
"keras","1","331d5b0102ab0cc79cece1f03cc551d8105db3c9","8e23a3ec47a2ccbf6cdd222a80886c6b9f17264f","keras/backend/tensorflow_backend.py;keras/backend/theano_backend.py;keras/initializers.py;tests/keras/backend/backend_test.py","keras/backend/tensorflow_backend.py;keras/backend/theano_backend.py;keras/initializers.py;tests/keras/backend/backend_test.py","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py;diff --git a/keras/backend/theano_backend.py b/keras/backend/theano_backend.py;diff --git a/keras/initializers.py b/keras/initializers.py;diff --git a/tests/keras/backend/backend_test.py b/tests/keras/backend/backend_test.py","tests/keras/initializers_test.py","","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py
index 408a9749..94272215 100644
--- a/keras/backend/tensorflow_backend.py
+++ b/keras/backend/tensorflow_backend.py
@@ -14,6 +14,7 @@ from tensorflow.python.ops import functional_ops
 from tensorflow.python.ops import ctc_ops as ctc
 from .common import floatx, epsilon, image_data_format
 
+import sys
 import functools
 import threading
 
@@ -1203,7 +1204,9 @@ def update(x, new_x):
     # Returns
         The variable `x` updated.
     """"""
-    return tf_state_ops.assign(x, new_x)
+    op = tf_state_ops.assign(x, new_x)
+    with tf.control_dependencies([op]):
+        return tf.identity(x)
 
 
 @symbolic
@@ -1217,7 +1220,9 @@ def update_add(x, increment):
     # Returns
         The variable `x` updated.
     """"""
-    return tf_state_ops.assign_add(x, increment)
+    op = tf_state_ops.assign_add(x, increment)
+    with tf.control_dependencies([op]):
+        return tf.identity(x)
 
 
 @symbolic
@@ -1231,7 +1236,9 @@ def update_sub(x, decrement):
     # Returns
         The variable `x` updated.
     """"""
-    return tf_state_ops.assign_sub(x, decrement)
+    op = tf_state_ops.assign_sub(x, decrement)
+    with tf.control_dependencies([op]):
+        return tf.identity(x)
 
 
 @symbolic
@@ -2880,6 +2887,7 @@ def get_variable_shape(x):
     return int_shape(x)
 
 
+@symbolic
 def print_tensor(x, message=''):
     """"""Prints `message` and the tensor value when evaluated.
 
@@ -2899,8 +2907,9 @@ def print_tensor(x, message=''):
     # Returns
         The same tensor `x`, unchanged.
     """"""
-    # TODO
-    return tf.Print(x, [x], message)
+    op = tf.print(message, x, output_stream=sys.stdout)
+    with tf.control_dependencies([op]):
+        return tf.identity(x)
 
 
 # GRAPH MANIPULATION
diff --git a/keras/backend/theano_backend.py b/keras/backend/theano_backend.py
index 141c6b9c..1f2789e9 100644
--- a/keras/backend/theano_backend.py
+++ b/keras/backend/theano_backend.py
@@ -1378,8 +1378,7 @@ def get_variable_shape(x):
 
 
 def print_tensor(x, message=''):
-    """"""Print the message and the tensor when evaluated and return the same
-    tensor.
+    """"""Print the message & the tensor when evaluated & return the same tensor.
     """"""
     p_op = Print(message)
     return p_op(x)
diff --git a/keras/initializers.py b/keras/initializers.py
index aea81b5c..eff00482 100644
--- a/keras/initializers.py
+++ b/keras/initializers.py
@@ -80,8 +80,11 @@ class RandomNormal(Initializer):
         self.seed = seed
 
     def __call__(self, shape, dtype=None):
-        return K.random_normal(shape, self.mean, self.stddev,
-                               dtype=dtype, seed=self.seed)
+        x = K.random_normal(shape, self.mean, self.stddev,
+                            dtype=dtype, seed=self.seed)
+        if self.seed is not None:
+            self.seed += 1
+        return x
 
     def get_config(self):
         return {
@@ -108,8 +111,11 @@ class RandomUniform(Initializer):
         self.seed = seed
 
     def __call__(self, shape, dtype=None):
-        return K.random_uniform(shape, self.minval, self.maxval,
-                                dtype=dtype, seed=self.seed)
+        x = K.random_uniform(shape, self.minval, self.maxval,
+                             dtype=dtype, seed=self.seed)
+        if self.seed is not None:
+            self.seed += 1
+        return x
 
     def get_config(self):
         return {
@@ -141,8 +147,11 @@ class TruncatedNormal(Initializer):
         self.seed = seed
 
     def __call__(self, shape, dtype=None):
-        return K.truncated_normal(shape, self.mean, self.stddev,
-                                  dtype=dtype, seed=self.seed)
+        x = K.truncated_normal(shape, self.mean, self.stddev,
+                               dtype=dtype, seed=self.seed)
+        if self.seed is not None:
+            self.seed += 1
+        return x
 
     def get_config(self):
         return {
@@ -210,12 +219,15 @@ class VarianceScaling(Initializer):
         if self.distribution == 'normal':
             # 0.879... = scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)
             stddev = np.sqrt(scale) / .87962566103423978
-            return K.truncated_normal(shape, 0., stddev,
-                                      dtype=dtype, seed=self.seed)
+            x = K.truncated_normal(shape, 0., stddev,
+                                   dtype=dtype, seed=self.seed)
         else:
             limit = np.sqrt(3. * scale)
-            return K.random_uniform(shape, -limit, limit,
-                                    dtype=dtype, seed=self.seed)
+            x = K.random_uniform(shape, -limit, limit,
+                                 dtype=dtype, seed=self.seed)
+        if self.seed is not None:
+            self.seed += 1
+        return x
 
     def get_config(self):
         return {
@@ -251,6 +263,7 @@ class Orthogonal(Initializer):
         rng = np.random
         if self.seed is not None:
             rng = np.random.RandomState(self.seed)
+            self.seed += 1
         a = rng.normal(0.0, 1.0, flat_shape)
         u, _, v = np.linalg.svd(a, full_matrices=False)
         # Pick the one with the correct shape.
diff --git a/tests/keras/backend/backend_test.py b/tests/keras/backend/backend_test.py
index 8d3a35d5..8e0af02e 100644
--- a/tests/keras/backend/backend_test.py
+++ b/tests/keras/backend/backend_test.py
@@ -480,11 +480,18 @@ class TestBackend(object):
         else:
             assert_list_pairwise(v_list, shape=False, allclose=False, itself=True)
 
-    def test_print_tensor(self):
+    def test_print_tensor(self, capsys):
+        for k in [KTH, KTF]:
+            x = k.placeholder((1, 1))
+            y = k.print_tensor(x, 'msg')
+            fn = k.function([x], [y])
+            _ = fn([np.ones((1, 1))])
+            out, err = capsys.readouterr()
+            # Theano inserts ""__str__ = "" for no good reason
+            assert out.replace('__str__ = ', '') == 'msg [[1.]]\n'
+
         check_single_tensor_operation('print_tensor', (), WITH_NP)
         check_single_tensor_operation('print_tensor', (2,), WITH_NP)
-        check_single_tensor_operation('print_tensor', (4, 3), WITH_NP)
-        check_single_tensor_operation('print_tensor', (1, 2, 3), WITH_NP)
 
     def test_elementwise_operations(self):
         check_single_tensor_operation('max', (4, 2), WITH_NP)
@@ -579,29 +586,41 @@ class TestBackend(object):
     def test_log(self):
         check_single_tensor_operation('log', (4, 2), WITH_NP)
 
+    @pytest.mark.skipif(K.backend() == 'theano',
+                        reason='theano returns tuples for update ops')
+    def test_update(self):
+        x = np.ones((3, 4))
+        x_var = K.variable(x)
+        new_x = np.random.random((3, 4))
+
+        op = K.update(x_var, new_x)
+        K.eval(op)
+
+        assert_allclose(new_x, K.eval(x_var), atol=1e-05)
+
     @pytest.mark.skipif(K.backend() == 'theano',
                         reason='theano returns tuples for update ops')
     def test_update_add(self):
-        x = np.random.randn(3, 4)
+        x = np.ones((3, 4))
         x_var = K.variable(x)
-        increment = np.random.randn(3, 4)
+        increment = np.random.random((3, 4))
 
-        x += increment
-        K.eval(K.update_add(x_var, increment))
+        op = K.update_add(x_var, increment)
+        K.eval(op)
 
-        assert_allclose(x, K.eval(x_var), atol=1e-05)
+        assert_allclose(x + increment, K.eval(x_var), atol=1e-05)
 
     @pytest.mark.skipif(K.backend() == 'theano',
                         reason='theano returns tuples for update ops')
     def test_update_sub(self):
-        x = np.random.randn(3, 4)
+        x = np.ones((3, 4))
         x_var = K.variable(x)
-        decrement = np.random.randn(3, 4)
+        decrement = np.random.random((3, 4))
 
-        x -= decrement
-        K.eval(K.update_sub(x_var, decrement))
+        op = K.update_sub(x_var, decrement)
+        K.eval(op)
 
-        assert_allclose(x, K.eval(x_var), atol=1e-05)
+        assert_allclose(x - decrement, K.eval(x_var), atol=1e-05)
 
     @pytest.mark.skipif(K.backend() == 'cntk',
                         reason='cntk doesn\'t support gradient in this way.')
@@ -712,7 +731,7 @@ class TestBackend(object):
         assert output == [21.]
         assert K.get_session().run(fetches=[x, y]) == [30., 40.]
 
-    @pytest.mark.skipif(K.backend() != 'tensorflow',
+    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),
                         reason='Uses the `options` and `run_metadata` arguments.')
     def test_function_tf_run_options_with_run_metadata(self):
         from tensorflow.core.protobuf import config_pb2
@@ -1363,58 +1382,41 @@ class TestBackend(object):
         assert_allclose(y1, y2, atol=1e-05)
 
     def test_random_normal(self):
-        # test standard normal as well as a normal with a different set of parameters
+        # TODO: make this a parameterized test
         for mean, std in [(0., 1.), (-10., 5.)]:
-            rand = K.eval(K.random_normal((300, 200),
-                                          mean=mean, stddev=std, seed=1337))
-            assert rand.shape == (300, 200)
+            rand = K.eval(K.random_normal((200, 200),
+                                          mean=mean,
+                                          stddev=std))
+            assert rand.shape == (200, 200)
             assert np.abs(np.mean(rand) - mean) < std * 0.015
             assert np.abs(np.std(rand) - std) < std * 0.015
 
-            # test that random_normal also generates different values when used
-            # within a function
-            r = K.random_normal((10, 10), mean=mean, stddev=std, seed=1337)
-            samples = np.array([K.eval(r) for _ in range(200)])
-            assert np.abs(np.mean(samples) - mean) < std * 0.015
-            assert np.abs(np.std(samples) - std) < std * 0.015
-
     def test_random_uniform(self):
         min_val = -1.
         max_val = 1.
-        rand = K.eval(K.random_uniform((200, 100), min_val, max_val))
-        assert rand.shape == (200, 100)
+        rand = K.eval(K.random_uniform((200, 200), min_val, max_val))
+        assert rand.shape == (200, 200)
         assert np.abs(np.mean(rand)) < 0.015
         assert max_val - 0.015 < np.max(rand) <= max_val
         assert min_val + 0.015 > np.min(rand) >= min_val
 
-        r = K.random_uniform((10, 10), minval=min_val, maxval=max_val)
-        samples = np.array([K.eval(r) for _ in range(200)])
-        assert np.abs(np.mean(samples)) < 0.015
-        assert max_val - 0.015 < np.max(samples) <= max_val
-        assert min_val + 0.015 > np.min(samples) >= min_val
-
     def test_random_binomial(self):
         p = 0.5
-        rand = K.eval(K.random_binomial((200, 100), p))
-        assert rand.shape == (200, 100)
+        rand = K.eval(K.random_binomial((200, 200), p))
+        assert rand.shape == (200, 200)
         assert np.abs(np.mean(rand) - p) < 0.015
         assert np.max(rand) == 1
         assert np.min(rand) == 0
 
-        r = K.random_binomial((10, 10), p)
-        samples = np.array([K.eval(r) for _ in range(200)])
-        assert np.abs(np.mean(samples) - p) < 0.015
-        assert np.max(samples) == 1
-        assert np.min(samples) == 0
-
     def test_truncated_normal(self):
         mean = 0.
         std = 1.
         min_val = -2.
         max_val = 2.
-        rand = K.eval(K.truncated_normal((300, 200),
-                                         mean=mean, stddev=std, seed=1337))
-        assert rand.shape == (300, 200)
+        rand = K.eval(K.truncated_normal((200, 200),
+                                         mean=mean,
+                                         stddev=std))
+        assert rand.shape == (200, 200)
         assert np.abs(np.mean(rand) - mean) < 0.015
         assert np.max(rand) <= max_val
         assert np.min(rand) >= min_val
@@ -2122,16 +2124,6 @@ class TestBackend(object):
                            np.asarray([-5., -4., 0., 4., 9.],
                                       dtype=np.float32))
 
-    @pytest.mark.skipif(K.backend() != 'tensorflow' or KTF._is_tf_1(),
-                        reason='This test is for tensorflow parallelism.')
-    def test_tensorflow_session_parallelism_settings(self, monkeypatch):
-        for threads in [1, 2]:
-            K.clear_session()
-            monkeypatch.setenv('OMP_NUM_THREADS', str(threads))
-            cfg = K.get_session()._config
-            assert cfg.intra_op_parallelism_threads == threads
-            assert cfg.inter_op_parallelism_threads == threads
-
 
 if __name__ == '__main__':
     pytest.main([__file__])
"
"keras","31","ced81968b0e9d8b1389e6580721ac60d9cf3ca60","e2a10a5e6e156a45e946c4d08db7133f997c1f9a","keras/backend/tensorflow_backend.py","keras/backend/tensorflow_backend.py","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py","tests/keras/backend/backend_test.py","","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py
index f145d43d..0e313bc5 100644
--- a/keras/backend/tensorflow_backend.py
+++ b/keras/backend/tensorflow_backend.py
@@ -3942,8 +3942,8 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):
         Tensor with shape (samples,1) containing the
             CTC loss of each element.
     """"""
-    label_length = tf.to_int32(tf.squeeze(label_length))
-    input_length = tf.to_int32(tf.squeeze(input_length))
+    label_length = tf.to_int32(tf.squeeze(label_length, axis=-1))
+    input_length = tf.to_int32(tf.squeeze(input_length, axis=-1))
     sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))
 
     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())
"
"keras","43","e3e97c401aa8251e957b73fba8ed4d108c106f12","b17169ca5d6cd1c8aeb237fc2bb0555c9e1b6a02","keras/utils/np_utils.py","keras/utils/np_utils.py","diff --git a/keras/utils/np_utils.py b/keras/utils/np_utils.py","tests/keras/utils/np_utils_test.py","","diff --git a/keras/utils/np_utils.py b/keras/utils/np_utils.py
index 7c3bccf4..572d0e5d 100644
--- a/keras/utils/np_utils.py
+++ b/keras/utils/np_utils.py
@@ -19,6 +19,8 @@ def to_categorical(y, num_classes=None):
     """"""
     y = np.array(y, dtype='int')
     input_shape = y.shape
+    if input_shape and input_shape[-1] == 1:
+        input_shape = tuple(input_shape[:-1])
     y = y.ravel()
     if not num_classes:
         num_classes = np.max(y) + 1
"
"keras","39","3a431ea52d090fb3ef8a1e0e5d7f796d9a42e097","a5ecde595c47f35fd7293d52eba48efd687ca94e","keras/utils/generic_utils.py","keras/utils/generic_utils.py","diff --git a/keras/utils/generic_utils.py b/keras/utils/generic_utils.py","tests/keras/utils/generic_utils_test.py","","diff --git a/keras/utils/generic_utils.py b/keras/utils/generic_utils.py
index 667e93f9..56e329e5 100644
--- a/keras/utils/generic_utils.py
+++ b/keras/utils/generic_utils.py
@@ -327,7 +327,7 @@ class Progbar(object):
         info = ' - %.0fs' % (now - self.start)
         if self.verbose == 1:
             if (not force and (now - self.last_update) < self.interval and
-                    current < self.target):
+                    (self.target is not None and current < self.target)):
                 return
 
             prev_total_width = self.total_width
"
"keras","44","cc08f0f01fe97a9659e3da8fa9b290a54992c74a","3292aa5a30350c67627f173ceac713956f68271f","keras/layers/recurrent.py","keras/layers/recurrent.py","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py","tests/keras/layers/recurrent_test.py","","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py
index 136fd3c8..8b2d8827 100644
--- a/keras/layers/recurrent.py
+++ b/keras/layers/recurrent.py
@@ -731,6 +731,8 @@ class RNN(Layer):
 
     @property
     def trainable_weights(self):
+        if not self.trainable:
+            return []
         if isinstance(self.cell, Layer):
             return self.cell.trainable_weights
         return []
@@ -738,6 +740,8 @@ class RNN(Layer):
     @property
     def non_trainable_weights(self):
         if isinstance(self.cell, Layer):
+            if not self.trainable:
+                return self.cell.weights
             return self.cell.non_trainable_weights
         return []
 
"
"keras","7","26b620fb37c885d60183f83abc744f43775ce75a","c05ef1fd95a6024155ab59656fef8dac5a45c335","keras/wrappers/scikit_learn.py","keras/wrappers/scikit_learn.py","diff --git a/keras/wrappers/scikit_learn.py b/keras/wrappers/scikit_learn.py","tests/keras/wrappers/scikit_learn_test.py","","diff --git a/keras/wrappers/scikit_learn.py b/keras/wrappers/scikit_learn.py
index 6ebf6fc8..83c3e3c4 100644
--- a/keras/wrappers/scikit_learn.py
+++ b/keras/wrappers/scikit_learn.py
@@ -320,7 +320,7 @@ class KerasRegressor(BaseWrapper):
                 Predictions.
         """"""
         kwargs = self.filter_sk_params(Sequential.predict, kwargs)
-        return np.squeeze(self.model.predict(x, **kwargs))
+        return np.squeeze(self.model.predict(x, **kwargs), axis=-1)
 
     def score(self, x, y, **kwargs):
         """"""Returns the mean loss on the given test data and labels.
"
"keras","25","b470a595f7278acf5e7e47521edf25d3c4f479f1","84e168b5fa55933e02e767ff7c86fcc0232aecc6","keras/applications/imagenet_utils.py","keras/applications/imagenet_utils.py","diff --git a/keras/applications/imagenet_utils.py b/keras/applications/imagenet_utils.py","tests/keras/applications/imagenet_utils_test.py","","diff --git a/keras/applications/imagenet_utils.py b/keras/applications/imagenet_utils.py
index cfdd1098..d6a3c23d 100644
--- a/keras/applications/imagenet_utils.py
+++ b/keras/applications/imagenet_utils.py
@@ -38,6 +38,8 @@ def _preprocess_numpy_input(x, data_format, mode):
     # Returns
         Preprocessed Numpy array.
     """"""
+    x = x.astype(K.floatx())
+
     if mode == 'tf':
         x /= 127.5
         x -= 1.
"
"keras","10","8f41e41eda6e8ea96403cae5798a5a89c8bb5605","c1c4afe60b1355a6c0e83577791a0423f37a3324","keras/engine/training_utils.py","keras/engine/training_utils.py","diff --git a/keras/engine/training_utils.py b/keras/engine/training_utils.py","tests/keras/engine/test_training.py","","diff --git a/keras/engine/training_utils.py b/keras/engine/training_utils.py
index e8116397..d1e3f9ba 100644
--- a/keras/engine/training_utils.py
+++ b/keras/engine/training_utils.py
@@ -432,7 +432,8 @@ def standardize_weights(y,
     """"""Performs sample weight validation and standardization.
 
     Everything gets normalized to a single sample-wise (or timestep-wise)
-    weight array.
+    weight array. If both `sample_weights` and `class_weights` are provided,
+    the weights are multiplied together.
 
     # Arguments
         y: Numpy array of model targets to be weighted.
@@ -478,10 +479,6 @@ def standardize_weights(y,
                              'sample-wise weights, make sure your '
                              'sample_weight array is 1D.')
 
-    if sample_weight is not None and class_weight is not None:
-        warnings.warn('Found both `sample_weight` and `class_weight`: '
-                      '`class_weight` argument will be ignored.')
-
     if sample_weight is not None:
         if len(sample_weight.shape) > len(y.shape):
             raise ValueError('Found a sample_weight with shape' +
@@ -495,22 +492,24 @@ def standardize_weights(y,
                              ' for an input with shape ' +
                              str(y.shape) + '. '
                              'sample_weight cannot be broadcast.')
-        return sample_weight
-    elif isinstance(class_weight, dict):
+
+    class_sample_weight = None
+    if isinstance(class_weight, dict):
         if len(y.shape) > 2:
             raise ValueError('`class_weight` not supported for '
                              '3+ dimensional targets.')
-        if y.shape[1] > 1:
-            y_classes = np.argmax(y, axis=1)
-        elif y.shape[1] == 1:
-            y_classes = np.reshape(y, y.shape[0])
+        if len(y.shape) == 2:
+            if y.shape[1] > 1:
+                y_classes = np.argmax(y, axis=1)
+            elif y.shape[1] == 1:
+                y_classes = np.reshape(y, y.shape[0])
         else:
             y_classes = y
 
-        weights = np.asarray([class_weight[cls] for cls in y_classes
-                              if cls in class_weight])
+        class_sample_weight = np.asarray(
+            [class_weight[cls] for cls in y_classes if cls in class_weight])
 
-        if len(weights) != len(y_classes):
+        if len(class_sample_weight) != len(y_classes):
             # subtract the sets to pick all missing classes
             existing_classes = set(y_classes)
             existing_class_weight = set(class_weight.keys())
@@ -519,12 +518,19 @@ def standardize_weights(y,
                              ' The classes %s exist in the data but not in '
                              '`class_weight`.'
                              % (existing_classes - existing_class_weight))
-        return weights
+
+    if sample_weight is not None and class_sample_weight is not None:
+        return sample_weight * class_sample_weight
+    if sample_weight is not None:
+        return sample_weight
+    if class_sample_weight is not None:
+        return class_sample_weight
+
+    # Everything has weight 1 by default.
+    if sample_weight_mode is None:
+        return np.ones((y.shape[0],), dtype=K.floatx())
     else:
-        if sample_weight_mode is None:
-            return np.ones((y.shape[0],), dtype=K.floatx())
-        else:
-            return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())
+        return np.ones((y.shape[0], y.shape[1]), dtype=K.floatx())
 
 
 def check_num_samples(ins,
"
"keras","13","2bfd1f2c950df5fc3f40b903c1966f1b0a48bee4","a07253d8269e1b750f0a64767cc9a07da8a3b7ea","keras/engine/training_generator.py","keras/engine/training_generator.py","diff --git a/keras/engine/training_generator.py b/keras/engine/training_generator.py","tests/keras/engine/test_training.py","","diff --git a/keras/engine/training_generator.py b/keras/engine/training_generator.py
index 5c5697c5..3c480f22 100644
--- a/keras/engine/training_generator.py
+++ b/keras/engine/training_generator.py
@@ -124,7 +124,8 @@ def fit_generator(model,
             elif val_gen:
                 val_data = validation_data
                 if isinstance(val_data, Sequence):
-                    val_enqueuer_gen = iter_sequence_infinite(generator)
+                    val_enqueuer_gen = iter_sequence_infinite(val_data)
+                    validation_steps = validation_steps or len(val_data)
                 else:
                     val_enqueuer_gen = val_data
             else:
"
"keras","41","a27b4a51f4880ad3a7669531b667c1ef44b173ef","4a58b178073f0ba3b166220f7ebd7d56149bfb20","keras/utils/data_utils.py;tests/test_multiprocessing.py","keras/utils/data_utils.py;tests/test_multiprocessing.py","diff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py;diff --git a/tests/test_multiprocessing.py b/tests/test_multiprocessing.py","tests/keras/utils/data_utils_test.py","","diff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py
index ce1a60ac..bc0d87ce 100644
--- a/keras/utils/data_utils.py
+++ b/keras/utils/data_utils.py
@@ -11,6 +11,7 @@ import sys
 import tarfile
 import threading
 import time
+import traceback
 import zipfile
 from abc import abstractmethod
 from multiprocessing.pool import ThreadPool
@@ -553,7 +554,7 @@ class OrderedEnqueuer(SequenceEnqueuer):
                     yield inputs
         except Exception as e:
             self.stop()
-            raise StopIteration(e)
+            six.raise_from(StopIteration(e), e)
 
     def _send_sequence(self):
         """"""Send current Sequence to all workers.""""""
@@ -614,6 +615,7 @@ class GeneratorEnqueuer(SequenceEnqueuer):
         self._use_multiprocessing = use_multiprocessing
         self._threads = []
         self._stop_event = None
+        self._manager = None
         self.queue = None
         self.seed = seed
 
@@ -631,18 +633,27 @@ class GeneratorEnqueuer(SequenceEnqueuer):
                 try:
                     if self._use_multiprocessing or self.queue.qsize() < max_queue_size:
                         generator_output = next(self._generator)
-                        self.queue.put(generator_output)
+                        self.queue.put((True, generator_output))
                     else:
                         time.sleep(self.wait_time)
                 except StopIteration:
                     break
-                except Exception:
+                except Exception as e:
+                    # Can't pick tracebacks.
+                    # As a compromise, print the traceback and pickle None instead.
+                    if self._use_multiprocessing:
+                        traceback.print_exc()
+                        setattr(e, '__traceback__', None)
+                    elif not hasattr(e, '__traceback__'):
+                        setattr(e, '__traceback__', sys.exc_info()[2])
+                    self.queue.put((False, e))
                     self._stop_event.set()
-                    raise
+                    break
 
         try:
             if self._use_multiprocessing:
-                self.queue = multiprocessing.Queue(maxsize=max_queue_size)
+                self._manager = multiprocessing.Manager()
+                self.queue = self._manager.Queue(maxsize=max_queue_size)
                 self._stop_event = multiprocessing.Event()
             else:
                 self.queue = queue.Queue()
@@ -686,9 +697,8 @@ class GeneratorEnqueuer(SequenceEnqueuer):
                 else:
                     thread.join(timeout)
 
-        if self._use_multiprocessing:
-            if self.queue is not None:
-                self.queue.close()
+        if self._manager:
+            self._manager.shutdown()
 
         self._threads = []
         self._stop_event = None
@@ -704,12 +714,22 @@ class GeneratorEnqueuer(SequenceEnqueuer):
         """"""
         while self.is_running():
             if not self.queue.empty():
-                inputs = self.queue.get()
-                if inputs is not None:
-                    yield inputs
+                success, value = self.queue.get()
+                # Rethrow any exceptions found in the queue
+                if not success:
+                    six.reraise(value.__class__, value, value.__traceback__)
+                # Yield regular values
+                if value is not None:
+                    yield value
             else:
                 all_finished = all([not thread.is_alive() for thread in self._threads])
                 if all_finished and self.queue.empty():
                     raise StopIteration()
                 else:
                     time.sleep(self.wait_time)
+
+        # Make sure to rethrow the first exception in the queue, if any
+        while not self.queue.empty():
+            success, value = self.queue.get()
+            if not success:
+                six.reraise(value.__class__, value, value.__traceback__)
diff --git a/tests/test_multiprocessing.py b/tests/test_multiprocessing.py
index 235c26a2..9b7a1632 100644
--- a/tests/test_multiprocessing.py
+++ b/tests/test_multiprocessing.py
@@ -232,7 +232,7 @@ def test_multiprocessing_fit_error():
         """"""Raises an exception after a few good batches""""""
         for i in range(good_batches):
             yield (np.random.randint(batch_size, 256, (50, 2)),
-                   np.random.randint(batch_size, 2, 50))
+                   np.random.randint(batch_size, 12, 50))
         raise RuntimeError
 
     model = Sequential()
@@ -241,13 +241,13 @@ def test_multiprocessing_fit_error():
 
     samples = batch_size * (good_batches + 1)
 
-    with pytest.raises(StopIteration):
+    with pytest.raises(RuntimeError):
         model.fit_generator(
             custom_generator(), samples, 1,
             workers=4, use_multiprocessing=True,
         )
 
-    with pytest.raises(StopIteration):
+    with pytest.raises(RuntimeError):
         model.fit_generator(
             custom_generator(), samples, 1,
             use_multiprocessing=False,
@@ -258,25 +258,26 @@ def test_multiprocessing_fit_error():
 def test_multiprocessing_evaluate_error():
     batch_size = 10
     good_batches = 3
+    workers = 4
 
     def custom_generator():
         """"""Raises an exception after a few good batches""""""
         for i in range(good_batches):
             yield (np.random.randint(batch_size, 256, (50, 2)),
-                   np.random.randint(batch_size, 2, 50))
+                   np.random.randint(batch_size, 12, 50))
         raise RuntimeError
 
     model = Sequential()
     model.add(Dense(1, input_shape=(2, )))
     model.compile(loss='mse', optimizer='adadelta')
 
-    with pytest.raises(StopIteration):
+    with pytest.raises(RuntimeError):
         model.evaluate_generator(
-            custom_generator(), good_batches + 1, 1,
-            workers=4, use_multiprocessing=True,
+            custom_generator(), good_batches * workers + 1, 1,
+            workers=workers, use_multiprocessing=True,
         )
 
-    with pytest.raises(StopIteration):
+    with pytest.raises(RuntimeError):
         model.evaluate_generator(
             custom_generator(), good_batches + 1, 1,
             use_multiprocessing=False,
@@ -299,13 +300,13 @@ def test_multiprocessing_predict_error():
     model.add(Dense(1, input_shape=(5,)))
     model.compile(loss='mse', optimizer='adadelta')
 
-    with pytest.raises(StopIteration):
+    with pytest.raises(RuntimeError):
         model.predict_generator(
             custom_generator(), good_batches * workers + 1, 1,
             workers=workers, use_multiprocessing=True,
         )
 
-    with pytest.raises(StopIteration):
+    with pytest.raises(RuntimeError):
         model.predict_generator(
             custom_generator(), good_batches + 1, 1,
             use_multiprocessing=False,
"
"keras","42","67a432c273cbd65866b1d2cb1e2c62714b633b6e","2f3edf96078d78450b985bdf3bfffe7e0c627169","keras/engine/training.py;keras/models.py","keras/engine/training.py;keras/models.py","diff --git a/keras/engine/training.py b/keras/engine/training.py;diff --git a/keras/models.py b/keras/models.py","tests/keras/engine/test_training.py","","diff --git a/keras/engine/training.py b/keras/engine/training.py
index 6cd7d35f..b87874f9 100644
--- a/keras/engine/training.py
+++ b/keras/engine/training.py
@@ -1900,7 +1900,7 @@ class Model(Container):
     @interfaces.legacy_generator_methods_support
     def fit_generator(self,
                       generator,
-                      steps_per_epoch,
+                      steps_per_epoch=None,
                       epochs=1,
                       verbose=1,
                       callbacks=None,
@@ -1941,7 +1941,9 @@ class Model(Container):
                 to yield from `generator` before declaring one epoch
                 finished and starting the next epoch. It should typically
                 be equal to the number of samples of your dataset
-                divided by the batch size. Not used if using `Sequence`.
+                divided by the batch size.
+                Optional for `Sequence`: if unspecified, will use
+                the `len(generator)` as a number of steps.
             epochs: Integer, total number of iterations on the data.
             verbose: Verbosity mode, 0, 1, or 2.
             callbacks: List of callbacks to be called during training.
@@ -1952,6 +1954,8 @@ class Model(Container):
             validation_steps: Only relevant if `validation_data`
                 is a generator. Total number of steps (batches of samples)
                 to yield from `generator` before stopping.
+                Optional for `Sequence`: if unspecified, will use
+                the `len(validation_data)` as a number of steps.
             class_weight: Dictionary mapping class indices to a weight
                 for the class.
             max_queue_size: Integer. Maximum size for the generator queue.
@@ -2005,15 +2009,33 @@ class Model(Container):
         if do_validation:
             self._make_test_function()
 
+        is_sequence = isinstance(generator, Sequence)
+        if not is_sequence and use_multiprocessing and workers > 1:
+            warnings.warn(
+                UserWarning('Using a generator with `use_multiprocessing=True`'
+                            ' and multiple workers may duplicate your data.'
+                            ' Please consider using the`keras.utils.Sequence'
+                            ' class.'))
+        if steps_per_epoch is None:
+            if is_sequence:
+                steps_per_epoch = len(generator)
+            else:
+                raise ValueError('`steps_per_epoch=None` is only valid for a'
+                                 ' generator based on the `keras.utils.Sequence`'
+                                 ' class. Please specify `steps_per_epoch` or use'
+                                 ' the `keras.utils.Sequence` class.')
+
         # python 2 has 'next', 3 has '__next__'
         # avoid any explicit version checks
         val_gen = (hasattr(validation_data, 'next') or
                    hasattr(validation_data, '__next__') or
                    isinstance(validation_data, Sequence))
-        if val_gen and not validation_steps:
-            raise ValueError('When using a generator for validation data, '
-                             'you must specify a value for '
-                             '`validation_steps`.')
+        if (val_gen and not isinstance(validation_data, Sequence) and
+                not validation_steps):
+            raise ValueError('`validation_steps=None` is only valid for a'
+                             ' generator based on the `keras.utils.Sequence`'
+                             ' class. Please specify `validation_steps` or use'
+                             ' the `keras.utils.Sequence` class.')
 
         # Prepare display labels.
         out_labels = self._get_deduped_metrics_names()
@@ -2059,15 +2081,6 @@ class Model(Container):
                 val_data += [0.]
             for cbk in callbacks:
                 cbk.validation_data = val_data
-        is_sequence = isinstance(generator, Sequence)
-        if not is_sequence and use_multiprocessing and workers > 1:
-            warnings.warn(
-                UserWarning('Using a generator with `use_multiprocessing=True`'
-                            ' and multiple workers may duplicate your data.'
-                            ' Please consider using the`keras.utils.Sequence'
-                            ' class.'))
-        if is_sequence:
-            steps_per_epoch = len(generator)
         enqueuer = None
 
         try:
@@ -2173,7 +2186,7 @@ class Model(Container):
         return self.history
 
     @interfaces.legacy_generator_methods_support
-    def evaluate_generator(self, generator, steps,
+    def evaluate_generator(self, generator, steps=None,
                            max_queue_size=10,
                            workers=1,
                            use_multiprocessing=False):
@@ -2190,7 +2203,8 @@ class Model(Container):
                     when using multiprocessing.
             steps: Total number of steps (batches of samples)
                 to yield from `generator` before stopping.
-                Not used if using Sequence.
+                Optional for `Sequence`: if unspecified, will use
+                the `len(generator)` as a number of steps.
             max_queue_size: maximum size for the generator queue
             workers: maximum number of processes to spin up
                 when using process based threading
@@ -2225,8 +2239,14 @@ class Model(Container):
                             ' and multiple workers may duplicate your data.'
                             ' Please consider using the`keras.utils.Sequence'
                             ' class.'))
-        if is_sequence:
-            steps = len(generator)
+        if steps is None:
+            if is_sequence:
+                steps = len(generator)
+            else:
+                raise ValueError('`steps=None` is only valid for a generator'
+                                 ' based on the `keras.utils.Sequence` class.'
+                                 ' Please specify `steps` or use the'
+                                 ' `keras.utils.Sequence` class.')
         enqueuer = None
 
         try:
@@ -2288,7 +2308,7 @@ class Model(Container):
             return averages
 
     @interfaces.legacy_generator_methods_support
-    def predict_generator(self, generator, steps,
+    def predict_generator(self, generator, steps=None,
                           max_queue_size=10,
                           workers=1,
                           use_multiprocessing=False,
@@ -2305,7 +2325,8 @@ class Model(Container):
                     when using multiprocessing.
             steps: Total number of steps (batches of samples)
                 to yield from `generator` before stopping.
-                Not used if using Sequence.
+                Optional for `Sequence`: if unspecified, will use
+                the `len(generator)` as a number of steps.
             max_queue_size: Maximum size for the generator queue.
             workers: Maximum number of processes to spin up
                 when using process based threading
@@ -2337,8 +2358,14 @@ class Model(Container):
                             ' and multiple workers may duplicate your data.'
                             ' Please consider using the`keras.utils.Sequence'
                             ' class.'))
-        if is_sequence:
-            steps = len(generator)
+        if steps is None:
+            if is_sequence:
+                steps = len(generator)
+            else:
+                raise ValueError('`steps=None` is only valid for a generator'
+                                 ' based on the `keras.utils.Sequence` class.'
+                                 ' Please specify `steps` or use the'
+                                 ' `keras.utils.Sequence` class.')
         enqueuer = None
 
         try:
diff --git a/keras/models.py b/keras/models.py
index a53b1dd8..f4487f06 100644
--- a/keras/models.py
+++ b/keras/models.py
@@ -1116,7 +1116,7 @@ class Sequential(Model):
 
     @interfaces.legacy_generator_methods_support
     def fit_generator(self, generator,
-                      steps_per_epoch,
+                      steps_per_epoch=None,
                       epochs=1,
                       verbose=1,
                       callbacks=None,
@@ -1148,6 +1148,8 @@ class Sequential(Model):
                 finished and starting the next epoch. It should typically
                 be equal to the number of samples of your dataset
                 divided by the batch size.
+                Optional for `Sequence`: if unspecified, will use
+                the `len(generator)` as a number of steps.
             epochs: Integer, total number of iterations on the data.
                 Note that in conjunction with initial_epoch, the parameter
                 epochs is to be understood as ""final epoch"". The model is
@@ -1165,6 +1167,8 @@ class Sequential(Model):
                 at the end of every epoch. It should typically
                 be equal to the number of samples of your
                 validation dataset divided by the batch size.
+                Optional for `Sequence`: if unspecified, will use
+                the `len(validation_data)` as a number of steps.
             class_weight: Dictionary mapping class indices to a weight
                 for the class.
             max_queue_size: Maximum size for the generator queue
@@ -1223,7 +1227,7 @@ class Sequential(Model):
                                         initial_epoch=initial_epoch)
 
     @interfaces.legacy_generator_methods_support
-    def evaluate_generator(self, generator, steps,
+    def evaluate_generator(self, generator, steps=None,
                            max_queue_size=10, workers=1,
                            use_multiprocessing=False):
         """"""Evaluates the model on a data generator.
@@ -1236,6 +1240,8 @@ class Sequential(Model):
                 or (inputs, targets, sample_weights)
             steps: Total number of steps (batches of samples)
                 to yield from `generator` before stopping.
+                Optional for `Sequence`: if unspecified, will use
+                the `len(generator)` as a number of steps.
             max_queue_size: maximum size for the generator queue
             workers: maximum number of processes to spin up
             use_multiprocessing: if True, use process based threading.
@@ -1263,7 +1269,7 @@ class Sequential(Model):
                                              use_multiprocessing=use_multiprocessing)
 
     @interfaces.legacy_generator_methods_support
-    def predict_generator(self, generator, steps,
+    def predict_generator(self, generator, steps=None,
                           max_queue_size=10, workers=1,
                           use_multiprocessing=False, verbose=0):
         """"""Generates predictions for the input samples from a data generator.
@@ -1275,6 +1281,8 @@ class Sequential(Model):
             generator: generator yielding batches of input samples.
             steps: Total number of steps (batches of samples)
                 to yield from `generator` before stopping.
+                Optional for `Sequence`: if unspecified, will use
+                the `len(generator)` as a number of steps.
             max_queue_size: maximum size for the generator queue
             workers: maximum number of processes to spin up
             use_multiprocessing: if True, use process based threading.
"
"keras","19","f9210387088fe91b5bc8999cf0cb41a0fe9eacf6","66f8cc7ac4942f7f9fe0164a2a854a6264b87735","keras/backend/cntk_backend.py;keras/layers/recurrent.py","keras/backend/cntk_backend.py;keras/layers/recurrent.py","diff --git a/keras/backend/cntk_backend.py b/keras/backend/cntk_backend.py;diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py","tests/keras/layers/recurrent_test.py","","diff --git a/keras/backend/cntk_backend.py b/keras/backend/cntk_backend.py
index 59430400..4c436f6d 100644
--- a/keras/backend/cntk_backend.py
+++ b/keras/backend/cntk_backend.py
@@ -1439,7 +1439,7 @@ def rnn(step_function, inputs, initial_states,
             for o, p in zip(new_states, place_holders):
                 n_s.append(o.replace_placeholders({p: o.output}))
             if len(n_s) > 0:
-                new_output = n_s[0]
+                new_output = n_s[-1]
             return new_output, n_s
 
         final_output, final_states = _recurrence(rnn_inputs, states, mask)
diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py
index 30859a93..c82e6a32 100644
--- a/keras/layers/recurrent.py
+++ b/keras/layers/recurrent.py
@@ -54,36 +54,56 @@ class StackedRNNCells(Layer):
                                  '`state_size` attribute. '
                                  'received cells:', cells)
         self.cells = cells
+        # reverse_state_order determines whether the state size will be in a
+        # reverse order of the cells' state. User might want to set this to True
+        # to keep the existing behavior. This is only useful when use
+        # `RNN(return_state=True)` since the state will be returned as the same
+        # order of state_size.
+        self.reverse_state_order = kwargs.pop('reverse_state_order', False)
+        if self.reverse_state_order:
+            warnings.warn('`reverse_state_order=True` in `StackedRNNCells` '
+                          'will soon be deprecated. Please update the code to '
+                          'work with the natural order of states if you '
+                          'reply on the RNN states, '
+                          'eg `RNN(return_state=True)`.')
         super(StackedRNNCells, self).__init__(**kwargs)
 
     @property
     def state_size(self):
-        # States are a flat list
-        # in reverse order of the cell stack.
-        # This allows to preserve the requirement
-        # `stack.state_size[0] == output_dim`.
-        # e.g. states of a 2-layer LSTM would be
-        # `[h2, c2, h1, c1]`
+        # States are a flat list of the individual cell state size.
+        # e.g. states of a 2-layer LSTM would be `[h1, c1, h2, c2]`.
         # (assuming one LSTM has states [h, c])
+        # In the case of reverse_state_order=True, the state_size will be
+        # `[h2, c2, h1, c1]`.
         state_size = []
-        for cell in self.cells[::-1]:
+        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:
             if hasattr(cell.state_size, '__len__'):
                 state_size += list(cell.state_size)
             else:
                 state_size.append(cell.state_size)
         return tuple(state_size)
 
+    @property
+    def output_size(self):
+        if getattr(self.cells[-1], 'output_size', None) is not None:
+            return self.cells[-1].output_size
+        if hasattr(self.cells[-1].state_size, '__len__'):
+            return self.cells[-1].state_size[0]
+        else:
+            return self.cells[-1].state_size
+
     def call(self, inputs, states, constants=None, **kwargs):
         # Recover per-cell states.
         nested_states = []
-        for cell in self.cells[::-1]:
+        for cell in self.cells[::-1] if self.reverse_state_order else self.cells:
             if hasattr(cell.state_size, '__len__'):
                 nested_states.append(states[:len(cell.state_size)])
                 states = states[len(cell.state_size):]
             else:
                 nested_states.append([states[0]])
                 states = states[1:]
-        nested_states = nested_states[::-1]
+        if self.reverse_state_order:
+            nested_states = nested_states[::-1]
 
         # Call the cells in order and store the returned states.
         new_nested_states = []
@@ -98,10 +118,12 @@ class StackedRNNCells(Layer):
 
         # Format the new states as a flat list
         # in reverse cell order.
-        states = []
-        for cell_states in new_nested_states[::-1]:
-            states += cell_states
-        return inputs, states
+        new_states = []
+        if self.reverse_state_order:
+            new_nested_states = new_nested_states[::-1]
+        for cell_states in new_nested_states:
+            new_states += cell_states
+        return inputs, new_states
 
     def build(self, input_shape):
         if isinstance(input_shape, list):
@@ -113,7 +135,9 @@ class StackedRNNCells(Layer):
                     cell.build([input_shape] + constants_shape)
                 else:
                     cell.build(input_shape)
-            if hasattr(cell.state_size, '__len__'):
+            if getattr(cell, 'output_size', None) is not None:
+                output_dim = cell.output_size
+            elif hasattr(cell.state_size, '__len__'):
                 output_dim = cell.state_size[0]
             else:
                 output_dim = cell.state_size
@@ -223,9 +247,12 @@ class RNN(Layer):
                 the size of the recurrent state
                 (which should be the same as the size of the cell output).
                 This can also be a list/tuple of integers
-                (one size per state). In this case, the first entry
-                (`state_size[0]`) should be the same as
-                the size of the cell output.
+                (one size per state).
+            - a `output_size` attribute. This can be a single integer or a
+                TensorShape, which represent the shape of the output. For
+                backward compatible reason, if this attribute is not available
+                for the cell, the value will be inferred by the first element
+                of the `state_size`.
             It is also possible for `cell` to be a list of RNN cell instances,
             in which cases the cells get stacked on after the other in the RNN,
             implementing an efficient stacked RNN.
@@ -414,7 +441,11 @@ class RNN(Layer):
             state_size = self.cell.state_size
         else:
             state_size = [self.cell.state_size]
-        output_dim = state_size[0]
+
+        if getattr(self.cell, 'output_size', None) is not None:
+            output_dim = self.cell.output_size
+        else:
+            output_dim = state_size[0]
 
         if self.return_sequences:
             output_shape = (input_shape[0], input_shape[1], output_dim)
@@ -827,6 +858,7 @@ class SimpleRNNCell(Layer):
         self.dropout = min(1., max(0., dropout))
         self.recurrent_dropout = min(1., max(0., recurrent_dropout))
         self.state_size = self.units
+        self.output_size = self.units
         self._dropout_mask = None
         self._recurrent_dropout_mask = None
 
@@ -1220,6 +1252,7 @@ class GRUCell(Layer):
         self.implementation = implementation
         self.reset_after = reset_after
         self.state_size = self.units
+        self.output_size = self.units
         self._dropout_mask = None
         self._recurrent_dropout_mask = None
 
@@ -1795,6 +1828,7 @@ class LSTMCell(Layer):
         self.recurrent_dropout = min(1., max(0., recurrent_dropout))
         self.implementation = implementation
         self.state_size = (self.units, self.units)
+        self.output_size = self.units
         self._dropout_mask = None
         self._recurrent_dropout_mask = None
 
"
"keras","9","0cd3b07eb5de1aaaad84d1ff7f7c2ed7dab4b23c","0505393746d56ddacc34bb1c016dba79429c9ac9","docs/autogen.py","docs/autogen.py","diff --git a/docs/autogen.py b/docs/autogen.py","tests/test_doc_auto_generation.py","","diff --git a/docs/autogen.py b/docs/autogen.py
index e3786a92..85acc296 100644
--- a/docs/autogen.py
+++ b/docs/autogen.py
@@ -117,8 +117,8 @@ def count_leading_spaces(s):
 def process_list_block(docstring, starting_point, section_end,
                        leading_spaces, marker):
     ending_point = docstring.find('\n\n', starting_point)
-    block = docstring[starting_point:(None if ending_point == -1 else
-                                      ending_point - 1)]
+    block = docstring[starting_point:(ending_point - 1 if ending_point > -1 else
+                                      section_end)]
     # Place marker for later reinjection.
     docstring_slice = docstring[starting_point:section_end].replace(block, marker)
     docstring = (docstring[:starting_point]
"
"keras","21","c7b7328cc99fd5d7c298e57c6020043451d89a61","1fc585adb57f20a2acf69f0cd08b731259b8d2f8","keras/callbacks.py","keras/callbacks.py","diff --git a/keras/callbacks.py b/keras/callbacks.py","tests/keras/test_callbacks.py","","diff --git a/keras/callbacks.py b/keras/callbacks.py
index 067169f3..04826921 100644
--- a/keras/callbacks.py
+++ b/keras/callbacks.py
@@ -477,6 +477,10 @@ class EarlyStopping(Callback):
         baseline: Baseline value for the monitored quantity to reach.
             Training will stop if the model doesn't show improvement
             over the baseline.
+        restore_best_weights: whether to restore model weights from
+            the epoch with the best value of the monitored quantity.
+            If False, the model weights obtained at the last step of
+            training are used.
     """"""
 
     def __init__(self,
@@ -485,7 +489,8 @@ class EarlyStopping(Callback):
                  patience=0,
                  verbose=0,
                  mode='auto',
-                 baseline=None):
+                 baseline=None,
+                 restore_best_weights=False):
         super(EarlyStopping, self).__init__()
 
         self.monitor = monitor
@@ -495,6 +500,8 @@ class EarlyStopping(Callback):
         self.min_delta = min_delta
         self.wait = 0
         self.stopped_epoch = 0
+        self.restore_best_weights = restore_best_weights
+        self.best_weights = None
 
         if mode not in ['auto', 'min', 'max']:
             warnings.warn('EarlyStopping mode %s is unknown, '
@@ -538,11 +545,17 @@ class EarlyStopping(Callback):
         if self.monitor_op(current - self.min_delta, self.best):
             self.best = current
             self.wait = 0
+            if self.restore_best_weights:
+                self.best_weights = self.model.get_weights()
         else:
             self.wait += 1
             if self.wait >= self.patience:
                 self.stopped_epoch = epoch
                 self.model.stop_training = True
+                if self.restore_best_weights:
+                    if self.verbose > 0:
+                        print(""Restoring model weights from the end of the best epoch"")
+                    self.model.set_weights(self.best_weights)
 
     def on_train_end(self, logs=None):
         if self.stopped_epoch > 0 and self.verbose > 0:
"
"keras","30","c08ef613af27da896cee168daeee5c6fad1980b6","2c8d1d03599cc03243bce8f07ed9c4a3d5f384f9","keras/engine/training.py","keras/engine/training.py","diff --git a/keras/engine/training.py b/keras/engine/training.py","tests/keras/engine/test_training.py","","diff --git a/keras/engine/training.py b/keras/engine/training.py
index c6691017..78be752b 100644
--- a/keras/engine/training.py
+++ b/keras/engine/training.py
@@ -2212,7 +2212,11 @@ class Model(Container):
                                          str(generator_output))
                     # build batch logs
                     batch_logs = {}
-                    if isinstance(x, list):
+                    if x is None or len(x) == 0:
+                        # Handle data tensors support when no input given
+                        # step-size = 1 for data tensors
+                        batch_size = 1
+                    elif isinstance(x, list):
                         batch_size = x[0].shape[0]
                     elif isinstance(x, dict):
                         batch_size = list(x.values())[0].shape[0]
@@ -2399,7 +2403,11 @@ class Model(Container):
                     outs = [outs]
                 outs_per_batch.append(outs)
 
-                if isinstance(x, list):
+                if x is None or len(x) == 0:
+                    # Handle data tensors support when no input given
+                    # step-size = 1 for data tensors
+                    batch_size = 1
+                elif isinstance(x, list):
                     batch_size = x[0].shape[0]
                 elif isinstance(x, dict):
                     batch_size = list(x.values())[0].shape[0]
"
"keras","15","5b6243485acc20cc36f2db4f258512c332d691ec","f60313e29657b2afb6a02f28dba5936bc0dd09e6","keras/callbacks.py","keras/callbacks.py","diff --git a/keras/callbacks.py b/keras/callbacks.py","tests/keras/test_callbacks.py","","diff --git a/keras/callbacks.py b/keras/callbacks.py
index ad3522d6..8c6f70c6 100644
--- a/keras/callbacks.py
+++ b/keras/callbacks.py
@@ -12,6 +12,8 @@ import numpy as np
 import time
 import json
 import warnings
+import io
+import sys
 
 from collections import deque
 from collections import OrderedDict
@@ -1122,7 +1124,12 @@ class CSVLogger(Callback):
         self.writer = None
         self.keys = None
         self.append_header = True
-        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''
+        if six.PY2:
+            self.file_flags = 'b'
+            self._open_args = {}
+        else:
+            self.file_flags = ''
+            self._open_args = {'newline': '\n'}
         super(CSVLogger, self).__init__()
 
     def on_train_begin(self, logs=None):
@@ -1130,9 +1137,12 @@ class CSVLogger(Callback):
             if os.path.exists(self.filename):
                 with open(self.filename, 'r' + self.file_flags) as f:
                     self.append_header = not bool(len(f.readline()))
-            self.csv_file = open(self.filename, 'a' + self.file_flags)
+            mode = 'a'
         else:
-            self.csv_file = open(self.filename, 'w' + self.file_flags)
+            mode = 'w'
+        self.csv_file = io.open(self.filename,
+                                mode + self.file_flags,
+                                **self._open_args)
 
     def on_epoch_end(self, epoch, logs=None):
         logs = logs or {}
@@ -1156,9 +1166,12 @@ class CSVLogger(Callback):
         if not self.writer:
             class CustomDialect(csv.excel):
                 delimiter = self.sep
-
+            fieldnames = ['epoch'] + self.keys
+            if six.PY2:
+                fieldnames = [unicode(x) for x in fieldnames]
             self.writer = csv.DictWriter(self.csv_file,
-                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)
+                                         fieldnames=fieldnames,
+                                         dialect=CustomDialect)
             if self.append_header:
                 self.writer.writeheader()
 
"
"keras","32","a3d160b9467c99cbb27f9aa0382c759f45c8ee66","709f791af201caaab4aa180bda259989087cfe47","keras/callbacks.py","keras/callbacks.py","diff --git a/keras/callbacks.py b/keras/callbacks.py","tests/keras/test_callbacks.py","","diff --git a/keras/callbacks.py b/keras/callbacks.py
index 8b5ec5f3..eb233de3 100644
--- a/keras/callbacks.py
+++ b/keras/callbacks.py
@@ -898,7 +898,7 @@ class ReduceLROnPlateau(Callback):
             monitored has stopped increasing; in `auto`
             mode, the direction is automatically inferred
             from the name of the monitored quantity.
-        epsilon: threshold for measuring the new optimum,
+        min_delta: threshold for measuring the new optimum,
             to only focus on significant changes.
         cooldown: number of epochs to wait before resuming
             normal operation after lr has been reduced.
@@ -906,16 +906,21 @@ class ReduceLROnPlateau(Callback):
     """"""
 
     def __init__(self, monitor='val_loss', factor=0.1, patience=10,
-                 verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):
+                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,
+                 **kwargs):
         super(ReduceLROnPlateau, self).__init__()
 
         self.monitor = monitor
         if factor >= 1.0:
             raise ValueError('ReduceLROnPlateau '
                              'does not support a factor >= 1.0.')
+        if 'epsilon' in kwargs:
+            min_delta = kwargs.pop('epsilon')
+            warnings.warn('`epsilon` argument is deprecated and '
+                          'will be removed, use `min_delta` insted.')
         self.factor = factor
         self.min_lr = min_lr
-        self.epsilon = epsilon
+        self.min_delta = min_delta
         self.patience = patience
         self.verbose = verbose
         self.cooldown = cooldown
@@ -936,10 +941,10 @@ class ReduceLROnPlateau(Callback):
             self.mode = 'auto'
         if (self.mode == 'min' or
            (self.mode == 'auto' and 'acc' not in self.monitor)):
-            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)
+            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)
             self.best = np.Inf
         else:
-            self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)
+            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)
             self.best = -np.Inf
         self.cooldown_counter = 0
         self.wait = 0
@@ -967,6 +972,7 @@ class ReduceLROnPlateau(Callback):
                 self.best = current
                 self.wait = 0
             elif not self.in_cooldown():
+                self.wait += 1
                 if self.wait >= self.patience:
                     old_lr = float(K.get_value(self.model.optimizer.lr))
                     if old_lr > self.min_lr:
@@ -978,7 +984,6 @@ class ReduceLROnPlateau(Callback):
                                   'rate to %s.' % (epoch + 1, new_lr))
                         self.cooldown_counter = self.cooldown
                         self.wait = 0
-                self.wait += 1
 
     def in_cooldown(self):
         return self.cooldown_counter > 0
"
"keras","11","36b9e4c055f32718a036cabaf767325b010c7485","d6b5c5ebb410e3366c9d7aca41977a60134bfe10","keras/engine/training_generator.py;keras/engine/training_utils.py;keras/utils/data_utils.py;tests/keras/utils/data_utils_test.py;tests/test_multiprocessing.py","keras/engine/training_generator.py;keras/engine/training_utils.py;keras/utils/data_utils.py;tests/keras/utils/data_utils_test.py;tests/test_multiprocessing.py","diff --git a/keras/engine/training_generator.py b/keras/engine/training_generator.py;diff --git a/keras/engine/training_utils.py b/keras/engine/training_utils.py;diff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py;diff --git a/tests/keras/utils/data_utils_test.py b/tests/keras/utils/data_utils_test.py;diff --git a/tests/test_multiprocessing.py b/tests/test_multiprocessing.py","tests/integration_tests/test_image_data_tasks.py","","diff --git a/keras/engine/training_generator.py b/keras/engine/training_generator.py
index 55e817a1..f74eddf8 100644
--- a/keras/engine/training_generator.py
+++ b/keras/engine/training_generator.py
@@ -7,6 +7,7 @@ from __future__ import print_function
 import warnings
 import numpy as np
 
+from .training_utils import is_sequence
 from .training_utils import iter_sequence_infinite
 from .. import backend as K
 from ..utils.data_utils import Sequence
@@ -40,15 +41,15 @@ def fit_generator(model,
     if do_validation:
         model._make_test_function()
 
-    is_sequence = isinstance(generator, Sequence)
-    if not is_sequence and use_multiprocessing and workers > 1:
+    use_sequence_api = is_sequence(generator)
+    if not use_sequence_api and use_multiprocessing and workers > 1:
         warnings.warn(
             UserWarning('Using a generator with `use_multiprocessing=True`'
                         ' and multiple workers may duplicate your data.'
                         ' Please consider using the`keras.utils.Sequence'
                         ' class.'))
     if steps_per_epoch is None:
-        if is_sequence:
+        if use_sequence_api:
             steps_per_epoch = len(generator)
         else:
             raise ValueError('`steps_per_epoch=None` is only valid for a'
@@ -59,10 +60,11 @@ def fit_generator(model,
 
     # python 2 has 'next', 3 has '__next__'
     # avoid any explicit version checks
+    val_use_sequence_api = is_sequence(validation_data)
     val_gen = (hasattr(validation_data, 'next') or
                hasattr(validation_data, '__next__') or
-               isinstance(validation_data, Sequence))
-    if (val_gen and not isinstance(validation_data, Sequence) and
+               val_use_sequence_api)
+    if (val_gen and not val_use_sequence_api and
             not validation_steps):
         raise ValueError('`validation_steps=None` is only valid for a'
                          ' generator based on the `keras.utils.Sequence`'
@@ -108,7 +110,7 @@ def fit_generator(model,
             if val_gen and workers > 0:
                 # Create an Enqueuer that can be reused
                 val_data = validation_data
-                if isinstance(val_data, Sequence):
+                if is_sequence(val_data):
                     val_enqueuer = OrderedEnqueuer(
                         val_data,
                         use_multiprocessing=use_multiprocessing)
@@ -122,7 +124,7 @@ def fit_generator(model,
                 val_enqueuer_gen = val_enqueuer.get()
             elif val_gen:
                 val_data = validation_data
-                if isinstance(val_data, Sequence):
+                if is_sequence(val_data):
                     val_enqueuer_gen = iter_sequence_infinite(val_data)
                     validation_steps = validation_steps or len(val_data)
                 else:
@@ -149,7 +151,7 @@ def fit_generator(model,
                     cbk.validation_data = val_data
 
         if workers > 0:
-            if is_sequence:
+            if use_sequence_api:
                 enqueuer = OrderedEnqueuer(
                     generator,
                     use_multiprocessing=use_multiprocessing,
@@ -161,7 +163,7 @@ def fit_generator(model,
             enqueuer.start(workers=workers, max_queue_size=max_queue_size)
             output_generator = enqueuer.get()
         else:
-            if is_sequence:
+            if use_sequence_api:
                 output_generator = iter_sequence_infinite(generator)
             else:
                 output_generator = generator
@@ -284,15 +286,15 @@ def evaluate_generator(model, generator,
     steps_done = 0
     outs_per_batch = []
     batch_sizes = []
-    is_sequence = isinstance(generator, Sequence)
-    if not is_sequence and use_multiprocessing and workers > 1:
+    use_sequence_api = is_sequence(generator)
+    if not use_sequence_api and use_multiprocessing and workers > 1:
         warnings.warn(
             UserWarning('Using a generator with `use_multiprocessing=True`'
                         ' and multiple workers may duplicate your data.'
                         ' Please consider using the`keras.utils.Sequence'
                         ' class.'))
     if steps is None:
-        if is_sequence:
+        if use_sequence_api:
             steps = len(generator)
         else:
             raise ValueError('`steps=None` is only valid for a generator'
@@ -303,7 +305,7 @@ def evaluate_generator(model, generator,
 
     try:
         if workers > 0:
-            if is_sequence:
+            if use_sequence_api:
                 enqueuer = OrderedEnqueuer(
                     generator,
                     use_multiprocessing=use_multiprocessing)
@@ -314,7 +316,7 @@ def evaluate_generator(model, generator,
             enqueuer.start(workers=workers, max_queue_size=max_queue_size)
             output_generator = enqueuer.get()
         else:
-            if is_sequence:
+            if use_sequence_api:
                 output_generator = iter_sequence_infinite(generator)
             else:
                 output_generator = generator
@@ -387,15 +389,15 @@ def predict_generator(model, generator,
 
     steps_done = 0
     all_outs = []
-    is_sequence = isinstance(generator, Sequence)
-    if not is_sequence and use_multiprocessing and workers > 1:
+    use_sequence_api = is_sequence(generator)
+    if not use_sequence_api and use_multiprocessing and workers > 1:
         warnings.warn(
             UserWarning('Using a generator with `use_multiprocessing=True`'
                         ' and multiple workers may duplicate your data.'
                         ' Please consider using the`keras.utils.Sequence'
                         ' class.'))
     if steps is None:
-        if is_sequence:
+        if use_sequence_api:
             steps = len(generator)
         else:
             raise ValueError('`steps=None` is only valid for a generator'
@@ -406,7 +408,7 @@ def predict_generator(model, generator,
 
     try:
         if workers > 0:
-            if is_sequence:
+            if use_sequence_api:
                 enqueuer = OrderedEnqueuer(
                     generator,
                     use_multiprocessing=use_multiprocessing)
@@ -417,7 +419,7 @@ def predict_generator(model, generator,
             enqueuer.start(workers=workers, max_queue_size=max_queue_size)
             output_generator = enqueuer.get()
         else:
-            if is_sequence:
+            if use_sequence_api:
                 output_generator = iter_sequence_infinite(generator)
             else:
                 output_generator = generator
diff --git a/keras/engine/training_utils.py b/keras/engine/training_utils.py
index ea22fc5a..26674ba4 100644
--- a/keras/engine/training_utils.py
+++ b/keras/engine/training_utils.py
@@ -10,6 +10,7 @@ import warnings
 
 from .. import backend as K
 from .. import losses
+from ..utils import Sequence
 from ..utils.generic_utils import to_list
 
 
@@ -589,3 +590,17 @@ def iter_sequence_infinite(seq):
     while True:
         for item in seq:
             yield item
+
+
+def is_sequence(seq):
+    """"""Determine if an object follows the Sequence API.
+
+    # Arguments
+        seq: a possible Sequence object
+
+    # Returns
+        boolean, whether the object follows the Sequence API.
+    """"""
+    # TODO Dref360: Decide which pattern to follow. First needs a new TF Version.
+    return (getattr(seq, 'use_sequence_api', False)
+            or set(dir(Sequence())).issubset(set(dir(seq) + ['use_sequence_api'])))
diff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py
index 51088192..492aa513 100644
--- a/keras/utils/data_utils.py
+++ b/keras/utils/data_utils.py
@@ -341,6 +341,8 @@ class Sequence(object):
     ```
     """"""
 
+    use_sequence_api = True
+
     @abstractmethod
     def __getitem__(self, index):
         """"""Gets batch at position `index`.
diff --git a/tests/keras/utils/data_utils_test.py b/tests/keras/utils/data_utils_test.py
index a825c53f..8e367ff4 100644
--- a/tests/keras/utils/data_utils_test.py
+++ b/tests/keras/utils/data_utils_test.py
@@ -22,7 +22,7 @@ from keras.utils.data_utils import validate_file
 from keras import backend as K
 
 pytestmark = pytest.mark.skipif(
-    K.backend() == 'tensorflow',
+    K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,
     reason='Temporarily disabled until the use_multiprocessing problem is solved')
 
 if sys.version_info < (3,):
diff --git a/tests/test_multiprocessing.py b/tests/test_multiprocessing.py
index f22cc40e..e8269d46 100644
--- a/tests/test_multiprocessing.py
+++ b/tests/test_multiprocessing.py
@@ -9,7 +9,7 @@ from keras.utils import Sequence
 from keras import backend as K
 
 pytestmark = pytest.mark.skipif(
-    K.backend() == 'tensorflow',
+    K.backend() == 'tensorflow' and 'TRAVIS_PYTHON_VERSION' in os.environ,
     reason='Temporarily disabled until the use_multiprocessing problem is solved')
 
 STEPS_PER_EPOCH = 100
"
"keras","6","88af7d0c97497b5c3a198ee9416b2accfbc72c36","4b54657ab4806b0aaef8f8eeb973edb83c3d3483","keras/engine/training_utils.py","keras/engine/training_utils.py","diff --git a/keras/engine/training_utils.py b/keras/engine/training_utils.py","tests/test_loss_masking.py","","diff --git a/keras/engine/training_utils.py b/keras/engine/training_utils.py
index 68f14ac0..e8116397 100644
--- a/keras/engine/training_utils.py
+++ b/keras/engine/training_utils.py
@@ -410,7 +410,7 @@ def weighted_masked_objective(fn):
             score_array *= mask
             #  the loss per batch should be proportional
             #  to the number of unmasked samples.
-            score_array /= K.mean(mask)
+            score_array /= K.mean(mask) + K.epsilon()
 
         # apply sample weighting
         if weights is not None:
"
"keras","35","06eaeebecfb73c23bfd531013ca172ee3bf5069c","738819de0b7e6bc45abed8d0640f02b81c6ac4e9","keras/preprocessing/image.py","keras/preprocessing/image.py","diff --git a/keras/preprocessing/image.py b/keras/preprocessing/image.py","tests/keras/preprocessing/image_test.py","","diff --git a/keras/preprocessing/image.py b/keras/preprocessing/image.py
index 97d97792..838d7bc3 100644
--- a/keras/preprocessing/image.py
+++ b/keras/preprocessing/image.py
@@ -580,8 +580,6 @@ class ImageDataGenerator(object):
         # Returns
             The inputs, normalized.
         """"""
-        if self.preprocessing_function:
-            x = self.preprocessing_function(x)
         if self.rescale:
             x *= self.rescale
         if self.samplewise_center:
@@ -951,6 +949,8 @@ class NumpyArrayIterator(Iterator):
                            dtype=K.floatx())
         for i, j in enumerate(index_array):
             x = self.x[j]
+            if self.image_data_generator.preprocessing_function:
+                x = self.image_data_generator.preprocessing_function(x)
             x = self.image_data_generator.random_transform(x.astype(K.floatx()))
             x = self.image_data_generator.standardize(x)
             batch_x[i] = x
@@ -1227,8 +1227,21 @@ class DirectoryIterator(Iterator):
             fname = self.filenames[j]
             img = load_img(os.path.join(self.directory, fname),
                            grayscale=grayscale,
-                           target_size=self.target_size,
+                           target_size=None,
                            interpolation=self.interpolation)
+            if self.image_data_generator.preprocessing_function:
+                img = self.image_data_generator.preprocessing_function(img)
+            if self.target_size is not None:
+                width_height_tuple = (self.target_size[1], self.target_size[0])
+                if img.size != width_height_tuple:
+                    if self.interpolation not in _PIL_INTERPOLATION_METHODS:
+                        raise ValueError(
+                            'Invalid interpolation method {} specified. Supported '
+                            'methods are {}'.format(
+                                self.interpolation,
+                                "", "".join(_PIL_INTERPOLATION_METHODS.keys())))
+                    resample = _PIL_INTERPOLATION_METHODS[self.interpolation]
+                    img = img.resize(width_height_tuple, resample)
             x = img_to_array(img, data_format=self.data_format)
             x = self.image_data_generator.random_transform(x)
             x = self.image_data_generator.standardize(x)
"
"keras","33","1c9a49781da2101507db23e2014e4e5d16bd2e52","70ad0d6e4a569701ef106058397ad0540ec08340","keras/preprocessing/text.py","keras/preprocessing/text.py","diff --git a/keras/preprocessing/text.py b/keras/preprocessing/text.py","tests/keras/preprocessing/text_test.py","","diff --git a/keras/preprocessing/text.py b/keras/preprocessing/text.py
index 8264c1a6..6bd17435 100644
--- a/keras/preprocessing/text.py
+++ b/keras/preprocessing/text.py
@@ -38,12 +38,21 @@ def text_to_word_sequence(text,
     if lower:
         text = text.lower()
 
-    if sys.version_info < (3,) and isinstance(text, unicode):
-        translate_map = dict((ord(c), unicode(split)) for c in filters)
+    if sys.version_info < (3,):
+        if isinstance(text, unicode):
+            translate_map = dict((ord(c), unicode(split)) for c in filters)
+            text = text.translate(translate_map)
+        elif len(split) == 1:
+            translate_map = maketrans(filters, split * len(filters))
+            text = text.translate(translate_map)
+        else:
+            for c in filters:
+                text = text.replace(c, split)
     else:
-        translate_map = maketrans(filters, split * len(filters))
+        translate_dict = dict((c, split) for c in filters)
+        translate_map = maketrans(translate_dict)
+        text = text.translate(translate_map)
 
-    text = text.translate(translate_map)
     seq = text.split(split)
     return [i for i in seq if i]
 
"
"keras","17","c913b6da92f6ab9a3f4c897caa4085e782a14680","5a6af4bc6d44e9adbc2a21804bfcd18c4ce849ef","keras/metrics.py","keras/metrics.py","diff --git a/keras/metrics.py b/keras/metrics.py","tests/keras/metrics_test.py","","diff --git a/keras/metrics.py b/keras/metrics.py
index 3d5df23b..7c89702e 100644
--- a/keras/metrics.py
+++ b/keras/metrics.py
@@ -34,7 +34,8 @@ def categorical_accuracy(y_true, y_pred):
 
 
 def sparse_categorical_accuracy(y_true, y_pred):
-    return K.cast(K.equal(K.max(y_true, axis=-1),
+    # flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)
+    return K.cast(K.equal(K.flatten(y_true),
                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())),
                   K.floatx())
 
"
"keras","28","6171b3656ebd9b6038f709ba83f7475de284ba4e","5422fdd38baad36730cb6aeb946e17eeae6a551c","keras/preprocessing/sequence.py","keras/preprocessing/sequence.py","diff --git a/keras/preprocessing/sequence.py b/keras/preprocessing/sequence.py","tests/keras/preprocessing/sequence_test.py","","diff --git a/keras/preprocessing/sequence.py b/keras/preprocessing/sequence.py
index 03906c04..7bfe97a9 100644
--- a/keras/preprocessing/sequence.py
+++ b/keras/preprocessing/sequence.py
@@ -326,9 +326,15 @@ class TimeseriesGenerator(Sequence):
         self.reverse = reverse
         self.batch_size = batch_size
 
+        if self.start_index > self.end_index:
+            raise ValueError('`start_index+length=%i > end_index=%i` '
+                             'is disallowed, as no part of the sequence '
+                             'would be left to be used as current step.'
+                             % (self.start_index, self.end_index))
+
     def __len__(self):
         return int(np.ceil(
-            (self.end_index - self.start_index) /
+            (self.end_index - self.start_index + 1) /
             (self.batch_size * self.stride)))
 
     def _empty_batch(self, num_rows):
@@ -341,11 +347,11 @@ class TimeseriesGenerator(Sequence):
     def __getitem__(self, index):
         if self.shuffle:
             rows = np.random.randint(
-                self.start_index, self.end_index, size=self.batch_size)
+                self.start_index, self.end_index + 1, size=self.batch_size)
         else:
             i = self.start_index + self.batch_size * self.stride * index
             rows = np.arange(i, min(i + self.batch_size *
-                                    self.stride, self.end_index), self.stride)
+                                    self.stride, self.end_index + 1), self.stride)
 
         samples, targets = self._empty_batch(len(rows))
         for j, row in enumerate(rows):
"
"keras","20","76da5f0a21ca98e4bf6706e182fb825243e76204","6dd087ab73b09e449144ff17450cc14f981b9ac2","keras/backend/cntk_backend.py;keras/backend/tensorflow_backend.py;keras/backend/theano_backend.py;keras/layers/convolutional.py;keras/utils/conv_utils.py","keras/backend/cntk_backend.py;keras/backend/tensorflow_backend.py;keras/backend/theano_backend.py;keras/layers/convolutional.py;keras/utils/conv_utils.py","diff --git a/keras/backend/cntk_backend.py b/keras/backend/cntk_backend.py;diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py;diff --git a/keras/backend/theano_backend.py b/keras/backend/theano_backend.py;diff --git a/keras/layers/convolutional.py b/keras/layers/convolutional.py;diff --git a/keras/utils/conv_utils.py b/keras/utils/conv_utils.py","tests/keras/layers/convolutional_test.py","","diff --git a/keras/backend/cntk_backend.py b/keras/backend/cntk_backend.py
index bbaa9f11..59430400 100644
--- a/keras/backend/cntk_backend.py
+++ b/keras/backend/cntk_backend.py
@@ -2186,7 +2186,7 @@ def in_top_k(predictions, targets, k):
 
 
 def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
-                     padding='valid', data_format=None):
+                     padding='valid', data_format=None, dilation_rate=(1, 1)):
     data_format = normalize_data_format(data_format)
 
     x = _preprocess_conv2d_input(x, data_format)
@@ -2208,7 +2208,8 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
             False,
             padding,
             padding],
-        output_shape=output_shape)
+        output_shape=output_shape,
+        dilation=dilation_rate)
     return _postprocess_conv2d_output(x, data_format)
 
 
diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py
index 2e44bb41..99e00949 100644
--- a/keras/backend/tensorflow_backend.py
+++ b/keras/backend/tensorflow_backend.py
@@ -3433,12 +3433,14 @@ def _preprocess_conv1d_input(x, data_format):
     return x, tf_data_format
 
 
-def _preprocess_conv2d_input(x, data_format):
+def _preprocess_conv2d_input(x, data_format, force_transpose=False):
     """"""Transpose and cast the input before the conv2d.
 
     # Arguments
         x: input tensor.
         data_format: string, `""channels_last""` or `""channels_first""`.
+        force_transpose: boolean, whether force to transpose input from NCHW to NHWC
+                        if the `data_format` is `""channels_first""`.
 
     # Returns
         A tensor.
@@ -3449,7 +3451,7 @@ def _preprocess_conv2d_input(x, data_format):
         x = tf.cast(x, 'float32')
     tf_data_format = 'NHWC'
     if data_format == 'channels_first':
-        if not _has_nchw_support():
+        if not _has_nchw_support() or force_transpose:
             x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC
         else:
             tf_data_format = 'NCHW'
@@ -3586,7 +3588,7 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',
 
 
 def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
-                     padding='valid', data_format=None):
+                     padding='valid', data_format=None, dilation_rate=(1, 1)):
     """"""2D deconvolution (i.e. transposed convolution).
 
     # Arguments
@@ -3598,6 +3600,7 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
         data_format: string, `""channels_last""` or `""channels_first""`.
             Whether to use Theano or TensorFlow/CNTK data format
             for inputs/kernels/outputs.
+        dilation_rate: tuple of 2 integers.
 
     # Returns
         A tensor, result of transposed 2D convolution.
@@ -3610,7 +3613,13 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
     if isinstance(output_shape, (tuple, list)):
         output_shape = tf.stack(output_shape)
 
-    x, tf_data_format = _preprocess_conv2d_input(x, data_format)
+    # tf.nn.atrous_conv2d_transpose input only supports NHWC format
+    if data_format == 'channels_first' and dilation_rate != (1, 1):
+        force_transpose = True
+    else:
+        force_transpose = False
+
+    x, tf_data_format = _preprocess_conv2d_input(x, data_format, force_transpose)
 
     if data_format == 'channels_first' and tf_data_format == 'NHWC':
         output_shape = (output_shape[0],
@@ -3627,9 +3636,15 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
     else:
         strides = (1, 1) + strides
 
-    x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,
-                               padding=padding,
-                               data_format=tf_data_format)
+    if dilation_rate == (1, 1):
+        x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,
+                                   padding=padding,
+                                   data_format=tf_data_format)
+    else:
+        assert dilation_rate[0] == dilation_rate[1]
+        x = tf.nn.atrous_conv2d_transpose(
+            x, kernel, output_shape, dilation_rate[0], padding)
+
     if data_format == 'channels_first' and tf_data_format == 'NHWC':
         x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW
     return x
diff --git a/keras/backend/theano_backend.py b/keras/backend/theano_backend.py
index cdb4a681..700a48fe 100644
--- a/keras/backend/theano_backend.py
+++ b/keras/backend/theano_backend.py
@@ -2134,7 +2134,7 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',
 
 
 def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
-                     padding='valid', data_format=None):
+                     padding='valid', data_format=None, dilation_rate=(1, 1)):
     """"""2D deconvolution (transposed convolution).
 
     # Arguments
@@ -2144,7 +2144,8 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
         padding: string, ""same"" or ""valid"".
         data_format: ""channels_last"" or ""channels_first"".
             Whether to use Theano or TensorFlow data format
-        in inputs/kernels/outputs.
+            in inputs/kernels/outputs.
+        dilation_rate: tuple of 2 integers.
 
     # Raises
         ValueError: if using an even kernel size with padding 'same'.
@@ -2177,7 +2178,8 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),
                                                         kshp=kernel_shape,
                                                         subsample=strides,
                                                         border_mode=th_padding,
-                                                        filter_flip=not flip_filters)
+                                                        filter_flip=not flip_filters,
+                                                        filter_dilation=dilation_rate)
     conv_out = op(kernel, x, output_shape[2:])
     conv_out = _postprocess_conv2d_output(conv_out, x, padding,
                                           kernel_shape, strides, data_format)
diff --git a/keras/layers/convolutional.py b/keras/layers/convolutional.py
index 672f8820..f3d41bcb 100644
--- a/keras/layers/convolutional.py
+++ b/keras/layers/convolutional.py
@@ -731,6 +731,7 @@ class Conv2DTranspose(Conv2D):
                  padding='valid',
                  output_padding=None,
                  data_format=None,
+                 dilation_rate=(1, 1),
                  activation=None,
                  use_bias=True,
                  kernel_initializer='glorot_uniform',
@@ -747,6 +748,7 @@ class Conv2DTranspose(Conv2D):
             strides=strides,
             padding=padding,
             data_format=data_format,
+            dilation_rate=dilation_rate,
             activation=activation,
             use_bias=use_bias,
             kernel_initializer=kernel_initializer,
@@ -820,11 +822,13 @@ class Conv2DTranspose(Conv2D):
         out_height = conv_utils.deconv_length(height,
                                               stride_h, kernel_h,
                                               self.padding,
-                                              out_pad_h)
+                                              out_pad_h,
+                                              self.dilation_rate[0])
         out_width = conv_utils.deconv_length(width,
                                              stride_w, kernel_w,
                                              self.padding,
-                                             out_pad_w)
+                                             out_pad_w,
+                                             self.dilation_rate[1])
         if self.data_format == 'channels_first':
             output_shape = (batch_size, self.filters, out_height, out_width)
         else:
@@ -836,7 +840,8 @@ class Conv2DTranspose(Conv2D):
             output_shape,
             self.strides,
             padding=self.padding,
-            data_format=self.data_format)
+            data_format=self.data_format,
+            dilation_rate=self.dilation_rate)
 
         if self.use_bias:
             outputs = K.bias_add(
@@ -867,17 +872,18 @@ class Conv2DTranspose(Conv2D):
                                                         stride_h,
                                                         kernel_h,
                                                         self.padding,
-                                                        out_pad_h)
+                                                        out_pad_h,
+                                                        self.dilation_rate[0])
         output_shape[w_axis] = conv_utils.deconv_length(output_shape[w_axis],
                                                         stride_w,
                                                         kernel_w,
                                                         self.padding,
-                                                        out_pad_w)
+                                                        out_pad_w,
+                                                        self.dilation_rate[1])
         return tuple(output_shape)
 
     def get_config(self):
         config = super(Conv2DTranspose, self).get_config()
-        config.pop('dilation_rate')
         config['output_padding'] = self.output_padding
         return config
 
diff --git a/keras/utils/conv_utils.py b/keras/utils/conv_utils.py
index c4370dbf..d8c65ff3 100644
--- a/keras/utils/conv_utils.py
+++ b/keras/utils/conv_utils.py
@@ -135,7 +135,8 @@ def conv_input_length(output_length, filter_size, padding, stride):
     return (output_length - 1) * stride - 2 * pad + filter_size
 
 
-def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):
+def deconv_length(dim_size, stride_size, kernel_size, padding,
+                  output_padding, dilation=1):
     """"""Determines output length of a transposed convolution given input length.
 
     # Arguments
@@ -146,6 +147,7 @@ def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):
         padding: One of `""same""`, `""valid""`, `""full""`.
         output_padding: Integer, amount of padding along the output dimension,
             Can be set to `None` in which case the output length is inferred.
+        dilation: dilation rate, integer.
 
     # Returns
         The output length (integer).
@@ -154,6 +156,9 @@ def deconv_length(dim_size, stride_size, kernel_size, padding, output_padding):
     if dim_size is None:
         return None
 
+    # Get the dilated kernel size
+    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)
+
     # Infer length if output padding is None, else compute the exact length
     if output_padding is None:
         if padding == 'valid':
"
"keras","37","81f6b3aa5b2b6215a533180e848a3b4dff851d03","1d2ad790dd43a2d702176c1170b2f3fd592a385a","keras/layers/recurrent.py;keras/layers/wrappers.py","keras/layers/recurrent.py;keras/layers/wrappers.py","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py;diff --git a/keras/layers/wrappers.py b/keras/layers/wrappers.py","tests/keras/layers/wrappers_test.py","","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py
index f31286db..81c367f9 100644
--- a/keras/layers/recurrent.py
+++ b/keras/layers/recurrent.py
@@ -518,12 +518,14 @@ class RNN(Layer):
             self._num_constants = len(constants)
             additional_specs += self.constants_spec
         # at this point additional_inputs cannot be empty
-        is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')
+        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])
         for tensor in additional_inputs:
-            if hasattr(tensor, '_keras_history') != is_keras_tensor:
+            if K.is_keras_tensor(tensor) != is_keras_tensor:
                 raise ValueError('The initial state or constants of an RNN'
                                  ' layer cannot be specified with a mix of'
-                                 ' Keras tensors and non-Keras tensors')
+                                 ' Keras tensors and non-Keras tensors'
+                                 ' (a ""Keras tensor"" is a tensor that was'
+                                 ' returned by a Keras layer, or by `Input`)')
 
         if is_keras_tensor:
             # Compute the full input spec, including state and constants
diff --git a/keras/layers/wrappers.py b/keras/layers/wrappers.py
index 7febff35..a5eed289 100644
--- a/keras/layers/wrappers.py
+++ b/keras/layers/wrappers.py
@@ -275,6 +275,7 @@ class Bidirectional(Wrapper):
         self.supports_masking = True
         self._trainable = True
         super(Bidirectional, self).__init__(layer, **kwargs)
+        self.input_spec = layer.input_spec
 
     @property
     def trainable(self):
@@ -313,6 +314,60 @@ class Bidirectional(Wrapper):
             return [output_shape] + state_shape + copy.copy(state_shape)
         return output_shape
 
+    def __call__(self, inputs, initial_state=None, **kwargs):
+        if isinstance(inputs, list):
+            if len(inputs) > 1:
+                initial_state = inputs[1:]
+            inputs = inputs[0]
+
+        if initial_state is None:
+            return super(Bidirectional, self).__call__(inputs, **kwargs)
+
+        # Standardize `initial_state` into list
+        if isinstance(initial_state, tuple):
+            initial_state = list(initial_state)
+        elif not isinstance(initial_state, list):
+            initial_state = [initial_state]
+
+        # Check if `initial_state` can be splitted into half
+        num_states = len(initial_state)
+        if num_states % 2 > 0:
+            raise ValueError(
+                'When passing `initial_state` to a Bidirectional RNN, the state '
+                'should be a list containing the states of the underlying RNNs. '
+                'Found: ' + str(initial_state))
+
+        # Applies the same workaround as in `RNN.__call__`, without handling constants
+        kwargs['initial_state'] = initial_state
+        additional_inputs = initial_state
+        additional_specs = [InputSpec(shape=K.int_shape(state))
+                            for state in initial_state]
+        self.forward_layer.state_spec = additional_specs[:num_states // 2]
+        self.backward_layer.state_spec = additional_specs[num_states // 2:]
+
+        is_keras_tensor = K.is_keras_tensor(additional_inputs[0])
+        for tensor in additional_inputs:
+            if K.is_keras_tensor(tensor) != is_keras_tensor:
+                raise ValueError('The initial state of a Bidirectional'
+                                 ' layer cannot be specified with a mix of'
+                                 ' Keras tensors and non-Keras tensors'
+                                 ' (a ""Keras tensor"" is a tensor that was'
+                                 ' returned by a Keras layer, or by `Input`)')
+
+        if is_keras_tensor:
+            # Compute the full input spec, including state
+            full_input = [inputs] + additional_inputs
+            full_input_spec = self.input_spec + additional_specs
+
+            # Perform the call with temporarily replaced input_spec
+            original_input_spec = self.input_spec
+            self.input_spec = full_input_spec
+            output = super(Bidirectional, self).__call__(full_input, **kwargs)
+            self.input_spec = original_input_spec
+            return output
+        else:
+            return super(Bidirectional, self).__call__(inputs, **kwargs)
+
     def call(self, inputs, training=None, mask=None, initial_state=None):
         kwargs = {}
         if has_arg(self.layer.call, 'training'):
@@ -321,11 +376,6 @@ class Bidirectional(Wrapper):
             kwargs['mask'] = mask
 
         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):
-            if not isinstance(initial_state, list):
-                raise ValueError(
-                    'When passing `initial_state` to a Bidirectional RNN, the state '
-                    'should be a list containing the states of the underlying RNNs. '
-                    'Found: ' + str(initial_state))
             forward_state = initial_state[:len(initial_state) // 2]
             backward_state = initial_state[len(initial_state) // 2:]
             y = self.forward_layer.call(inputs, initial_state=forward_state, **kwargs)
"
"keras","27","49f5b931410bc2e56378f20a15e8ac919e0efb88","b076e227da6beaf87d6c84eff1a92285e4662acf","keras/layers/wrappers.py","keras/layers/wrappers.py","diff --git a/keras/layers/wrappers.py b/keras/layers/wrappers.py","tests/keras/layers/wrappers_test.py","","diff --git a/keras/layers/wrappers.py b/keras/layers/wrappers.py
index c4afab1b..3bf489d1 100644
--- a/keras/layers/wrappers.py
+++ b/keras/layers/wrappers.py
@@ -487,12 +487,24 @@ class Bidirectional(Wrapper):
             return self.forward_layer.updates + self.backward_layer.updates
         return []
 
+    def get_updates_for(self, inputs=None):
+        forward_updates = self.forward_layer.get_updates_for(inputs)
+        backward_updates = self.backward_layer.get_updates_for(inputs)
+        return (super(Wrapper, self).get_updates_for(inputs) +
+                forward_updates + backward_updates)
+
     @property
     def losses(self):
         if hasattr(self.forward_layer, 'losses'):
             return self.forward_layer.losses + self.backward_layer.losses
         return []
 
+    def get_losses_for(self, inputs=None):
+        forward_losses = self.forward_layer.get_losses_for(inputs)
+        backward_losses = self.backward_layer.get_losses_for(inputs)
+        return (super(Wrapper, self).get_losses_for(inputs) +
+                forward_losses + backward_losses)
+
     @property
     def constraints(self):
         constraints = {}
"
"keras","45","d368dc870bfd8fdd4ca0ff82bd5b61aa549291c5","159bb1aac17a8de0f96997d35703b8f26926a848","keras/layers/recurrent.py","keras/layers/recurrent.py","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py","tests/keras/layers/recurrent_test.py","","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py
index ec9fa871..46eb08cd 100644
--- a/keras/layers/recurrent.py
+++ b/keras/layers/recurrent.py
@@ -1803,10 +1803,15 @@ class LSTMCell(Layer):
                 inputs_f = inputs
                 inputs_c = inputs
                 inputs_o = inputs
-            x_i = K.dot(inputs_i, self.kernel_i) + self.bias_i
-            x_f = K.dot(inputs_f, self.kernel_f) + self.bias_f
-            x_c = K.dot(inputs_c, self.kernel_c) + self.bias_c
-            x_o = K.dot(inputs_o, self.kernel_o) + self.bias_o
+            x_i = K.dot(inputs_i, self.kernel_i)
+            x_f = K.dot(inputs_f, self.kernel_f)
+            x_c = K.dot(inputs_c, self.kernel_c)
+            x_o = K.dot(inputs_o, self.kernel_o)
+            if self.use_bias:
+                x_i = K.bias_add(x_i, self.bias_i)
+                x_f = K.bias_add(x_f, self.bias_f)
+                x_c = K.bias_add(x_c, self.bias_c)
+                x_o = K.bias_add(x_o, self.bias_o)
 
             if 0 < self.recurrent_dropout < 1.:
                 h_tm1_i = h_tm1 * rec_dp_mask[0]
"
"keras","12","6dff721a3a8755356b2e89d02ef63ad8ab38ec95","6dff721a3a8755356b2e89d02ef63ad8ab38ec95","","","","tests/keras/metrics_test.py","",""
"keras","29","a341c014412cbfc86a9dd9816ae228e398dff3a2","adc321b4d7a4e22f6bdb00b404dfe5e23d4887aa","keras/engine/training.py","keras/engine/training.py","diff --git a/keras/engine/training.py b/keras/engine/training.py","tests/keras/metrics_test.py","","diff --git a/keras/engine/training.py b/keras/engine/training.py
index 78be752b..e78477e4 100644
--- a/keras/engine/training.py
+++ b/keras/engine/training.py
@@ -853,6 +853,7 @@ class Model(Container):
         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)
         self.metrics_updates = []
         self.stateful_metric_names = []
+        self.stateful_metric_functions = []
         with K.name_scope('metrics'):
             for i in range(len(self.outputs)):
                 if i in skip_target_indices:
@@ -929,6 +930,7 @@ class Model(Container):
                         # stateful metrics (i.e. metrics layers).
                         if isinstance(metric_fn, Layer) and metric_fn.stateful:
                             self.stateful_metric_names.append(metric_name)
+                            self.stateful_metric_functions.append(metric_fn)
                             self.metrics_updates += metric_fn.updates
 
                 handle_metrics(output_metrics)
@@ -1174,9 +1176,8 @@ class Model(Container):
 
         for epoch in range(initial_epoch, epochs):
             # Reset stateful metrics
-            for m in self.metrics:
-                if isinstance(m, Layer) and m.stateful:
-                    m.reset_states()
+            for m in self.stateful_metric_functions:
+                m.reset_states()
             callbacks.on_epoch_begin(epoch)
             epoch_logs = {}
             if steps_per_epoch is not None:
@@ -1363,9 +1364,8 @@ class Model(Container):
         """"""
 
         if hasattr(self, 'metrics'):
-            for m in self.metrics:
-                if isinstance(m, Layer) and m.stateful:
-                    m.reset_states()
+            for m in self.stateful_metric_functions:
+                m.reset_states()
             stateful_metric_indices = [
                 i for i, name in enumerate(self.metrics_names)
                 if str(name) in self.stateful_metric_names]
@@ -2185,9 +2185,8 @@ class Model(Container):
             # Construct epoch logs.
             epoch_logs = {}
             while epoch < epochs:
-                for m in self.metrics:
-                    if isinstance(m, Layer) and m.stateful:
-                        m.reset_states()
+                for m in self.stateful_metric_functions:
+                    m.reset_states()
                 callbacks.on_epoch_begin(epoch)
                 steps_done = 0
                 batch_index = 0
@@ -2331,9 +2330,8 @@ class Model(Container):
 
         stateful_metric_indices = []
         if hasattr(self, 'metrics'):
-            for i, m in enumerate(self.metrics):
-                if isinstance(m, Layer) and m.stateful:
-                    m.reset_states()
+            for m in self.stateful_metric_functions:
+                m.reset_states()
             stateful_metric_indices = [
                 i for i, name in enumerate(self.metrics_names)
                 if str(name) in self.stateful_metric_names]
"
"keras","34","7ef5244a2f1f7f7b76e3c804b82cbb20cdf4d139","4b74fc5418944c9f449eb88ed4b40ada280fa5ca","keras/engine/training.py;keras/utils/data_utils.py","keras/engine/training.py;keras/utils/data_utils.py","diff --git a/keras/engine/training.py b/keras/engine/training.py;diff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py","tests/test_multiprocessing.py","","diff --git a/keras/engine/training.py b/keras/engine/training.py
index ea641c3a..44d7587b 100644
--- a/keras/engine/training.py
+++ b/keras/engine/training.py
@@ -2162,7 +2162,10 @@ class Model(Container):
                         val_enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                         validation_generator = val_enqueuer.get()
                     else:
-                        validation_generator = validation_data
+                        if isinstance(validation_data, Sequence):
+                            validation_generator = iter(validation_data)
+                        else:
+                            validation_generator = validation_data
                 else:
                     if len(validation_data) == 2:
                         val_x, val_y = validation_data
@@ -2194,7 +2197,10 @@ class Model(Container):
                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                 output_generator = enqueuer.get()
             else:
-                output_generator = generator
+                if is_sequence:
+                    output_generator = iter(generator)
+                else:
+                    output_generator = generator
 
             callback_model.stop_training = False
             # Construct epoch logs.
@@ -2366,7 +2372,10 @@ class Model(Container):
                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                 output_generator = enqueuer.get()
             else:
-                output_generator = generator
+                if is_sequence:
+                    output_generator = iter(generator)
+                else:
+                    output_generator = generator
 
             while steps_done < steps:
                 generator_output = next(output_generator)
@@ -2490,7 +2499,10 @@ class Model(Container):
                 enqueuer.start(workers=workers, max_queue_size=max_queue_size)
                 output_generator = enqueuer.get()
             else:
-                output_generator = generator
+                if is_sequence:
+                    output_generator = iter(generator)
+                else:
+                    output_generator = generator
 
             if verbose == 1:
                 progbar = Progbar(target=steps)
diff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py
index 514ad655..1edd5531 100644
--- a/keras/utils/data_utils.py
+++ b/keras/utils/data_utils.py
@@ -366,6 +366,12 @@ class Sequence(object):
         """"""
         pass
 
+    def __iter__(self):
+        """"""Create an infinite generator that iterate over the Sequence.""""""
+        while True:
+            for item in (self[i] for i in range(len(self))):
+                yield item
+
 
 # Global variables to be shared across processes
 _SHARED_SEQUENCES = {}
"
"keras","4","b0bfd5201da2bfced84028bcc5bda05bdfd75af7","4185cbb50bfcae9cc30b0fc7b67e81d67a50a8ac","keras/optimizers.py","keras/optimizers.py","diff --git a/keras/optimizers.py b/keras/optimizers.py","tests/keras/optimizers_test.py","","diff --git a/keras/optimizers.py b/keras/optimizers.py
index 0dade2fd..89fe2967 100644
--- a/keras/optimizers.py
+++ b/keras/optimizers.py
@@ -703,7 +703,7 @@ class TFOptimizer(Optimizer):
 
     @interfaces.legacy_get_updates_support
     def get_updates(self, loss, params):
-        grads = self.optimizer.compute_gradients(loss, params)
+        grads = self.optimizer.compute_gradients(loss, var_list=params)
         self.updates = [K.update_add(self.iterations, 1)]
         opt_update = self.optimizer.apply_gradients(
             grads, global_step=self.iterations)
"
"keras","26","87417470c8168772559be0531e297120c569a422","97d5fa920e4f8248128f7c1b460fd9bb20d3478f","keras/backend/tensorflow_backend.py","keras/backend/tensorflow_backend.py","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py","tests/keras/backend/backend_test.py","","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py
index cd57dfbe..117c72e5 100644
--- a/keras/backend/tensorflow_backend.py
+++ b/keras/backend/tensorflow_backend.py
@@ -2871,7 +2871,10 @@ def rnn(step_function, inputs, initial_states,
                 tiled_mask_t = tf.tile(mask_t,
                                        tf.stack([1, tf.shape(output)[1]]))
                 output = tf.where(tiled_mask_t, output, states[0])
-                new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]
+                new_states = [
+                    tf.where(tf.tile(mask_t, tf.stack([1, tf.shape(new_states[i])[1]])),
+                             new_states[i], states[i]) for i in range(len(states))
+                ]
                 output_ta_t = output_ta_t.write(time, output)
                 return (time + 1, output_ta_t) + tuple(new_states)
         else:
"
"keras","5","b847b4601d608050bab6eccd049fce28b7bf1b1f","e11c48d9ce3ee47bb8a966549b14cbd5b10ee70d","keras/utils/data_utils.py","keras/utils/data_utils.py","diff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py","tests/keras/utils/data_utils_test.py","","diff --git a/keras/utils/data_utils.py b/keras/utils/data_utils.py
index 82eb431b..6c8eabc9 100644
--- a/keras/utils/data_utils.py
+++ b/keras/utils/data_utils.py
@@ -170,7 +170,10 @@ def get_file(fname,
         Path to the downloaded file
     """"""  # noqa
     if cache_dir is None:
-        cache_dir = os.path.join(os.path.expanduser('~'), '.keras')
+        if 'KERAS_HOME' in os.environ:
+            cache_dir = os.environ.get('KERAS_HOME')
+        else:
+            cache_dir = os.path.join(os.path.expanduser('~'), '.keras')
     if md5_hash is not None and file_hash is None:
         file_hash = md5_hash
         hash_algorithm = 'md5'
"
"keras","40","871007dbb0e6211459b9d16244cc3c9683459df7","4cad455ef4da600c96ddc69800bab39d0e52b677","keras/layers/recurrent.py","keras/layers/recurrent.py","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py","tests/keras/layers/recurrent_test.py","","diff --git a/keras/layers/recurrent.py b/keras/layers/recurrent.py
index a94d0623..4b371ff2 100644
--- a/keras/layers/recurrent.py
+++ b/keras/layers/recurrent.py
@@ -395,9 +395,10 @@ class RNN(Layer):
             input_shape = input_shape[0]
 
         if hasattr(self.cell.state_size, '__len__'):
-            output_dim = self.cell.state_size[0]
+            state_size = self.cell.state_size
         else:
-            output_dim = self.cell.state_size
+            state_size = [self.cell.state_size]
+        output_dim = state_size[0]
 
         if self.return_sequences:
             output_shape = (input_shape[0], input_shape[1], output_dim)
@@ -405,7 +406,7 @@ class RNN(Layer):
             output_shape = (input_shape[0], output_dim)
 
         if self.return_state:
-            state_shape = [(input_shape[0], output_dim) for _ in self.states]
+            state_shape = [(input_shape[0], dim) for dim in state_size]
             return [output_shape] + state_shape
         else:
             return output_shape
"
"keras","24","d7884570b10951d156aa086ee29a4df9eab79cf3","bcf0031b54d555179be81c088cc3df0a723d7907","keras/callbacks.py","keras/callbacks.py","diff --git a/keras/callbacks.py b/keras/callbacks.py","tests/keras/test_callbacks.py","","diff --git a/keras/callbacks.py b/keras/callbacks.py
index 0a00a950..e28e74b4 100644
--- a/keras/callbacks.py
+++ b/keras/callbacks.py
@@ -786,8 +786,12 @@ class TensorBoard(Callback):
                         tf.summary.image(mapped_weight_name, w_img)
 
                 if hasattr(layer, 'output'):
-                    tf.summary.histogram('{}_out'.format(layer.name),
-                                         layer.output)
+                    if isinstance(layer.output, list):
+                        for i, output in enumerate(layer.output):
+                            tf.summary.histogram('{}_out_{}'.format(layer.name, i), output)
+                    else:
+                        tf.summary.histogram('{}_out'.format(layer.name),
+                                             layer.output)
         self.merged = tf.summary.merge_all()
 
         if self.write_graph:
"
"keras","36","85f011df5a5c0fcf1f01b39eca338eb6b7e58401","fb1887d132a8ce8548ff53d868a6ba531cd63b34","keras/backend/tensorflow_backend.py","keras/backend/tensorflow_backend.py","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py","tests/keras/layers/convolutional_test.py","","diff --git a/keras/backend/tensorflow_backend.py b/keras/backend/tensorflow_backend.py
index 8fdb9aa8..27f8afed 100644
--- a/keras/backend/tensorflow_backend.py
+++ b/keras/backend/tensorflow_backend.py
@@ -3416,10 +3416,10 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,
     padding = _preprocess_padding(padding)
     if tf_data_format == 'NHWC':
         spatial_start_dim = 1
-        strides = (1, 1) + strides + (1,)
+        strides = (1,) + strides * 2 + (1,)
     else:
         spatial_start_dim = 2
-        strides = (1, 1, 1) + strides
+        strides = (1, 1) + strides * 2
     x = tf.expand_dims(x, spatial_start_dim)
     depthwise_kernel = tf.expand_dims(depthwise_kernel, 0)
     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)
"
"cookiecutter","3","5c282f020a8db7e5e7c4e7b51b010556ca31fb7f","7129d474206761a6156925db78eee4b62a0e3944","cookiecutter/prompt.py","cookiecutter/prompt.py","diff --git a/cookiecutter/prompt.py b/cookiecutter/prompt.py","tests/test_read_user_choice.py","","diff --git a/cookiecutter/prompt.py b/cookiecutter/prompt.py
index 45a065b..da4ada2 100644
--- a/cookiecutter/prompt.py
+++ b/cookiecutter/prompt.py
@@ -88,7 +88,7 @@ def read_user_choice(var_name, options):
     ))
 
     user_choice = click.prompt(
-        prompt, type=click.Choice(choices), default=default
+        prompt, type=click.Choice(choices), default=default, show_choices=False
     )
     return choice_map[user_choice]
 
"
"cookiecutter","2","d7e7b28811e474e14d1bed747115e47dcdd15ba3","90434ff4ea4477941444f1e83313beb414838535","cookiecutter/hooks.py","cookiecutter/hooks.py","diff --git a/cookiecutter/hooks.py b/cookiecutter/hooks.py","tests/test_hooks.py","","diff --git a/cookiecutter/hooks.py b/cookiecutter/hooks.py
index 20ccae2..3c73f74 100644
--- a/cookiecutter/hooks.py
+++ b/cookiecutter/hooks.py
@@ -54,11 +54,14 @@ def find_hook(hook_name, hooks_dir='hooks'):
         logger.debug('No hooks/dir in template_dir')
         return None
 
+    scripts = []
     for hook_file in os.listdir(hooks_dir):
         if valid_hook(hook_file, hook_name):
-            return os.path.abspath(os.path.join(hooks_dir, hook_file))
+            scripts.append(os.path.abspath(os.path.join(hooks_dir, hook_file)))
 
-    return None
+    if len(scripts) == 0:
+        return None
+    return scripts
 
 
 def run_script(script_path, cwd='.'):
@@ -119,9 +122,10 @@ def run_hook(hook_name, project_dir, context):
     :param project_dir: The directory to execute the script from.
     :param context: Cookiecutter project context.
     """"""
-    script = find_hook(hook_name)
-    if script is None:
+    scripts = find_hook(hook_name)
+    if not scripts:
         logger.debug('No %s hook found', hook_name)
         return
     logger.debug('Running hook %s', hook_name)
-    run_script_with_context(script, project_dir, context)
+    for script in scripts:
+        run_script_with_context(script, project_dir, context)
"
"cookiecutter","1","c15633745df6abdb24e02746b82aadb20b8cdf8c","7f6804c4953a18386809f11faf4d86898570debc","cookiecutter/generate.py","cookiecutter/generate.py","diff --git a/cookiecutter/generate.py b/cookiecutter/generate.py","tests/test_generate_context.py","","diff --git a/cookiecutter/generate.py b/cookiecutter/generate.py
index 37365a4..c526b97 100644
--- a/cookiecutter/generate.py
+++ b/cookiecutter/generate.py
@@ -82,7 +82,7 @@ def generate_context(
     context = OrderedDict([])
 
     try:
-        with open(context_file) as file_handle:
+        with open(context_file, encoding='utf-8') as file_handle:
             obj = json.load(file_handle, object_pairs_hook=OrderedDict)
     except ValueError as e:
         # JSON decoding error.  Let's throw a new exception that is more
"
"cookiecutter","4","9568ab6ecd2d6836646006c59473c4a4ac0dee04","457a1a4e862aab4102b644ff1d2b2e2b5a766b3c","cookiecutter/exceptions.py;cookiecutter/generate.py;cookiecutter/hooks.py","cookiecutter/exceptions.py;cookiecutter/generate.py;cookiecutter/hooks.py","diff --git a/cookiecutter/exceptions.py b/cookiecutter/exceptions.py;diff --git a/cookiecutter/generate.py b/cookiecutter/generate.py;diff --git a/cookiecutter/hooks.py b/cookiecutter/hooks.py","tests/test_hooks.py","","diff --git a/cookiecutter/exceptions.py b/cookiecutter/exceptions.py
index 0ad6b58..415f99e 100755
--- a/cookiecutter/exceptions.py
+++ b/cookiecutter/exceptions.py
@@ -81,3 +81,9 @@ class InvalidModeException(CookiecutterException):
     Raised when cookiecutter is called with both `no_input==True` and
     `replay==True` at the same time.
     """"""
+
+
+class FailedHookException(CookiecutterException):
+    """"""
+    Raised when a hook script fails
+    """"""
diff --git a/cookiecutter/generate.py b/cookiecutter/generate.py
index a38ef26..d69f1de 100755
--- a/cookiecutter/generate.py
+++ b/cookiecutter/generate.py
@@ -24,11 +24,12 @@ from binaryornot.check import is_binary
 from .exceptions import (
     NonTemplatedInputDirException,
     ContextDecodingException,
+    FailedHookException,
     OutputDirExistsException
 )
 from .find import find_template
 from .utils import make_sure_path_exists, work_in
-from .hooks import run_hook, EXIT_SUCCESS
+from .hooks import run_hook
 
 
 def copy_without_render(path, context):
@@ -257,7 +258,10 @@ def generate_files(repo_dir, context=None, output_dir='.',
 
     # run pre-gen hook from repo_dir
     with work_in(repo_dir):
-        if run_hook('pre_gen_project', project_dir, context) != EXIT_SUCCESS:
+        try:
+            run_hook('pre_gen_project', project_dir, context)
+        except FailedHookException:
+            shutil.rmtree(project_dir, ignore_errors=True)
             logging.error(""Stopping generation because pre_gen_project""
                           "" hook script didn't exit sucessfully"")
             return
diff --git a/cookiecutter/hooks.py b/cookiecutter/hooks.py
index 550d6de..81045d1 100755
--- a/cookiecutter/hooks.py
+++ b/cookiecutter/hooks.py
@@ -18,6 +18,7 @@ import tempfile
 from jinja2 import Template
 
 from cookiecutter import utils
+from .exceptions import FailedHookException
 
 
 _HOOKS = [
@@ -69,7 +70,10 @@ def run_script(script_path, cwd='.'):
         shell=run_thru_shell,
         cwd=cwd
     )
-    return proc.wait()
+    exit_status = proc.wait()
+    if exit_status != EXIT_SUCCESS:
+        raise FailedHookException(
+            ""Hook script failed (exit status: %d)"" % exit_status)
 
 
 def run_script_with_context(script_path, cwd, context):
@@ -91,7 +95,7 @@ def run_script_with_context(script_path, cwd, context):
     ) as temp:
         temp.write(Template(contents).render(**context))
 
-    return run_script(temp.name, cwd)
+    run_script(temp.name, cwd)
 
 
 def run_hook(hook_name, project_dir, context):
@@ -105,5 +109,5 @@ def run_hook(hook_name, project_dir, context):
     script = find_hooks().get(hook_name)
     if script is None:
         logging.debug('No hooks found')
-        return EXIT_SUCCESS
-    return run_script_with_context(script, project_dir, context)
+        return
+    run_script_with_context(script, project_dir, context)
"
"tqdm","3","c2599e3cd6087429f48bae34347ec5d2473c8392","73962a47026dd980ac0758820efc9c41cbf938e0","tqdm/_tqdm.py","tqdm/_tqdm.py","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py","tqdm/tests/tests_tqdm.py","","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py
index 318249b..9e54c5f 100755
--- a/tqdm/_tqdm.py
+++ b/tqdm/_tqdm.py
@@ -947,6 +947,17 @@ class tqdm(Comparable):
         # NB: Avoid race conditions by setting start_t at the very end of init
         self.start_t = self.last_print_t
 
+    def __bool__(self):
+        if self.total is not None:
+            return self.total > 0
+        if self.iterable is None:
+            raise TypeError('Boolean cast is undefined'
+                            ' for tqdm objects that have no iterable or total')
+        return bool(self.iterable)
+
+    def __nonzero__(self):
+        return self.__bool__()
+
     def __len__(self):
         return self.total if self.iterable is None else \
             (self.iterable.shape[0] if hasattr(self.iterable, ""shape"")
"
"tqdm","2","bef86db56654d271838b145ad77f7040a73a7b4d","127af5caf19e7d29c346f5ca8a9c7ef3004b664b","tqdm/std.py;tqdm/utils.py","tqdm/std.py;tqdm/utils.py","diff --git a/tqdm/std.py b/tqdm/std.py;diff --git a/tqdm/utils.py b/tqdm/utils.py","tqdm/tests/tests_tqdm.py","","diff --git a/tqdm/std.py b/tqdm/std.py
index 0b57f31..14ab11a 100644
--- a/tqdm/std.py
+++ b/tqdm/std.py
@@ -485,8 +485,7 @@ class tqdm(Comparable):
             if not _is_ascii(full_bar.charset) and _is_ascii(bar_format):
                 bar_format = _unicode(bar_format)
             res = bar_format.format(bar=full_bar, **format_dict)
-            if ncols:
-                return disp_trim(res, ncols)
+            return disp_trim(res, ncols) if ncols else res
 
         elif bar_format:
             # user-specified bar_format but no total
@@ -502,8 +501,7 @@ class tqdm(Comparable):
                 if ncols else 10,
                 charset=Bar.BLANK)
             res = bar_format.format(bar=full_bar, **format_dict)
-            if ncols:
-                return disp_trim(res, ncols)
+            return disp_trim(res, ncols) if ncols else res
         else:
             # no total: no progressbar, ETA, just progress stats
             return ((prefix + "": "") if prefix else '') + \
diff --git a/tqdm/utils.py b/tqdm/utils.py
index 474b1c8..a9a42be 100644
--- a/tqdm/utils.py
+++ b/tqdm/utils.py
@@ -360,8 +360,10 @@ def disp_trim(data, length):
     if len(data) == disp_len(data):
         return data[:length]
 
+    ansi_present = bool(RE_ANSI.search(data))
     while disp_len(data) > length:  # carefully delete one char at a time
         data = data[:-1]
-    if RE_ANSI.search(data):  # assume ANSI reset is required
-        return data + ""\033[0m""
+    if ansi_present and bool(RE_ANSI.search(data)):
+        # assume ANSI reset is required
+        return data if data.endswith(""\033[0m"") else data + ""\033[0m""
     return data
"
"tqdm","8","08b8ad1ff3bd003ef8309faaa0cc108ffa40317d","cae9d139c6df5614be3bf6e25ccbd600ee3286dc","tqdm/_tqdm.py","tqdm/_tqdm.py","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py","tqdm/tests/tests_tqdm.py","","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py
index 469947a..cb28ec5 100644
--- a/tqdm/_tqdm.py
+++ b/tqdm/_tqdm.py
@@ -222,7 +222,7 @@ class tqdm(object):
                     # Format left/right sides of the bar, and format the bar
                     # later in the remaining space (avoid breaking display)
                     l_bar_user, r_bar_user = bar_format.split('{bar}')
-                    l_bar, r_bar = l_bar.format(**bar_args), r_bar.format(**bar_args)
+                    l_bar, r_bar = l_bar_user.format(**bar_args), r_bar_user.format(**bar_args)
                 else:
                     # Else no progress bar, we can just format and return
                     return bar_format.format(**bar_args)
"
"tqdm","1","8cc777fe8401a05d07f2c97e65d15e4460feab88","c0dcf39b046d1b4ff6de14ac99ad9a1b10487512","tqdm/contrib/__init__.py","tqdm/contrib/__init__.py","diff --git a/tqdm/contrib/__init__.py b/tqdm/contrib/__init__.py","tqdm/tests/tests_contrib.py","","diff --git a/tqdm/contrib/__init__.py b/tqdm/contrib/__init__.py
index 1dddacf..935ab63 100644
--- a/tqdm/contrib/__init__.py
+++ b/tqdm/contrib/__init__.py
@@ -38,7 +38,7 @@ def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto,
         if isinstance(iterable, np.ndarray):
             return tqdm_class(np.ndenumerate(iterable),
                               total=total or len(iterable), **tqdm_kwargs)
-    return enumerate(tqdm_class(iterable, start, **tqdm_kwargs))
+    return enumerate(tqdm_class(iterable, **tqdm_kwargs), start)
 
 
 def _tzip(iter1, *iter2plus, **tqdm_kwargs):
"
"tqdm","7","caefe02fd6f3165e5634460ab20caf4c60400120","4efd35246c924236f34d8130b1055a3c3ba78605","tqdm/_main.py","tqdm/_main.py","diff --git a/tqdm/_main.py b/tqdm/_main.py","tqdm/tests/tests_main.py","","diff --git a/tqdm/_main.py b/tqdm/_main.py
index 9c9273f..b2fd1d3 100644
--- a/tqdm/_main.py
+++ b/tqdm/_main.py
@@ -89,7 +89,7 @@ def posix_pipe(fin, fout, delim='\n', buf_size=256,
 # ((opt, type), ... )
 RE_OPTS = re.compile(r'\n {8}(\S+)\s{2,}:\s*([^,]+)')
 # better split method assuming no positional args
-RE_SHLEX = re.compile(r'\s*--?([^\s=]+)(?:\s*|=|$)')
+RE_SHLEX = re.compile(r'\s*(?<!\S)--?([^\s=]+)(?:\s*|=|$)')
 
 # TODO: add custom support for some of the following?
 UNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')
"
"tqdm","9","9da1d5d116aec7a23d8f6dc22d5e23ecb1c40a7c","d877c1dfb4739852105f7b967a8fceb869ac5042","tqdm/_tqdm.py","tqdm/_tqdm.py","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py","tqdm/tests/tests_tqdm.py","","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py
index da02d13..c4df592 100644
--- a/tqdm/_tqdm.py
+++ b/tqdm/_tqdm.py
@@ -38,9 +38,9 @@ def format_sizeof(num, suffix=''):
         Number with Order of Magnitude SI unit postfix.
     """"""
     for unit in ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z']:
-        if abs(num) < 1000.0:
-            if abs(num) < 100.0:
-                if abs(num) < 10.0:
+        if abs(num) < 999.95:
+            if abs(num) < 99.95:
+                if abs(num) < 9.995:
                     return '{0:1.2f}'.format(num) + unit + suffix
                 return '{0:2.1f}'.format(num) + unit + suffix
             return '{0:3.0f}'.format(num) + unit + suffix
@@ -271,7 +271,7 @@ class tqdm(object):
         if ascii is None:
             ascii = not _supports_unicode(file)
 
-        if gui: # pragma: no cover
+        if gui:  # pragma: no cover
             try:
                 import matplotlib as mpl
                 import matplotlib.pyplot as plt
@@ -298,7 +298,7 @@ class tqdm(object):
         self.unit_scale = unit_scale
         self.gui = gui
 
-        if gui: # pragma: no cover
+        if gui:  # pragma: no cover
             # Initialize the GUI display
             if not disable:
                 file.write('Warning: GUI is experimental/alpha\n')
@@ -360,7 +360,7 @@ class tqdm(object):
         self.n = 0
 
     def __len__(self):
-        return len(self.iterable)
+        return len(self.iterable) if self.iterable else self.total
 
     def __iter__(self):
         ''' Backward-compatibility to use: for x in tqdm(iterable) '''
@@ -386,7 +386,7 @@ class tqdm(object):
             last_print_n = self.last_print_n
             n = self.n
             gui = self.gui
-            if gui: # pragma: no cover
+            if gui:  # pragma: no cover
                 plt = self.plt
                 ax = self.ax
                 xdata = self.xdata
@@ -409,7 +409,7 @@ class tqdm(object):
                     delta_t = cur_t - last_print_t
                     if delta_t >= mininterval:
                         elapsed = cur_t - start_t
-                        if gui: # pragma: no cover
+                        if gui:  # pragma: no cover
                             # Inline due to multiple calls
                             total = self.total
                             # instantaneous rate
"
"tqdm","6","a4b9c86db548b2d0dbb5af7a6bbdc26ab47e1eec","6dad2e89019317e875c46d5a3a82a811ad6de2f9","tqdm/_tqdm.py","tqdm/_tqdm.py","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py","tqdm/tests/tests_synchronisation.py","","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py
index 04b2961..ab42ed8 100755
--- a/tqdm/_tqdm.py
+++ b/tqdm/_tqdm.py
@@ -853,7 +853,7 @@ class tqdm(object):
         return self.total if self.iterable is None else \
             (self.iterable.shape[0] if hasattr(self.iterable, ""shape"")
              else len(self.iterable) if hasattr(self.iterable, ""__len__"")
-             else self.total)
+             else getattr(self, ""total"", None))
 
     def __enter__(self):
         return self
"
"tqdm","4","03b347646492131d889871939b40457d29147216","964dee631d0ed30e2f799b42fc58ba5e73795a08","tqdm/_tqdm.py","tqdm/_tqdm.py","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py","tqdm/tests/tests_tqdm.py","","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py
index df6414e..ea58409 100755
--- a/tqdm/_tqdm.py
+++ b/tqdm/_tqdm.py
@@ -320,7 +320,8 @@ class tqdm(Comparable):
 
         # apply custom scale if necessary
         if unit_scale and unit_scale not in (True, 1):
-            total *= unit_scale
+            if total:
+                total *= unit_scale
             n *= unit_scale
             if rate:
                 rate *= unit_scale  # by default rate = 1 / self.avg_time
"
"tqdm","5","19b08ab34fdbfa0275bc5cb2430436c724c7e759","4f340697af69b71850aad496387c9c5aa1904136","tqdm/_tqdm.py","tqdm/_tqdm.py","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py","tqdm/tests/tests_tqdm.py","","diff --git a/tqdm/_tqdm.py b/tqdm/_tqdm.py
index f0261da..2ab2854 100755
--- a/tqdm/_tqdm.py
+++ b/tqdm/_tqdm.py
@@ -748,12 +748,19 @@ class tqdm(Comparable):
         if disable is None and hasattr(file, ""isatty"") and not file.isatty():
             disable = True
 
+        if total is None and iterable is not None:
+            try:
+                total = len(iterable)
+            except (TypeError, AttributeError):
+                total = None
+
         if disable:
             self.iterable = iterable
             self.disable = disable
             self.pos = self._get_free_pos(self)
             self._instances.remove(self)
             self.n = initial
+            self.total = total
             return
 
         if kwargs:
@@ -766,12 +773,6 @@ class tqdm(Comparable):
                 else TqdmKeyError(""Unknown argument(s): "" + str(kwargs)))
 
         # Preprocess the arguments
-        if total is None and iterable is not None:
-            try:
-                total = len(iterable)
-            except (TypeError, AttributeError):
-                total = None
-
         if ((ncols is None) and (file in (sys.stderr, sys.stdout))) or \
                 dynamic_ncols:  # pragma: no cover
             if dynamic_ncols:
"
"sanic","3","91f6abaa81248189fbcbdf685e8bdcbb7846609f","861e87347a2d373d6ffa387965a6887c83af632c","sanic/app.py","sanic/app.py","diff --git a/sanic/app.py b/sanic/app.py","tests/test_url_for.py","","diff --git a/sanic/app.py b/sanic/app.py
index abdd36f..d12dc59 100644
--- a/sanic/app.py
+++ b/sanic/app.py
@@ -830,6 +830,14 @@ class Sanic:
                 ""Endpoint with name `{}` was not found"".format(view_name)
             )
 
+        # If the route has host defined, split that off
+        # TODO: Retain netloc and path separately in Route objects
+        host = uri.find(""/"")
+        if host > 0:
+            host, uri = uri[:host], uri[host:]
+        else:
+            host = None
+
         if view_name == ""static"" or view_name.endswith("".static""):
             filename = kwargs.pop(""filename"", None)
             # it's static folder
@@ -862,7 +870,7 @@ class Sanic:
 
         netloc = kwargs.pop(""_server"", None)
         if netloc is None and external:
-            netloc = self.config.get(""SERVER_NAME"", """")
+            netloc = host or self.config.get(""SERVER_NAME"", """")
 
         if external:
             if not scheme:
"
"sanic","2","ba9b432993019b0af0c4827a5ed42aaa091bd17d","801595e24acdf8050b8d3ffa512d424147848d32","sanic/server.py","sanic/server.py","diff --git a/sanic/server.py b/sanic/server.py","tests/test_app.py","","diff --git a/sanic/server.py b/sanic/server.py
index 47ebcd9..b9e7219 100644
--- a/sanic/server.py
+++ b/sanic/server.py
@@ -735,6 +735,26 @@ class AsyncioServer:
             task = asyncio.ensure_future(coro, loop=self.loop)
             return task
 
+    def start_serving(self):
+        if self.server:
+            try:
+                return self.server.start_serving()
+            except AttributeError:
+                raise NotImplementedError(
+                    ""server.start_serving not available in this version ""
+                    ""of asyncio or uvloop.""
+                )
+
+    def serve_forever(self):
+        if self.server:
+            try:
+                return self.server.serve_forever()
+            except AttributeError:
+                raise NotImplementedError(
+                    ""server.serve_forever not available in this version ""
+                    ""of asyncio or uvloop.""
+                )
+
     def __await__(self):
         """"""Starts the asyncio server, returns AsyncServerCoro""""""
         task = asyncio.ensure_future(self.serve_coro)
"
"sanic","1","e7001b00747b659f7042b0534802b936ee8a53e0","44973125c15304b4262c51c78b5a86bd1daafa86","sanic/app.py","sanic/app.py","diff --git a/sanic/app.py b/sanic/app.py","tests/test_blueprints.py","","diff --git a/sanic/app.py b/sanic/app.py
index 6d7f2da..7ef1c94 100644
--- a/sanic/app.py
+++ b/sanic/app.py
@@ -653,7 +653,7 @@ class Sanic:
                 if _rn not in self.named_response_middleware:
                     self.named_response_middleware[_rn] = deque()
                 if middleware not in self.named_response_middleware[_rn]:
-                    self.named_response_middleware[_rn].append(middleware)
+                    self.named_response_middleware[_rn].appendleft(middleware)
 
     # Decorator
     def middleware(self, middleware_or_request):
"
"sanic","4","e506c89304948bba593e8603ecace1c495b06fd5","e81a8ce07329e95d3d0899b1d774f21759c28e0e","sanic/request.py","sanic/request.py","diff --git a/sanic/request.py b/sanic/request.py","tests/test_requests.py","","diff --git a/sanic/request.py b/sanic/request.py
index 972f7db..9712aeb 100644
--- a/sanic/request.py
+++ b/sanic/request.py
@@ -519,8 +519,11 @@ class Request:
         :rtype: str
         """"""
         # Full URL SERVER_NAME can only be handled in app.url_for
-        if ""//"" in self.app.config.SERVER_NAME:
-            return self.app.url_for(view_name, _external=True, **kwargs)
+        try:
+            if ""//"" in self.app.config.SERVER_NAME:
+                return self.app.url_for(view_name, _external=True, **kwargs)
+        except AttributeError:
+            pass
 
         scheme = self.scheme
         host = self.server_name
"
"sanic","5","e3a27c2cc485d57aa1ff87d9f69098e4ab12727e","b63c06c75a54752d7f3115d3c635580db44b8399","sanic/log.py","sanic/log.py","diff --git a/sanic/log.py b/sanic/log.py","tests/test_logging.py","","diff --git a/sanic/log.py b/sanic/log.py
index cb8ca52..08fc835 100644
--- a/sanic/log.py
+++ b/sanic/log.py
@@ -6,7 +6,7 @@ LOGGING_CONFIG_DEFAULTS = dict(
     version=1,
     disable_existing_loggers=False,
     loggers={
-        ""root"": {""level"": ""INFO"", ""handlers"": [""console""]},
+        ""sanic.root"": {""level"": ""INFO"", ""handlers"": [""console""]},
         ""sanic.error"": {
             ""level"": ""INFO"",
             ""handlers"": [""error_console""],
"
"httpie","3","8c33e5e3d31d3cd6476c4d9bc963a4c529f883d2","589887939507ff26d36ec74bd2c045819cfa3d56","httpie/sessions.py","httpie/sessions.py","diff --git a/httpie/sessions.py b/httpie/sessions.py","tests/test_sessions.py","","diff --git a/httpie/sessions.py b/httpie/sessions.py
index e61a8e3..32254bf 100644
--- a/httpie/sessions.py
+++ b/httpie/sessions.py
@@ -101,6 +101,10 @@ class Session(BaseConfigDict):
 
         """"""
         for name, value in request_headers.items():
+
+            if value is None:
+                continue  # Ignore explicitely unset headers
+
             value = value.decode('utf8')
             if name == 'User-Agent' and value.startswith('HTTPie/'):
                 continue
"
"httpie","2","356e0436510fee70b4071fac58be81c0a0a7db59","e18b609ef7d867d6efa0efe42c832be5e0d09338","httpie/client.py;httpie/core.py","httpie/client.py;httpie/core.py","diff --git a/httpie/client.py b/httpie/client.py;diff --git a/httpie/core.py b/httpie/core.py","tests/test_redirects.py","","diff --git a/httpie/client.py b/httpie/client.py
index 1115f4d..482f9dc 100644
--- a/httpie/client.py
+++ b/httpie/client.py
@@ -40,6 +40,7 @@ def get_response(args, config_dir):
     """"""Send the request and return a `request.Response`.""""""
 
     requests_session = get_requests_session()
+    requests_session.max_redirects = args.max_redirects
 
     if not args.session and not args.session_read_only:
         kwargs = get_requests_kwargs(args)
diff --git a/httpie/core.py b/httpie/core.py
index c0014ce..3dbe7f2 100644
--- a/httpie/core.py
+++ b/httpie/core.py
@@ -192,7 +192,6 @@ def main(args=sys.argv[1:], env=Environment(), error=None):
         error('Too many redirects (--max-redirects=%s).', args.max_redirects)
     except Exception as e:
         # TODO: Better distinction between expected and unexpected errors.
-        #       Network errors vs. bugs, etc.
         if traceback:
             raise
         msg = str(e)
"
"httpie","1","001bda19450ad85c91345eea3cfa3991e1d492ba","5300b0b490b8db48fac30b5e32164be93dc574b7","httpie/downloads.py","httpie/downloads.py","diff --git a/httpie/downloads.py b/httpie/downloads.py","tests/test_downloads.py","","diff --git a/httpie/downloads.py b/httpie/downloads.py
index b49e335..972151e 100644
--- a/httpie/downloads.py
+++ b/httpie/downloads.py
@@ -7,6 +7,7 @@ from __future__ import division
 import os
 import re
 import sys
+import errno
 import mimetypes
 import threading
 from time import sleep, time
@@ -135,12 +136,43 @@ def filename_from_url(url, content_type):
     return fn
 
 
+def trim_filename(filename, max_len):
+    if len(filename) > max_len:
+        trim_by = len(filename) - max_len
+        name, ext = os.path.splitext(filename)
+        if trim_by >= len(name):
+            filename = filename[:-trim_by]
+        else:
+            filename = name[:-trim_by] + ext
+    return filename
+
+
+def get_filename_max_length(directory):
+    try:
+        max_len = os.pathconf(directory, 'PC_NAME_MAX')
+    except OSError as e:
+        if e.errno == errno.EINVAL:
+            max_len = 255
+        else:
+            raise
+    return max_len
+
+
+def trim_filename_if_needed(filename, directory='.', extra=0):
+    max_len = get_filename_max_length(directory) - extra
+    if len(filename) > max_len:
+        filename = trim_filename(filename, max_len)
+    return filename
+
+
 def get_unique_filename(filename, exists=os.path.exists):
     attempt = 0
     while True:
         suffix = '-' + str(attempt) if attempt > 0 else ''
-        if not exists(filename + suffix):
-            return filename + suffix
+        try_filename = trim_filename_if_needed(filename, extra=len(suffix))
+        try_filename += suffix
+        if not exists(try_filename):
+            return try_filename
         attempt += 1
 
 
"
"httpie","4","8c892edd4fe700a7ca5cc733dcb4817831d253e2","040d981f00c3f6830b2d0db3daf3c64c080e96e3","httpie/models.py","httpie/models.py","diff --git a/httpie/models.py b/httpie/models.py","tests/test_regressions.py","","diff --git a/httpie/models.py b/httpie/models.py
index f6b9dff..af6163f 100644
--- a/httpie/models.py
+++ b/httpie/models.py
@@ -102,8 +102,7 @@ class HTTPRequest(HTTPMessage):
         )
 
         headers = dict(self._orig.headers)
-
-        if 'Host' not in headers:
+        if 'Host' not in self._orig.headers:
             headers['Host'] = url.netloc.split('@')[-1]
 
         headers = ['%s: %s' % (name, value)
"
"httpie","5","16df8848e81eefac830f407e4b985f42b52970da","90af1f742230831792d74d303d1e7ce56c96d4bd","httpie/cli.py","httpie/cli.py","diff --git a/httpie/cli.py b/httpie/cli.py","tests/tests.py","","diff --git a/httpie/cli.py b/httpie/cli.py
index 22797a5..29a55f6 100644
--- a/httpie/cli.py
+++ b/httpie/cli.py
@@ -36,14 +36,24 @@ class KeyValueType(object):
     """"""A type used with `argparse`.""""""
     def __init__(self, *separators):
         self.separators = separators
+        self.escapes = ['\\\\' + sep for sep in separators]
 
     def __call__(self, string):
         found = {}
+        found_escapes = []
+        for esc in self.escapes:
+            found_escapes += [m.span() for m in re.finditer(esc, string)]
         for sep in self.separators:
-            regex = '[^\\\\]' + sep
-            match = re.search(regex, string)
-            if match:
-                found[match.start() + 1] = sep
+            matches = re.finditer(sep, string)
+            for match in matches:
+                start, end = match.span()
+                inside_escape = False
+                for estart, eend in found_escapes:
+                    if start >= estart and end <= eend:
+                        inside_escape = True
+                        break
+                if not inside_escape:
+                    found[start] = sep
 
         if not found:
             #noinspection PyExceptionInherit
"
"matplotlib","22","a87cd8a4c43688137155accdc57ed64fb95e2d40","2349d826a170a10510d1ee8be02eaff51f7ee89f","lib/matplotlib/axes/_axes.py","lib/matplotlib/axes/_axes.py","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py
index 636868f87..9d609aa98 100644
--- a/lib/matplotlib/axes/_axes.py
+++ b/lib/matplotlib/axes/_axes.py
@@ -6573,6 +6573,9 @@ optional.
         if bin_range is not None:
             bin_range = self.convert_xunits(bin_range)
 
+        if not cbook.is_scalar_or_string(bins):
+            bins = self.convert_xunits(bins)
+
         # We need to do to 'weights' what was done to 'x'
         if weights is not None:
             w = cbook._reshape_2D(weights, 'weights')
"
"matplotlib","18","7d958c63f4fbcd8a28df666d738400b251f89af7","47479b04b6718a65ebaac094db106d44b40da509","lib/matplotlib/projections/polar.py","lib/matplotlib/projections/polar.py","diff --git a/lib/matplotlib/projections/polar.py b/lib/matplotlib/projections/polar.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/projections/polar.py b/lib/matplotlib/projections/polar.py
index c33fa0560..61f2e5a6a 100644
--- a/lib/matplotlib/projections/polar.py
+++ b/lib/matplotlib/projections/polar.py
@@ -398,6 +398,7 @@ class RadialLocator(mticker.Locator):
     :class:`~matplotlib.ticker.Locator` (which may be different
     depending on the scale of the *r*-axis.
     """"""
+
     def __init__(self, base, axes=None):
         self.base = base
         self._axes = axes
@@ -429,6 +430,11 @@ class RadialLocator(mticker.Locator):
         # docstring inherited
         return self.base.refresh()
 
+    def nonsingular(self, vmin, vmax):
+        # docstring inherited
+        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)  # Init. limits.
+                else self.base.nonsingular(vmin, vmax))
+
     def view_limits(self, vmin, vmax):
         vmin, vmax = self.base.view_limits(vmin, vmax)
         if vmax > vmin:
"
"matplotlib","14","dbc35a9d625e162445f864b9f463c9961888e901","c986a12b6d0d0d44f5a87f7cd4408f38040e2537","lib/matplotlib/text.py","lib/matplotlib/text.py","diff --git a/lib/matplotlib/text.py b/lib/matplotlib/text.py","lib/matplotlib/tests/test_text.py","","diff --git a/lib/matplotlib/text.py b/lib/matplotlib/text.py
index 78fb5c57f..2a8a708e7 100644
--- a/lib/matplotlib/text.py
+++ b/lib/matplotlib/text.py
@@ -174,8 +174,12 @@ class Text(Artist):
 
     def update(self, kwargs):
         # docstring inherited
-        # Update bbox last, as it depends on font properties.
         sentinel = object()  # bbox can be None, so use another sentinel.
+        # Update fontproperties first, as it has lowest priority.
+        fontproperties = kwargs.pop(""fontproperties"", sentinel)
+        if fontproperties is not sentinel:
+            self.set_fontproperties(fontproperties)
+        # Update bbox last, as it depends on font properties.
         bbox = kwargs.pop(""bbox"", sentinel)
         super().update(kwargs)
         if bbox is not sentinel:
"
"matplotlib","3","5e046f72ae82788788c7e9b9354b87b131891cd8","2a3707d9c3472b1a010492322b6946388d4989ae","lib/matplotlib/markers.py","lib/matplotlib/markers.py","diff --git a/lib/matplotlib/markers.py b/lib/matplotlib/markers.py","lib/matplotlib/tests/test_marker.py","","diff --git a/lib/matplotlib/markers.py b/lib/matplotlib/markers.py
index afa8c105e..e9c5fbc21 100644
--- a/lib/matplotlib/markers.py
+++ b/lib/matplotlib/markers.py
@@ -232,7 +232,10 @@ class MarkerStyle:
         self._snap_threshold = None
         self._joinstyle = 'round'
         self._capstyle = 'butt'
-        self._filled = True
+        # Initial guess: Assume the marker is filled unless the fillstyle is
+        # set to 'none'. The marker function will override this for unfilled
+        # markers.
+        self._filled = self._fillstyle != 'none'
         self._marker_function()
 
     def __bool__(self):
"
"matplotlib","16","89ff308306bafac22647c050a42141f040210216","5d99e151be80bcb0b3b6d081fd3038330f573d94","lib/matplotlib/transforms.py","lib/matplotlib/transforms.py","diff --git a/lib/matplotlib/transforms.py b/lib/matplotlib/transforms.py","lib/matplotlib/tests/test_colorbar.py","","diff --git a/lib/matplotlib/transforms.py b/lib/matplotlib/transforms.py
index e4a37373e..481b70741 100644
--- a/lib/matplotlib/transforms.py
+++ b/lib/matplotlib/transforms.py
@@ -2812,6 +2812,10 @@ def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):
         vmin, vmax = vmax, vmin
         swapped = True
 
+    # Expand vmin, vmax to float: if they were integer types, they can wrap
+    # around in abs (abs(np.int8(-128)) == -128) and vmax - vmin can overflow.
+    vmin, vmax = map(float, [vmin, vmax])
+
     maxabsvalue = max(abs(vmin), abs(vmax))
     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:
         vmin = -expander
"
"matplotlib","2","2a3707d9c3472b1a010492322b6946388d4989ae","d86cc2bab8183fd3288ed474e4dfd33e0f018908","lib/matplotlib/axes/_axes.py","lib/matplotlib/axes/_axes.py","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py
index f1a16cbd6..81f8aa4ee 100644
--- a/lib/matplotlib/axes/_axes.py
+++ b/lib/matplotlib/axes/_axes.py
@@ -4393,8 +4393,9 @@ default: :rc:`scatter.edgecolors`
             - 'none': No patch boundary will be drawn.
             - A color or sequence of colors.
 
-            For non-filled markers, the *edgecolors* kwarg is ignored and
-            forced to 'face' internally.
+            For non-filled markers, *edgecolors* is ignored. Instead, the color
+            is determined like with 'face', i.e. from *c*, *colors*, or
+            *facecolors*.
 
         plotnonfinite : bool, default: False
             Set to plot points with nonfinite *c*, in conjunction with
@@ -4476,7 +4477,6 @@ default: :rc:`scatter.edgecolors`
         path = marker_obj.get_path().transformed(
             marker_obj.get_transform())
         if not marker_obj.is_filled():
-            edgecolors = 'face'
             if linewidths is None:
                 linewidths = rcParams['lines.linewidth']
             elif np.iterable(linewidths):
@@ -4488,8 +4488,8 @@ default: :rc:`scatter.edgecolors`
 
         collection = mcoll.PathCollection(
                 (path,), scales,
-                facecolors=colors,
-                edgecolors=edgecolors,
+                facecolors=colors if marker_obj.is_filled() else 'none',
+                edgecolors=edgecolors if marker_obj.is_filled() else colors,
                 linewidths=linewidths,
                 offsets=offsets,
                 transOffset=kwargs.pop('transform', self.transData),
"
"matplotlib","8","54bd6f19b23dc940a4d572583c449613b6b1ae3c","2f41868de465b86d2fc357f7ed58ff323d58030f","lib/matplotlib/axes/_base.py","lib/matplotlib/axes/_base.py","diff --git a/lib/matplotlib/axes/_base.py b/lib/matplotlib/axes/_base.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_base.py b/lib/matplotlib/axes/_base.py
index aec405415..30376c242 100644
--- a/lib/matplotlib/axes/_base.py
+++ b/lib/matplotlib/axes/_base.py
@@ -3332,6 +3332,9 @@ class _AxesBase(martist.Artist):
         left, right = sorted([left, right], reverse=bool(reverse))
 
         self._viewLim.intervalx = (left, right)
+        # Mark viewlims as no longer stale without triggering an autoscale.
+        for ax in self._shared_x_axes.get_siblings(self):
+            ax._stale_viewlim_x = False
         if auto is not None:
             self._autoscaleXon = bool(auto)
 
@@ -3601,6 +3604,9 @@ class _AxesBase(martist.Artist):
         bottom, top = sorted([bottom, top], reverse=bool(reverse))
 
         self._viewLim.intervaly = (bottom, top)
+        # Mark viewlims as no longer stale without triggering an autoscale.
+        for ax in self._shared_y_axes.get_siblings(self):
+            ax._stale_viewlim_y = False
         if auto is not None:
             self._autoscaleYon = bool(auto)
 
"
"matplotlib","23","bb6a4af984778dbac55c8391ae29fa2b5b201361","418a1adf597b4d7759dcd64b864bd64cc1b507f4","lib/matplotlib/axes/_base.py","lib/matplotlib/axes/_base.py","diff --git a/lib/matplotlib/axes/_base.py b/lib/matplotlib/axes/_base.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_base.py b/lib/matplotlib/axes/_base.py
index 2e519a74c..7ecddbc02 100644
--- a/lib/matplotlib/axes/_base.py
+++ b/lib/matplotlib/axes/_base.py
@@ -1507,8 +1507,8 @@ class _AxesBase(martist.Artist):
             return
 
         dL = self.dataLim
-        x0, x1 = map(x_trf.inverted().transform, dL.intervalx)
-        y0, y1 = map(y_trf.inverted().transform, dL.intervaly)
+        x0, x1 = map(x_trf.transform, dL.intervalx)
+        y0, y1 = map(y_trf.transform, dL.intervaly)
         xr = 1.05 * (x1 - x0)
         yr = 1.05 * (y1 - y0)
 
"
"matplotlib","1","c404d1f716e8aaefd4d7371ff49673e9c1f7f07c","5324adaec6a7fd3d78dea7b28451d5f6e95392a6","lib/matplotlib/backend_bases.py;lib/matplotlib/figure.py;lib/matplotlib/tight_layout.py","lib/matplotlib/backend_bases.py;lib/matplotlib/figure.py;lib/matplotlib/tight_layout.py","diff --git a/lib/matplotlib/backend_bases.py b/lib/matplotlib/backend_bases.py;diff --git a/lib/matplotlib/figure.py b/lib/matplotlib/figure.py;diff --git a/lib/matplotlib/tight_layout.py b/lib/matplotlib/tight_layout.py","lib/matplotlib/tests/test_bbox_tight.py","","diff --git a/lib/matplotlib/backend_bases.py b/lib/matplotlib/backend_bases.py
index 71eb153f2..8009207dd 100644
--- a/lib/matplotlib/backend_bases.py
+++ b/lib/matplotlib/backend_bases.py
@@ -46,6 +46,7 @@ from matplotlib._pylab_helpers import Gcf
 from matplotlib.backend_managers import ToolManager
 from matplotlib.transforms import Affine2D
 from matplotlib.path import Path
+from matplotlib.cbook import _setattr_cm
 
 
 _log = logging.getLogger(__name__)
@@ -1502,15 +1503,14 @@ class KeyEvent(LocationEvent):
         self.key = key
 
 
-def _get_renderer(figure, print_method=None, *, draw_disabled=False):
+def _get_renderer(figure, print_method=None):
     """"""
     Get the renderer that would be used to save a `~.Figure`, and cache it on
     the figure.
 
-    If *draw_disabled* is True, additionally replace drawing methods on
-    *renderer* by no-ops.  This is used by the tight-bbox-saving renderer,
-    which needs to walk through the artist tree to compute the tight-bbox, but
-    for which the output file may be closed early.
+    If you need a renderer without any active draw methods use
+    cbook._setattr_cm to temporary patch them out at your call site.
+
     """"""
     # This is implemented by triggering a draw, then immediately jumping out of
     # Figure.draw() by raising an exception.
@@ -1529,12 +1529,6 @@ def _get_renderer(figure, print_method=None, *, draw_disabled=False):
         except Done as exc:
             renderer, = figure._cachedRenderer, = exc.args
 
-    if draw_disabled:
-        for meth_name in dir(RendererBase):
-            if (meth_name.startswith(""draw_"")
-                    or meth_name in [""open_group"", ""close_group""]):
-                setattr(renderer, meth_name, lambda *args, **kwargs: None)
-
     return renderer
 
 
@@ -2093,9 +2087,18 @@ class FigureCanvasBase:
                     renderer = _get_renderer(
                         self.figure,
                         functools.partial(
-                            print_method, orientation=orientation),
-                        draw_disabled=True)
-                    self.figure.draw(renderer)
+                            print_method, orientation=orientation)
+                    )
+                    no_ops = {
+                        meth_name: lambda *args, **kwargs: None
+                        for meth_name in dir(RendererBase)
+                        if (meth_name.startswith(""draw_"")
+                            or meth_name in [""open_group"", ""close_group""])
+                    }
+
+                    with _setattr_cm(renderer, **no_ops):
+                        self.figure.draw(renderer)
+
                     bbox_inches = self.figure.get_tightbbox(
                         renderer, bbox_extra_artists=bbox_extra_artists)
                     if pad_inches is None:
diff --git a/lib/matplotlib/figure.py b/lib/matplotlib/figure.py
index 9e7fc4c2d..83577ac92 100644
--- a/lib/matplotlib/figure.py
+++ b/lib/matplotlib/figure.py
@@ -2392,6 +2392,8 @@ default: 'top'
 
         from .tight_layout import (
             get_renderer, get_subplotspec_list, get_tight_layout_figure)
+        from .cbook import _setattr_cm
+        from .backend_bases import RendererBase
 
         subplotspec_list = get_subplotspec_list(self.axes)
         if None in subplotspec_list:
@@ -2402,9 +2404,17 @@ default: 'top'
         if renderer is None:
             renderer = get_renderer(self)
 
-        kwargs = get_tight_layout_figure(
-            self, self.axes, subplotspec_list, renderer,
-            pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)
+        no_ops = {
+            meth_name: lambda *args, **kwargs: None
+            for meth_name in dir(RendererBase)
+            if (meth_name.startswith(""draw_"")
+                or meth_name in [""open_group"", ""close_group""])
+        }
+
+        with _setattr_cm(renderer, **no_ops):
+            kwargs = get_tight_layout_figure(
+                self, self.axes, subplotspec_list, renderer,
+                pad=pad, h_pad=h_pad, w_pad=w_pad, rect=rect)
         if kwargs:
             self.subplots_adjust(**kwargs)
 
diff --git a/lib/matplotlib/tight_layout.py b/lib/matplotlib/tight_layout.py
index 43b578fef..df5500504 100644
--- a/lib/matplotlib/tight_layout.py
+++ b/lib/matplotlib/tight_layout.py
@@ -173,7 +173,7 @@ def get_renderer(fig):
             return canvas.get_renderer()
         else:
             from . import backend_bases
-            return backend_bases._get_renderer(fig, draw_disabled=True)
+            return backend_bases._get_renderer(fig)
 
 
 def get_subplotspec_list(axes_list, grid_spec=None):
"
"matplotlib","7","969513e2a5227331a2eb9e4bc4ba8448a0f9831d","ac400b51bb31b91920ee9aae02a0606a67983a8f","lib/matplotlib/colors.py","lib/matplotlib/colors.py","diff --git a/lib/matplotlib/colors.py b/lib/matplotlib/colors.py","lib/matplotlib/tests/test_colors.py","","diff --git a/lib/matplotlib/colors.py b/lib/matplotlib/colors.py
index ee1e5f880..ffc259ac3 100644
--- a/lib/matplotlib/colors.py
+++ b/lib/matplotlib/colors.py
@@ -1936,7 +1936,7 @@ class LightSource:
                                  .format(lookup.keys)) from err
 
         # Only apply result where hillshade intensity isn't masked
-        if hasattr(intensity, 'mask'):
+        if np.ma.is_masked(intensity):
             mask = intensity.mask[..., 0]
             for i in range(3):
                 blend[..., i][mask] = rgb[..., i][mask]
"
"matplotlib","25","9a5473dbac05f3d6773b42c8e16d58a7dc3159b8","184225bc5639fbd2f29c1253602806a8b6462d9f","lib/matplotlib/collections.py","lib/matplotlib/collections.py","diff --git a/lib/matplotlib/collections.py b/lib/matplotlib/collections.py","lib/matplotlib/tests/test_collections.py","","diff --git a/lib/matplotlib/collections.py b/lib/matplotlib/collections.py
index b7a7219ca..74c8b936d 100644
--- a/lib/matplotlib/collections.py
+++ b/lib/matplotlib/collections.py
@@ -1470,12 +1470,15 @@ class EventCollection(LineCollection):
 
         .. plot:: gallery/lines_bars_and_markers/eventcollection_demo.py
         """"""
-
+        if positions is None:
+            raise ValueError('positions must be an array-like object')
+        # Force a copy of positions
+        positions = np.array(positions, copy=True)
         segment = (lineoffset + linelength / 2.,
                    lineoffset - linelength / 2.)
-        if positions is None or len(positions) == 0:
+        if positions.size == 0:
             segments = []
-        elif hasattr(positions, 'ndim') and positions.ndim > 1:
+        elif positions.ndim > 1:
             raise ValueError('positions cannot be an array with more than '
                              'one dimension.')
         elif (orientation is None or orientation.lower() == 'none' or
"
"matplotlib","10","b31d64ce3910e8d297d8300690e459587f77181f","1986da3968ee76c6bce8f4c04aed80e23bd4ecfa","lib/matplotlib/axis.py","lib/matplotlib/axis.py","diff --git a/lib/matplotlib/axis.py b/lib/matplotlib/axis.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axis.py b/lib/matplotlib/axis.py
index b252f8d3a..cfc7739d8 100644
--- a/lib/matplotlib/axis.py
+++ b/lib/matplotlib/axis.py
@@ -824,7 +824,11 @@ class Axis(martist.Artist):
                 self._minor_tick_kw.update(kwtrans)
                 for tick in self.minorTicks:
                     tick._apply_params(**kwtrans)
-            # special-case label color to also apply to the offset text
+            # labelOn and labelcolor also apply to the offset text.
+            if 'label1On' in kwtrans or 'label2On' in kwtrans:
+                self.offsetText.set_visible(
+                    self._major_tick_kw.get('label1On', False)
+                    or self._major_tick_kw.get('label2On', False))
             if 'labelcolor' in kwtrans:
                 self.offsetText.set_color(kwtrans['labelcolor'])
 
"
"matplotlib","13","2c845db9d961ddea6e072c3e6dbb0bec823ac25e","eb4b15b47a4d019f0e9edb2fb0587aebc2dbd8e8","lib/matplotlib/path.py","lib/matplotlib/path.py","diff --git a/lib/matplotlib/path.py b/lib/matplotlib/path.py","lib/matplotlib/tests/test_path.py","","diff --git a/lib/matplotlib/path.py b/lib/matplotlib/path.py
index 93165dc68..f56746b88 100644
--- a/lib/matplotlib/path.py
+++ b/lib/matplotlib/path.py
@@ -330,6 +330,7 @@ class Path:
         if not args:
             return Path(np.empty([0, 2], dtype=np.float32))
 
+        # concatenate paths
         vertices = np.concatenate([x.vertices for x in args])
         codes = np.empty(len(vertices), dtype=cls.code_type)
         i = 0
@@ -341,6 +342,16 @@ class Path:
                 codes[i:i + len(path.codes)] = path.codes
             i += len(path.vertices)
 
+        # remove internal STOP's, replace kinal stop if present
+        last_vert = None
+        if codes.size > 0 and codes[-1] == cls.STOP:
+            last_vert = vertices[-1]
+        vertices = vertices[codes != cls.STOP, :]
+        codes = codes[codes != cls.STOP]
+        if last_vert is not None:
+            vertices = np.append(vertices, [last_vert], axis=0)
+            codes = np.append(codes, cls.STOP)
+
         return cls(vertices, codes)
 
     def __repr__(self):
"
"matplotlib","19","47cfa35fc01af26498f493affcd8cb42b8fb2fc8","670d5614e884be8013cf5dbb2160b4d33314d771","lib/matplotlib/projections/polar.py","lib/matplotlib/projections/polar.py","diff --git a/lib/matplotlib/projections/polar.py b/lib/matplotlib/projections/polar.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/projections/polar.py b/lib/matplotlib/projections/polar.py
index 2aa1bd35e..9da973c97 100644
--- a/lib/matplotlib/projections/polar.py
+++ b/lib/matplotlib/projections/polar.py
@@ -385,6 +385,7 @@ class RadialLocator(mticker.Locator):
     :class:`~matplotlib.ticker.Locator` (which may be different
     depending on the scale of the *r*-axis.
     """"""
+
     def __init__(self, base, axes=None):
         self.base = base
         self._axes = axes
@@ -416,6 +417,11 @@ class RadialLocator(mticker.Locator):
         # docstring inherited
         return self.base.refresh()
 
+    def nonsingular(self, vmin, vmax):
+        # docstring inherited
+        return ((0, 1) if (vmin, vmax) == (-np.inf, np.inf)  # Init. limits.
+                else self.base.nonsingular(vmin, vmax))
+
     def view_limits(self, vmin, vmax):
         vmin, vmax = self.base.view_limits(vmin, vmax)
         if vmax > vmin:
"
"matplotlib","9","8673167d4c44dc2ede02fedd718dadf7f88e102d","c01f9d3eff9b6239446c1bb2d205eccd69054aeb","lib/matplotlib/projections/polar.py","lib/matplotlib/projections/polar.py","diff --git a/lib/matplotlib/projections/polar.py b/lib/matplotlib/projections/polar.py","lib/matplotlib/tests/test_polar.py","","diff --git a/lib/matplotlib/projections/polar.py b/lib/matplotlib/projections/polar.py
index 902946c6b..0649a03c8 100644
--- a/lib/matplotlib/projections/polar.py
+++ b/lib/matplotlib/projections/polar.py
@@ -949,6 +949,7 @@ class PolarAxes(Axes):
     @cbook._delete_parameter(""3.3"", ""args"")
     @cbook._delete_parameter(""3.3"", ""kwargs"")
     def draw(self, renderer, *args, **kwargs):
+        self._unstale_viewLim()
         thetamin, thetamax = np.rad2deg(self._realViewLim.intervalx)
         if thetamin > thetamax:
             thetamin, thetamax = thetamax, thetamin
"
"matplotlib","21","e240493a899ac05cb992cdb88f5487386586090e","6fceb054369445d0b20d2864957e8bcfd8d2cb87","lib/matplotlib/axes/_axes.py","lib/matplotlib/axes/_axes.py","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py
index 152d00b0e..212c405b5 100644
--- a/lib/matplotlib/axes/_axes.py
+++ b/lib/matplotlib/axes/_axes.py
@@ -3910,10 +3910,13 @@ class Axes(_AxesBase):
 
         zdelta = 0.1
 
-        def line_props_with_rcdefaults(subkey, explicit, zdelta=0):
+        def line_props_with_rcdefaults(subkey, explicit, zdelta=0,
+                                       use_marker=True):
             d = {k.split('.')[-1]: v for k, v in rcParams.items()
                  if k.startswith(f'boxplot.{subkey}')}
             d['zorder'] = zorder + zdelta
+            if not use_marker:
+                d['marker'] = ''
             if explicit is not None:
                 d.update(
                     cbook.normalize_kwargs(explicit, mlines.Line2D._alias_map))
@@ -3934,15 +3937,16 @@ class Axes(_AxesBase):
                     cbook.normalize_kwargs(
                         boxprops, mpatches.PathPatch._alias_map))
         else:
-            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops)
+            final_boxprops = line_props_with_rcdefaults('boxprops', boxprops,
+                                                        use_marker=False)
         final_whiskerprops = line_props_with_rcdefaults(
-            'whiskerprops', whiskerprops)
+            'whiskerprops', whiskerprops, use_marker=False)
         final_capprops = line_props_with_rcdefaults(
-            'capprops', capprops)
+            'capprops', capprops, use_marker=False)
         final_flierprops = line_props_with_rcdefaults(
             'flierprops', flierprops)
         final_medianprops = line_props_with_rcdefaults(
-            'medianprops', medianprops, zdelta)
+            'medianprops', medianprops, zdelta, use_marker=False)
         final_meanprops = line_props_with_rcdefaults(
             'meanprops', meanprops, zdelta)
         removed_prop = 'marker' if meanline else 'linestyle'
"
"matplotlib","30","3b26ee6f6b31bc0cca4be0407dbb40f44756030d","d4de838fe7b38abb02f061540fd93962cc063fc4","lib/matplotlib/colors.py","lib/matplotlib/colors.py","diff --git a/lib/matplotlib/colors.py b/lib/matplotlib/colors.py","lib/matplotlib/tests/test_colors.py","","diff --git a/lib/matplotlib/colors.py b/lib/matplotlib/colors.py
index 2bfe2b07d..e72cc5709 100644
--- a/lib/matplotlib/colors.py
+++ b/lib/matplotlib/colors.py
@@ -377,16 +377,20 @@ def makeMappingArray(N, data, gamma=1.0):
     if (np.diff(x) < 0).any():
         raise ValueError(""data mapping points must have x in increasing order"")
     # begin generation of lookup table
-    x = x * (N - 1)
-    xind = (N - 1) * np.linspace(0, 1, N) ** gamma
-    ind = np.searchsorted(x, xind)[1:-1]
-
-    distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])
-    lut = np.concatenate([
-        [y1[0]],
-        distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],
-        [y0[-1]],
-    ])
+    if N == 1:
+        # convention: use the y = f(x=1) value for a 1-element lookup table
+        lut = np.array(y0[-1])
+    else:
+        x = x * (N - 1)
+        xind = (N - 1) * np.linspace(0, 1, N) ** gamma
+        ind = np.searchsorted(x, xind)[1:-1]
+
+        distance = (xind[1:-1] - x[ind - 1]) / (x[ind] - x[ind - 1])
+        lut = np.concatenate([
+            [y1[0]],
+            distance * (y0[ind] - y1[ind - 1]) + y1[ind - 1],
+            [y0[-1]],
+        ])
     # ensure that the lut is confined to values between 0 and 1 by clipping it
     return np.clip(lut, 0.0, 1.0)
 
"
"matplotlib","15","6a8e39f4eba28e99d7b1dd454b001b987dd0bbca","c7df5d2770030fe4588a0fc1ab4449a689554dfc","examples/userdemo/colormap_normalizations.py;examples/userdemo/colormap_normalizations_symlognorm.py;lib/matplotlib/colors.py;tutorials/colors/colormapnorms.py","examples/userdemo/colormap_normalizations.py;examples/userdemo/colormap_normalizations_symlognorm.py;lib/matplotlib/colors.py;tutorials/colors/colormapnorms.py","diff --git a/examples/userdemo/colormap_normalizations.py b/examples/userdemo/colormap_normalizations.py;diff --git a/examples/userdemo/colormap_normalizations_symlognorm.py b/examples/userdemo/colormap_normalizations_symlognorm.py;diff --git a/lib/matplotlib/colors.py b/lib/matplotlib/colors.py;diff --git a/tutorials/colors/colormapnorms.py b/tutorials/colors/colormapnorms.py","lib/matplotlib/tests/test_colors.py","","diff --git a/examples/userdemo/colormap_normalizations.py b/examples/userdemo/colormap_normalizations.py
index 419a4b505..2844fd9f9 100644
--- a/examples/userdemo/colormap_normalizations.py
+++ b/examples/userdemo/colormap_normalizations.py
@@ -69,7 +69,7 @@ fig, ax = plt.subplots(2, 1)
 
 pcm = ax[0].pcolormesh(X, Y, Z1,
                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,
-                                              vmin=-1.0, vmax=1.0),
+                                              vmin=-1.0, vmax=1.0, base=10),
                        cmap='RdBu_r')
 fig.colorbar(pcm, ax=ax[0], extend='both')
 
diff --git a/examples/userdemo/colormap_normalizations_symlognorm.py b/examples/userdemo/colormap_normalizations_symlognorm.py
index 780381e43..b0fbf0dc3 100644
--- a/examples/userdemo/colormap_normalizations_symlognorm.py
+++ b/examples/userdemo/colormap_normalizations_symlognorm.py
@@ -29,7 +29,7 @@ fig, ax = plt.subplots(2, 1)
 
 pcm = ax[0].pcolormesh(X, Y, Z,
                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,
-                                              vmin=-1.0, vmax=1.0),
+                                              vmin=-1.0, vmax=1.0, base=10),
                        cmap='RdBu_r')
 fig.colorbar(pcm, ax=ax[0], extend='both')
 
diff --git a/lib/matplotlib/colors.py b/lib/matplotlib/colors.py
index 52d2b2438..7f3ed2afb 100644
--- a/lib/matplotlib/colors.py
+++ b/lib/matplotlib/colors.py
@@ -1202,8 +1202,8 @@ class SymLogNorm(Normalize):
     *linthresh* allows the user to specify the size of this range
     (-*linthresh*, *linthresh*).
     """"""
-    def __init__(self, linthresh, linscale=1.0,
-                 vmin=None, vmax=None, clip=False):
+    def __init__(self, linthresh, linscale=1.0, vmin=None, vmax=None,
+                 clip=False, base=None):
         """"""
         Parameters
         ----------
@@ -1213,14 +1213,29 @@ class SymLogNorm(Normalize):
         linscale : float, default: 1
             This allows the linear range (-*linthresh* to *linthresh*) to be
             stretched relative to the logarithmic range. Its value is the
-            number of decades to use for each half of the linear range. For
-            example, when *linscale* == 1.0 (the default), the space used for
-            the positive and negative halves of the linear range will be equal
-            to one decade in the logarithmic range.
+            number of powers of *base* (decades for base 10) to use for each
+            half of the linear range. For example, when *linscale* == 1.0
+            (the default), the space used for the positive and negative halves
+            of the linear range will be equal to a decade in the logarithmic
+            range if ``base=10``.
+        base : float, default: None
+            For v3.2 the default is the old value of ``np.e``, but that is
+            deprecated for v3.3 when base will default to 10.  During the
+            transition, specify the *base* kwarg to avoid a deprecation
+            warning.
         """"""
         Normalize.__init__(self, vmin, vmax, clip)
+        if base is None:
+            self._base = np.e
+            cbook.warn_deprecated(""3.3"", message=""default base will change ""
+                ""from np.e to 10.  To suppress this warning specify the base ""
+                ""kwarg."")
+        else:
+            self._base = base
+        self._log_base = np.log(self._base)
+
         self.linthresh = float(linthresh)
-        self._linscale_adj = (linscale / (1.0 - np.e ** -1))
+        self._linscale_adj = (linscale / (1.0 - self._base ** -1))
         if vmin is not None and vmax is not None:
             self._transform_vmin_vmax()
 
@@ -1255,7 +1270,8 @@ class SymLogNorm(Normalize):
         with np.errstate(invalid=""ignore""):
             masked = np.abs(a) > self.linthresh
         sign = np.sign(a[masked])
-        log = (self._linscale_adj + np.log(np.abs(a[masked]) / self.linthresh))
+        log = (self._linscale_adj +
+               np.log(np.abs(a[masked]) / self.linthresh) / self._log_base)
         log *= sign * self.linthresh
         a[masked] = log
         a[~masked] *= self._linscale_adj
@@ -1265,7 +1281,8 @@ class SymLogNorm(Normalize):
         """"""Inverse inplace Transformation.""""""
         masked = np.abs(a) > (self.linthresh * self._linscale_adj)
         sign = np.sign(a[masked])
-        exp = np.exp(sign * a[masked] / self.linthresh - self._linscale_adj)
+        exp = np.power(self._base,
+                       sign * a[masked] / self.linthresh - self._linscale_adj)
         exp *= sign * self.linthresh
         a[masked] = exp
         a[~masked] /= self._linscale_adj
diff --git a/tutorials/colors/colormapnorms.py b/tutorials/colors/colormapnorms.py
index 412278ac4..ea982b243 100644
--- a/tutorials/colors/colormapnorms.py
+++ b/tutorials/colors/colormapnorms.py
@@ -98,7 +98,7 @@ fig, ax = plt.subplots(2, 1)
 
 pcm = ax[0].pcolormesh(X, Y, Z,
                        norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,
-                                              vmin=-1.0, vmax=1.0),
+                                              vmin=-1.0, vmax=1.0, base=10),
                        cmap='RdBu_r')
 fig.colorbar(pcm, ax=ax[0], extend='both')
 
"
"matplotlib","11","f8459a513c3f67447ceb1a07c29760d504517ff2","af745264376a10782bd0d8b96d255f958c2950f3","lib/matplotlib/text.py","lib/matplotlib/text.py","diff --git a/lib/matplotlib/text.py b/lib/matplotlib/text.py","lib/matplotlib/tests/test_text.py","","diff --git a/lib/matplotlib/text.py b/lib/matplotlib/text.py
index 429d31fb9..77ab7145a 100644
--- a/lib/matplotlib/text.py
+++ b/lib/matplotlib/text.py
@@ -891,12 +891,12 @@ class Text(Artist):
         #return _unit_box
         if not self.get_visible():
             return Bbox.unit()
-        if dpi is not None:
-            dpi_orig = self.figure.dpi
-            self.figure.dpi = dpi
+        if dpi is None:
+            dpi = self.figure.dpi
         if self.get_text() == '':
-            tx, ty = self._get_xy_display()
-            return Bbox.from_bounds(tx, ty, 0, 0)
+            with cbook._setattr_cm(self.figure, dpi=dpi):
+                tx, ty = self._get_xy_display()
+                return Bbox.from_bounds(tx, ty, 0, 0)
 
         if renderer is not None:
             self._renderer = renderer
@@ -905,13 +905,12 @@ class Text(Artist):
         if self._renderer is None:
             raise RuntimeError('Cannot get window extent w/o renderer')
 
-        bbox, info, descent = self._get_layout(self._renderer)
-        x, y = self.get_unitless_position()
-        x, y = self.get_transform().transform((x, y))
-        bbox = bbox.translated(x, y)
-        if dpi is not None:
-            self.figure.dpi = dpi_orig
-        return bbox
+        with cbook._setattr_cm(self.figure, dpi=dpi):
+            bbox, info, descent = self._get_layout(self._renderer)
+            x, y = self.get_unitless_position()
+            x, y = self.get_transform().transform((x, y))
+            bbox = bbox.translated(x, y)
+            return bbox
 
     def set_backgroundcolor(self, color):
         """"""
"
"matplotlib","6","fb9e72309aeefb51d1a56ecbdb1e7399d105ff06","d84f4a7ac8d43d1288cb8ce11ab60ae557306b7e","lib/matplotlib/axes/_axes.py","lib/matplotlib/axes/_axes.py","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py
index 1eb12b4ca..7008e4c9e 100644
--- a/lib/matplotlib/axes/_axes.py
+++ b/lib/matplotlib/axes/_axes.py
@@ -4281,9 +4281,15 @@ class Axes(_AxesBase):
             except ValueError:
                 pass  # Failed to convert to float array; must be color specs.
             else:
+                # handle the documented special case of a 2D array with 1
+                # row which as RGB(A) to broadcast.
+                if c.shape == (1, 4) or c.shape == (1, 3):
+                    c_is_mapped = False
+                    if c.size != xsize:
+                        valid_shape = False
                 # If c can be either mapped values or a RGB(A) color, prefer
                 # the former if shapes match, the latter otherwise.
-                if c.size == xsize:
+                elif c.size == xsize:
                     c = c.ravel()
                     c_is_mapped = True
                 else:  # Wrong size; it must not be intended for mapping.
"
"matplotlib","17","58c66982d98851c56b045137ced803fd62c6c5e8","05a5db0fec2eced55076736f0b9520641b279ad6","lib/matplotlib/transforms.py","lib/matplotlib/transforms.py","diff --git a/lib/matplotlib/transforms.py b/lib/matplotlib/transforms.py","lib/matplotlib/tests/test_colorbar.py","","diff --git a/lib/matplotlib/transforms.py b/lib/matplotlib/transforms.py
index ce517c1d0..3292340bf 100644
--- a/lib/matplotlib/transforms.py
+++ b/lib/matplotlib/transforms.py
@@ -2791,6 +2791,10 @@ def nonsingular(vmin, vmax, expander=0.001, tiny=1e-15, increasing=True):
         vmin, vmax = vmax, vmin
         swapped = True
 
+    # Expand vmin, vmax to float: if they were integer types, they can wrap
+    # around in abs (abs(np.int8(-128)) == -128) and vmax - vmin can overflow.
+    vmin, vmax = map(float, [vmin, vmax])
+
     maxabsvalue = max(abs(vmin), abs(vmax))
     if maxabsvalue < (1e6 / tiny) * np.finfo(float).tiny:
         vmin = -expander
"
"matplotlib","28","896fb8140a600341ebb5eaa4191584f23df2d2a0","94ac78a47cfaed9a09e5fd8295b88e8248b67f55","lib/matplotlib/axes/_base.py","lib/matplotlib/axes/_base.py","diff --git a/lib/matplotlib/axes/_base.py b/lib/matplotlib/axes/_base.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_base.py b/lib/matplotlib/axes/_base.py
index 28c4c9caa..6c3d95fe1 100644
--- a/lib/matplotlib/axes/_base.py
+++ b/lib/matplotlib/axes/_base.py
@@ -3249,7 +3249,10 @@ class _AxesBase(martist.Artist):
             if right is None:
                 right = old_right
 
-        if self.get_xscale() == 'log':
+        if self.get_xscale() == 'log' and (left <= 0 or right <= 0):
+            # Axes init calls set_xlim(0, 1) before get_xlim() can be called,
+            # so only grab the limits if we really need them.
+            old_left, old_right = self.get_xlim()
             if left <= 0:
                 cbook._warn_external(
                     'Attempted to set non-positive left xlim on a '
"
"matplotlib","20","805f451b7122d1ef595c7a01592bb2f500aed41f","5375487ab28c877c8008c5a178e0b81a6e267957","lib/matplotlib/_constrained_layout.py;lib/matplotlib/backend_bases.py","lib/matplotlib/_constrained_layout.py;lib/matplotlib/backend_bases.py","diff --git a/lib/matplotlib/_constrained_layout.py b/lib/matplotlib/_constrained_layout.py;diff --git a/lib/matplotlib/backend_bases.py b/lib/matplotlib/backend_bases.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/_constrained_layout.py b/lib/matplotlib/_constrained_layout.py
index c6e0e7dc0..d7ad26c48 100644
--- a/lib/matplotlib/_constrained_layout.py
+++ b/lib/matplotlib/_constrained_layout.py
@@ -254,10 +254,7 @@ def _make_ghost_gridspec_slots(fig, gs):
             # this gridspec slot doesn't have an axis so we
             # make a ""ghost"".
             ax = fig.add_subplot(gs[nn])
-            ax.set_frame_on(False)
-            ax.set_xticks([])
-            ax.set_yticks([])
-            ax.set_facecolor((1, 0, 0, 0))
+            ax.set_visible(False)
 
 
 def _make_layout_margins(ax, renderer, h_pad, w_pad):
diff --git a/lib/matplotlib/backend_bases.py b/lib/matplotlib/backend_bases.py
index 16f084ddb..45460bfa5 100644
--- a/lib/matplotlib/backend_bases.py
+++ b/lib/matplotlib/backend_bases.py
@@ -1853,7 +1853,7 @@ class FigureCanvasBase:
 
     def inaxes(self, xy):
         """"""
-        Check if a point is in an axes.
+        Return the topmost visible `~.axes.Axes` containing the point *xy*.
 
         Parameters
         ----------
@@ -1864,12 +1864,11 @@ class FigureCanvasBase:
 
         Returns
         -------
-        axes: topmost axes containing the point, or None if no axes.
-
+        axes : `~matplotlib.axes.Axes` or None
+            The topmost visible axes containing the point, or None if no axes.
         """"""
         axes_list = [a for a in self.figure.get_axes()
-                     if a.patch.contains_point(xy)]
-
+                     if a.patch.contains_point(xy) and a.get_visible()]
         if axes_list:
             axes = cbook._topmost_artist(axes_list)
         else:
"
"matplotlib","27","11269123d516cda369764a081ddfb8c1a10ddc53","02f25d60139b160fd9b321802b4a2f6c6f3f8672","lib/matplotlib/colorbar.py","lib/matplotlib/colorbar.py","diff --git a/lib/matplotlib/colorbar.py b/lib/matplotlib/colorbar.py","lib/matplotlib/tests/test_colorbar.py","","diff --git a/lib/matplotlib/colorbar.py b/lib/matplotlib/colorbar.py
index 22fdb6801..0100218ac 100644
--- a/lib/matplotlib/colorbar.py
+++ b/lib/matplotlib/colorbar.py
@@ -724,7 +724,7 @@ class ColorbarBase(_ColorbarMappableDummy):
 
     def set_label(self, label, **kw):
         """"""Label the long axis of the colorbar.""""""
-        self._label = str(label)
+        self._label = label
         self._labelkw = kw
         self._set_label()
 
"
"matplotlib","12","e92685a26442bfced06067934f34c104487583a8","382be60aec3e6ebbf92f3d5792ba059bf3cfe6cf","lib/matplotlib/axes/_axes.py","lib/matplotlib/axes/_axes.py","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py
index f6fe59e4c..de807b686 100644
--- a/lib/matplotlib/axes/_axes.py
+++ b/lib/matplotlib/axes/_axes.py
@@ -1123,15 +1123,19 @@ class Axes(_AxesBase):
         if not np.iterable(xmax):
             xmax = [xmax]
 
-        y, xmin, xmax = cbook.delete_masked_points(y, xmin, xmax)
-
+        # Create and combine masked_arrays from input
+        y, xmin, xmax = cbook._combine_masks(y, xmin, xmax)
         y = np.ravel(y)
-        xmin = np.resize(xmin, y.shape)
-        xmax = np.resize(xmax, y.shape)
+        xmin = np.ravel(xmin)
+        xmax = np.ravel(xmax)
+
+        masked_verts = np.ma.empty((len(y), 2, 2))
+        masked_verts[:, 0, 0] = xmin
+        masked_verts[:, 0, 1] = y
+        masked_verts[:, 1, 0] = xmax
+        masked_verts[:, 1, 1] = y
 
-        verts = [((thisxmin, thisy), (thisxmax, thisy))
-                 for thisxmin, thisxmax, thisy in zip(xmin, xmax, y)]
-        lines = mcoll.LineCollection(verts, colors=colors,
+        lines = mcoll.LineCollection(masked_verts, colors=colors,
                                      linestyles=linestyles, label=label)
         self.add_collection(lines, autolim=False)
         lines.update(kwargs)
@@ -1201,15 +1205,19 @@ class Axes(_AxesBase):
         if not np.iterable(ymax):
             ymax = [ymax]
 
-        x, ymin, ymax = cbook.delete_masked_points(x, ymin, ymax)
-
+        # Create and combine masked_arrays from input
+        x, ymin, ymax = cbook._combine_masks(x, ymin, ymax)
         x = np.ravel(x)
-        ymin = np.resize(ymin, x.shape)
-        ymax = np.resize(ymax, x.shape)
+        ymin = np.ravel(ymin)
+        ymax = np.ravel(ymax)
+
+        masked_verts = np.ma.empty((len(x), 2, 2))
+        masked_verts[:, 0, 0] = x
+        masked_verts[:, 0, 1] = ymin
+        masked_verts[:, 1, 0] = x
+        masked_verts[:, 1, 1] = ymax
 
-        verts = [((thisx, thisymin), (thisx, thisymax))
-                 for thisx, thisymin, thisymax in zip(x, ymin, ymax)]
-        lines = mcoll.LineCollection(verts, colors=colors,
+        lines = mcoll.LineCollection(masked_verts, colors=colors,
                                      linestyles=linestyles, label=label)
         self.add_collection(lines, autolim=False)
         lines.update(kwargs)
"
"matplotlib","29","cdf9e30e4f3fb7747b178ee9c3849dfdebee7bd0","fc51b411ba5d0984544ecff97e0a28ea4b6a6d03","lib/matplotlib/axis.py","lib/matplotlib/axis.py","diff --git a/lib/matplotlib/axis.py b/lib/matplotlib/axis.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axis.py b/lib/matplotlib/axis.py
index 5cbba5b88..261b8e920 100644
--- a/lib/matplotlib/axis.py
+++ b/lib/matplotlib/axis.py
@@ -1004,11 +1004,10 @@ class Axis(martist.Artist):
         the top for the y-axis; the ""inverse"" direction is increasing to the
         left for the x-axis and to the bottom for the y-axis.
         """"""
-        a, b = self.get_view_interval()
-        if inverted:
-            self.set_view_interval(max(a, b), min(a, b), ignore=True)
-        else:
-            self.set_view_interval(min(a, b), max(a, b), ignore=True)
+        # Currently, must be implemented in subclasses using set_xlim/set_ylim
+        # rather than generically using set_view_interval, so that shared
+        # axes get updated as well.
+        raise NotImplementedError('Derived must override')
 
     def set_default_intervals(self):
         """"""
@@ -2156,6 +2155,11 @@ class XAxis(Axis):
     def get_minpos(self):
         return self.axes.dataLim.minposx
 
+    def set_inverted(self, inverted):
+        # docstring inherited
+        a, b = self.get_view_interval()
+        self.axes.set_xlim(sorted((a, b), reverse=inverted), auto=None)
+
     def set_default_intervals(self):
         # docstring inherited
         xmin, xmax = 0., 1.
@@ -2458,6 +2462,11 @@ class YAxis(Axis):
     def get_minpos(self):
         return self.axes.dataLim.minposy
 
+    def set_inverted(self, inverted):
+        # docstring inherited
+        a, b = self.get_view_interval()
+        self.axes.set_ylim(sorted((a, b), reverse=inverted), auto=None)
+
     def set_default_intervals(self):
         # docstring inherited
         ymin, ymax = 0., 1.
"
"matplotlib","4","793c6b05381231371267b44b107726f3878e14f2","fafa132484872141431a5be3727eab1d8b3c7b82","lib/matplotlib/axes/_axes.py;lib/matplotlib/pyplot.py","lib/matplotlib/axes/_axes.py;lib/matplotlib/pyplot.py","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py;diff --git a/lib/matplotlib/pyplot.py b/lib/matplotlib/pyplot.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py
index 286bf4b87..c43c3e133 100644
--- a/lib/matplotlib/axes/_axes.py
+++ b/lib/matplotlib/axes/_axes.py
@@ -1116,7 +1116,7 @@ class Axes(_AxesBase):
 
     @_preprocess_data(replace_names=[""y"", ""xmin"", ""xmax"", ""colors""],
                       label_namer=""y"")
-    def hlines(self, y, xmin, xmax, colors='k', linestyles='solid',
+    def hlines(self, y, xmin, xmax, colors=None, linestyles='solid',
                label='', **kwargs):
         """"""
         Plot horizontal lines at each *y* from *xmin* to *xmax*.
@@ -1130,7 +1130,7 @@ class Axes(_AxesBase):
             Respective beginning and end of each line. If scalars are
             provided, all lines will have same length.
 
-        colors : list of colors, default: 'k'
+        colors : list of colors, default: :rc:`lines.color`
 
         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
 
@@ -1196,7 +1196,7 @@ class Axes(_AxesBase):
 
     @_preprocess_data(replace_names=[""x"", ""ymin"", ""ymax"", ""colors""],
                       label_namer=""x"")
-    def vlines(self, x, ymin, ymax, colors='k', linestyles='solid',
+    def vlines(self, x, ymin, ymax, colors=None, linestyles='solid',
                label='', **kwargs):
         """"""
         Plot vertical lines.
@@ -1212,7 +1212,7 @@ class Axes(_AxesBase):
             Respective beginning and end of each line. If scalars are
             provided, all lines will have same length.
 
-        colors : list of colors, default: 'k'
+        colors : list of colors, default: :rc:`lines.color`
 
         linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional
 
diff --git a/lib/matplotlib/pyplot.py b/lib/matplotlib/pyplot.py
index 32f63db83..24b26e49c 100644
--- a/lib/matplotlib/pyplot.py
+++ b/lib/matplotlib/pyplot.py
@@ -2601,7 +2601,7 @@ def hist2d(
 # Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
 @_copy_docstring_and_deprecators(Axes.hlines)
 def hlines(
-        y, xmin, xmax, colors='k', linestyles='solid', label='', *,
+        y, xmin, xmax, colors=None, linestyles='solid', label='', *,
         data=None, **kwargs):
     return gca().hlines(
         y, xmin, xmax, colors=colors, linestyles=linestyles,
@@ -2972,7 +2972,7 @@ def violinplot(
 # Autogenerated by boilerplate.py.  Do not edit as changes will be lost.
 @_copy_docstring_and_deprecators(Axes.vlines)
 def vlines(
-        x, ymin, ymax, colors='k', linestyles='solid', label='', *,
+        x, ymin, ymax, colors=None, linestyles='solid', label='', *,
         data=None, **kwargs):
     return gca().vlines(
         x, ymin, ymax, colors=colors, linestyles=linestyles,
"
"matplotlib","26","04d9d28b820af0b4df230a3c67314ed5b0af8fdd","557375ff91f64c1b827f6014da2d369513a69316","lib/matplotlib/axis.py","lib/matplotlib/axis.py","diff --git a/lib/matplotlib/axis.py b/lib/matplotlib/axis.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axis.py b/lib/matplotlib/axis.py
index 7d1bd4271..c827510e3 100644
--- a/lib/matplotlib/axis.py
+++ b/lib/matplotlib/axis.py
@@ -1906,7 +1906,7 @@ def _make_getset_interval(method_name, lim_name, attr_name):
                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),
                        ignore=True)
             else:
-                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),
+                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),
                        ignore=True)
         self.stale = True
 
"
"matplotlib","5","49593b73854e10ace8f3c05343220328def6a328","66289c4f1895b8c65ca92a03d92f6b1cfa552267","lib/matplotlib/axes/_axes.py","lib/matplotlib/axes/_axes.py","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axes/_axes.py b/lib/matplotlib/axes/_axes.py
index cc36b6871..556883ad0 100644
--- a/lib/matplotlib/axes/_axes.py
+++ b/lib/matplotlib/axes/_axes.py
@@ -4492,7 +4492,12 @@ default: :rc:`scatter.edgecolors`
             marker_obj.get_transform())
         if not marker_obj.is_filled():
             edgecolors = 'face'
-            linewidths = rcParams['lines.linewidth']
+            if linewidths is None:
+                linewidths = rcParams['lines.linewidth']
+            elif np.iterable(linewidths):
+                linewidths = [
+                    lw if lw is not None else rcParams['lines.linewidth']
+                    for lw in linewidths]
 
         offsets = np.ma.column_stack([x, y])
 
"
"matplotlib","24","9a5473dbac05f3d6773b42c8e16d58a7dc3159b8","407a9fe71a4c0a8ba4914b8f54f21d32d6dd2d74","lib/matplotlib/axis.py","lib/matplotlib/axis.py","diff --git a/lib/matplotlib/axis.py b/lib/matplotlib/axis.py","lib/matplotlib/tests/test_axes.py","","diff --git a/lib/matplotlib/axis.py b/lib/matplotlib/axis.py
index f645b4b44..a4cdffc55 100644
--- a/lib/matplotlib/axis.py
+++ b/lib/matplotlib/axis.py
@@ -1892,7 +1892,7 @@ def _make_getset_interval(method_name, lim_name, attr_name):
                 setter(self, min(vmin, vmax, oldmin), max(vmin, vmax, oldmax),
                        ignore=True)
             else:
-                setter(self, max(vmin, vmax, oldmax), min(vmin, vmax, oldmin),
+                setter(self, max(vmin, vmax, oldmin), min(vmin, vmax, oldmax),
                        ignore=True)
         self.stale = True
 
"
"thefuck","22","faa7ee603057fa98c25507d30180c055d10d13d4","e2e8b6fc865452b4cfc1bed70e5b9b49807258ae","thefuck/types.py","thefuck/types.py","diff --git a/thefuck/types.py b/thefuck/types.py","tests/test_types.py","","diff --git a/thefuck/types.py b/thefuck/types.py
index 69b174e..cf09137 100644
--- a/thefuck/types.py
+++ b/thefuck/types.py
@@ -81,9 +81,10 @@ class SortedCorrectedCommandsSequence(object):
 
     def _realise(self):
         """"""Realises generator, removes duplicates and sorts commands.""""""
-        commands = self._remove_duplicates(self._commands)
-        self._cached = [self._cached[0]] + sorted(
-            commands, key=lambda corrected_command: corrected_command.priority)
+        if self._cached:
+            commands = self._remove_duplicates(self._commands)
+            self._cached = [self._cached[0]] + sorted(
+                commands, key=lambda corrected_command: corrected_command.priority)
         self._realised = True
         debug('SortedCommandsSequence was realised with: {}, after: {}'.format(
             self._cached, '\n'.join(format_stack())), self._settings)
"
"thefuck","18","b65a9a0a4fd9bef394b45a1d367d29aa1e1c403e","c3b1ba763708b8faaaf55717c436c4cd4c57a7ea","thefuck/rules/sudo.py","thefuck/rules/sudo.py","diff --git a/thefuck/rules/sudo.py b/thefuck/rules/sudo.py","tests/rules/test_sudo.py","","diff --git a/thefuck/rules/sudo.py b/thefuck/rules/sudo.py
index 121efe7..9711444 100644
--- a/thefuck/rules/sudo.py
+++ b/thefuck/rules/sudo.py
@@ -21,6 +21,9 @@ patterns = ['permission denied',
 
 
 def match(command):
+    if command.script_parts and command.script_parts[0] == 'sudo':
+        return False
+
     for pattern in patterns:
         if pattern.lower() in command.stderr.lower()\
                 or pattern.lower() in command.stdout.lower():
"
"thefuck","14","183b70c8b8885843efefd2bd4e74dc0a7d42d173","db6053b301e2b3f4363401e457b5dc4ad2e8429b","thefuck/shells/fish.py","thefuck/shells/fish.py","diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py","tests/shells/test_fish.py","","diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py
index bc2b2ec..b1ef2d7 100644
--- a/thefuck/shells/fish.py
+++ b/thefuck/shells/fish.py
@@ -7,11 +7,10 @@ from .generic import Generic
 
 class Fish(Generic):
     def _get_overridden_aliases(self):
-        overridden_aliases = os.environ.get('TF_OVERRIDDEN_ALIASES', '').strip()
-        if overridden_aliases:
-            return [alias.strip() for alias in overridden_aliases.split(',')]
-        else:
-            return ['cd', 'grep', 'ls', 'man', 'open']
+        default = {'cd', 'grep', 'ls', 'man', 'open'}
+        for alias in os.environ.get('TF_OVERRIDDEN_ALIASES', '').split(','):
+            default.add(alias.strip())
+        return default
 
     def app_alias(self, fuck):
         # It is VERY important to have the variables declared WITHIN the alias
"
"thefuck","3","ac343fb1bd7dadbb9e1a9fd7e3071f5778e338a4","ce5feaebf7fd5c1190b5e14fbc1e962cc8db5f39","thefuck/shells/fish.py","thefuck/shells/fish.py","diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py","tests/shells/test_fish.py","","diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py
index 1435f90..8b4ca57 100644
--- a/thefuck/shells/fish.py
+++ b/thefuck/shells/fish.py
@@ -105,9 +105,9 @@ class Fish(Generic):
 
     def info(self):
         """"""Returns the name and version of the current shell""""""
-        proc = Popen(['fish', '-c', 'echo $FISH_VERSION'],
+        proc = Popen(['fish', '--version'],
                      stdout=PIPE, stderr=DEVNULL)
-        version = proc.stdout.read().decode('utf-8').strip()
+        version = proc.stdout.read().decode('utf-8').split()[-1]
         return u'Fish Shell {}'.format(version)
 
     def put_to_history(self, command):
"
"thefuck","16","d92765d5df6607cb2f2fb67cee7b63f64ac7aa6b","bb5f6bb705a3b217eb682f3357ec6bbb709555c1","thefuck/shells/bash.py;thefuck/shells/fish.py;thefuck/shells/zsh.py;thefuck/types.py","thefuck/shells/bash.py;thefuck/shells/fish.py;thefuck/shells/zsh.py;thefuck/types.py","diff --git a/thefuck/shells/bash.py b/thefuck/shells/bash.py;diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py;diff --git a/thefuck/shells/zsh.py b/thefuck/shells/zsh.py;diff --git a/thefuck/types.py b/thefuck/types.py","tests/shells/test_bash.py;tests/shells/test_zsh.py","","diff --git a/thefuck/shells/bash.py b/thefuck/shells/bash.py
index d6e9b2c..8f4e0e1 100644
--- a/thefuck/shells/bash.py
+++ b/thefuck/shells/bash.py
@@ -6,9 +6,11 @@ from .generic import Generic
 
 class Bash(Generic):
     def app_alias(self, fuck):
-        alias = ""TF_ALIAS={0}"" \
-                "" alias {0}='PYTHONIOENCODING=utf-8"" \
-                "" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && "" \
+        # It is VERY important to have the variables declared WITHIN the alias
+        alias = ""alias {0}='TF_CMD=$(TF_ALIAS={0}"" \
+                "" PYTHONIOENCODING=utf-8"" \
+                "" TF_SHELL_ALIASES=$(alias)"" \
+                "" thefuck $(fc -ln -1)) &&"" \
                 "" eval $TF_CMD"".format(fuck)
 
         if settings.alter_history:
diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py
index fff003b..bc2b2ec 100644
--- a/thefuck/shells/fish.py
+++ b/thefuck/shells/fish.py
@@ -14,6 +14,7 @@ class Fish(Generic):
             return ['cd', 'grep', 'ls', 'man', 'open']
 
     def app_alias(self, fuck):
+        # It is VERY important to have the variables declared WITHIN the alias
         return ('function {0} -d ""Correct your previous console command""\n'
                 '  set -l fucked_up_command $history[1]\n'
                 '  env TF_ALIAS={0} PYTHONIOENCODING=utf-8'
diff --git a/thefuck/shells/zsh.py b/thefuck/shells/zsh.py
index a8c0587..e522d6a 100644
--- a/thefuck/shells/zsh.py
+++ b/thefuck/shells/zsh.py
@@ -7,10 +7,11 @@ from .generic import Generic
 
 class Zsh(Generic):
     def app_alias(self, alias_name):
-        alias = ""alias {0}='TF_ALIAS={0}"" \
+        # It is VERY important to have the variables declared WITHIN the alias
+        alias = ""alias {0}='TF_CMD=$(TF_ALIAS={0}"" \
                 "" PYTHONIOENCODING=utf-8"" \
-                ' TF_SHELL_ALIASES=$(alias)' \
-                "" TF_CMD=$(thefuck $(fc -ln -1 | tail -n 1)) &&"" \
+                "" TF_SHELL_ALIASES=$(alias)"" \
+                "" thefuck $(fc -ln -1 | tail -n 1)) &&"" \
                 "" eval $TF_CMD"".format(alias_name)
 
         if settings.alter_history:
diff --git a/thefuck/types.py b/thefuck/types.py
index dcd99b6..81a7d1b 100644
--- a/thefuck/types.py
+++ b/thefuck/types.py
@@ -282,5 +282,5 @@ class CorrectedCommand(object):
             compatibility_call(self.side_effect, old_cmd, self.script)
         # This depends on correct setting of PYTHONIOENCODING by the alias:
         logs.debug(u'PYTHONIOENCODING: {}'.format(
-            os.environ.get('PYTHONIOENCODING', '>-not-set-<')))
+            os.environ.get('PYTHONIOENCODING', '!!not-set!!')))
         print(self.script)
"
"thefuck","2","40ab4eb62db57627bff10cf029d29c94704086a2","78ef9eec88f43d5727986be2237f6e0e250cbbbc","thefuck/utils.py","thefuck/utils.py","diff --git a/thefuck/utils.py b/thefuck/utils.py","tests/test_utils.py","","diff --git a/thefuck/utils.py b/thefuck/utils.py
index bd8028e..6112c01 100644
--- a/thefuck/utils.py
+++ b/thefuck/utils.py
@@ -118,7 +118,7 @@ def get_all_executables():
     tf_entry_points = ['thefuck', 'fuck']
 
     bins = [exe.name.decode('utf8') if six.PY2 else exe.name
-            for path in os.environ.get('PATH', '').split(':')
+            for path in os.environ.get('PATH', '').split(os.pathsep)
             for exe in _safe(lambda: list(Path(path).iterdir()), [])
             if not _safe(exe.is_dir, True)
             and exe.name not in tf_entry_points]
"
"thefuck","8","449cb9a00693c8b4d97d5fda8d732cf0978e117e","be48f027847161f907def8987706041c65a1fd58","thefuck/rules/dnf_no_such_command.py","thefuck/rules/dnf_no_such_command.py","diff --git a/thefuck/rules/dnf_no_such_command.py b/thefuck/rules/dnf_no_such_command.py","tests/rules/test_dnf_no_such_command.py","","diff --git a/thefuck/rules/dnf_no_such_command.py b/thefuck/rules/dnf_no_such_command.py
index 70c7939..9a70c4f 100644
--- a/thefuck/rules/dnf_no_such_command.py
+++ b/thefuck/rules/dnf_no_such_command.py
@@ -15,10 +15,7 @@ def match(command):
 
 
 def _parse_operations(help_text_lines):
-    # The regex has to be a bytes-style regex since reading from a file
-    # like stdin returns a bytes-style object and a string-style regex
-    # wouldn't work.
-    operation_regex = re.compile(b'^([a-z-]+) +', re.MULTILINE)
+    operation_regex = re.compile(r'^([a-z-]+) +', re.MULTILINE)
     return operation_regex.findall(help_text_lines)
 
 
@@ -26,7 +23,7 @@ def _get_operations():
     proc = subprocess.Popen([""dnf"", '--help'],
                             stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE)
-    lines = proc.stdout.read()
+    lines = proc.stdout.read().decode(""utf-8"")
 
     return _parse_operations(lines)
 
"
"thefuck","23","4129ff2717cc6e6fa51d70cc4e6c31d56ef8e2c9","9a02e821cdc58a4aba2c0acc521fb25cacab87a5","thefuck/utils.py","thefuck/utils.py","diff --git a/thefuck/utils.py b/thefuck/utils.py","tests/test_utils.py","","diff --git a/thefuck/utils.py b/thefuck/utils.py
index 14ca1df..2736e85 100644
--- a/thefuck/utils.py
+++ b/thefuck/utils.py
@@ -2,6 +2,7 @@ from difflib import get_close_matches
 from functools import wraps
 import shelve
 from decorator import decorator
+from contextlib import closing
 import tempfile
 
 import os
@@ -176,11 +177,13 @@ def cache(*depends_on):
             return fn(*args, **kwargs)
 
         cache_path = os.path.join(tempfile.gettempdir(), '.thefuck-cache')
+        # A bit obscure, but simplest way to generate unique key for
+        # functions and methods in python 2 and 3:
         key = '{}.{}'.format(fn.__module__, repr(fn).split('at')[0])
 
         etag = '.'.join(_get_mtime(name) for name in depends_on)
 
-        with shelve.open(cache_path) as db:
+        with closing(shelve.open(cache_path)) as db:
             if db.get(key, {}).get('etag') == etag:
                 return db[key]['value']
             else:
"
"thefuck","1","2ced7a7f33ae0bec3ffc7a43ce95330bdf6cfcb9","444908ce1c17767ef4aaf9e0b4950497914f7f63","thefuck/rules/pip_unknown_command.py","thefuck/rules/pip_unknown_command.py","diff --git a/thefuck/rules/pip_unknown_command.py b/thefuck/rules/pip_unknown_command.py","tests/rules/test_pip_unknown_command.py","","diff --git a/thefuck/rules/pip_unknown_command.py b/thefuck/rules/pip_unknown_command.py
index 75fcc7c..2720cda 100644
--- a/thefuck/rules/pip_unknown_command.py
+++ b/thefuck/rules/pip_unknown_command.py
@@ -12,8 +12,8 @@ def match(command):
 
 
 def get_new_command(command):
-    broken_cmd = re.findall(r'ERROR: unknown command \""([a-z]+)\""',
+    broken_cmd = re.findall(r'ERROR: unknown command ""([^""]+)""',
                             command.output)[0]
-    new_cmd = re.findall(r'maybe you meant \""([a-z]+)\""', command.output)[0]
+    new_cmd = re.findall(r'maybe you meant ""([^""]+)""', command.output)[0]
 
     return replace_argument(command.script, broken_cmd, new_cmd)
"
"thefuck","31","66e2ec7e3f0d3f848c01d87bb3503b0ff90fc78a","1285303363bc420bd7606bd5f808e3f2b4f0e83f","thefuck/rules/git_diff_staged.py","thefuck/rules/git_diff_staged.py","diff --git a/thefuck/rules/git_diff_staged.py b/thefuck/rules/git_diff_staged.py","tests/rules/test_git_diff_staged.py","","diff --git a/thefuck/rules/git_diff_staged.py b/thefuck/rules/git_diff_staged.py
index a35234e..c879cf4 100644
--- a/thefuck/rules/git_diff_staged.py
+++ b/thefuck/rules/git_diff_staged.py
@@ -10,4 +10,4 @@ def match(command, settings):
 
 @utils.git_support
 def get_new_command(command, settings):
-    return '{} --staged'.format(command.script)
+    return command.script.replace(' diff', ' diff --staged')
"
"thefuck","7","64d6835e15a22cf8803d398cbb593f748c550e8a","75d2c43997ca703150cbdb4c46ed7b2e2e71fd11","thefuck/rules/php_s.py","thefuck/rules/php_s.py","diff --git a/thefuck/rules/php_s.py b/thefuck/rules/php_s.py","tests/rules/test_php_s.py","","diff --git a/thefuck/rules/php_s.py b/thefuck/rules/php_s.py
index 10c0996..114fb15 100644
--- a/thefuck/rules/php_s.py
+++ b/thefuck/rules/php_s.py
@@ -3,11 +3,8 @@ from thefuck.utils import replace_argument, for_app
 
 @for_app('php')
 def match(command):
-    return ""php -s"" in command.script
+    return "" -s "" in command.script
 
 
 def get_new_command(command):
     return replace_argument(command.script, ""-s"", ""-S"")
-
-
-requires_output = False
"
"thefuck","25","42a8b4f639269886e468762e6d100b6f01aad8ab","298c04f89c081dc16c8653aa017ca85dd14bfad6","thefuck/rules/mkdir_p.py","thefuck/rules/mkdir_p.py","diff --git a/thefuck/rules/mkdir_p.py b/thefuck/rules/mkdir_p.py","tests/rules/test_mkdir_p.py","","diff --git a/thefuck/rules/mkdir_p.py b/thefuck/rules/mkdir_p.py
index 03b40ce..a62b447 100644
--- a/thefuck/rules/mkdir_p.py
+++ b/thefuck/rules/mkdir_p.py
@@ -10,4 +10,4 @@ def match(command, settings):
 
 @sudo_support
 def get_new_command(command, settings):
-    return re.sub('^mkdir (.*)', 'mkdir -p \\1', command.script)
+    return re.sub('\\bmkdir (.*)', 'mkdir -p \\1', command.script)
"
"thefuck","10","8bd6c5da67e55c64257345efa4e3cc454c42475c","0c84eefa55fc1b4bc4940b41d74568884344e35c","thefuck/rules/man.py","thefuck/rules/man.py","diff --git a/thefuck/rules/man.py b/thefuck/rules/man.py","tests/rules/test_man.py","","diff --git a/thefuck/rules/man.py b/thefuck/rules/man.py
index 3d0347a..e4ec54d 100644
--- a/thefuck/rules/man.py
+++ b/thefuck/rules/man.py
@@ -12,16 +12,22 @@ def get_new_command(command):
     if '2' in command.script:
         return command.script.replace(""2"", ""3"")
 
+    last_arg = command.script_parts[-1]
+    help_command = last_arg + ' --help'
+
+    # If there are no man pages for last_arg, suggest `last_arg --help` instead.
+    # Otherwise, suggest `--help` after suggesting other man page sections.
+    if command.stderr.strip() == 'No manual entry for ' + last_arg:
+        return [help_command]
+
     split_cmd2 = command.script_parts
     split_cmd3 = split_cmd2[:]
 
     split_cmd2.insert(1, ' 2 ')
     split_cmd3.insert(1, ' 3 ')
 
-    last_arg = command.script_parts[-1]
-
     return [
-        last_arg + ' --help',
         """".join(split_cmd3),
         """".join(split_cmd2),
+        help_command,
     ]
"
"thefuck","13","2af65071d84a7d1d14a4126364d9b4c9b5241f3c","237bc579994de633fe104714156ddfa925a50b6e","thefuck/rules/git_branch_exists.py","thefuck/rules/git_branch_exists.py","diff --git a/thefuck/rules/git_branch_exists.py b/thefuck/rules/git_branch_exists.py","tests/rules/test_git_branch_exists.py","","diff --git a/thefuck/rules/git_branch_exists.py b/thefuck/rules/git_branch_exists.py
index a2c0078..25b7e50 100644
--- a/thefuck/rules/git_branch_exists.py
+++ b/thefuck/rules/git_branch_exists.py
@@ -6,8 +6,7 @@ from thefuck.utils import eager
 
 @git_support
 def match(command):
-    return ('branch' in command.script
-            and ""fatal: A branch named '"" in command.stderr
+    return (""fatal: A branch named '"" in command.stderr
             and "" already exists."" in command.stderr)
 
 
@@ -17,7 +16,9 @@ def get_new_command(command):
     branch_name = re.findall(
         r""fatal: A branch named '([^']*)' already exists."", command.stderr)[0]
     new_command_templates = [['git branch -d {0}', 'git branch {0}'],
+                             ['git branch -d {0}', 'git checkout -b {0}'],
                              ['git branch -D {0}', 'git branch {0}'],
+                             ['git branch -D {0}', 'git checkout -b {0}'],
                              ['git checkout {0}']]
     for new_command_template in new_command_templates:
         yield shell.and_(*new_command_template).format(branch_name)
"
"thefuck","19","959b96cf6ec8cedda05dc58efe0e0f3bd6ed2f4e","dc23d67a42dad54308a753639edd1ea0d15cb2e7","thefuck/rules/git_push_force.py","thefuck/rules/git_push_force.py","diff --git a/thefuck/rules/git_push_force.py b/thefuck/rules/git_push_force.py","tests/rules/test_git_push_force.py","","diff --git a/thefuck/rules/git_push_force.py b/thefuck/rules/git_push_force.py
index 1bb23f8..918af8c 100644
--- a/thefuck/rules/git_push_force.py
+++ b/thefuck/rules/git_push_force.py
@@ -12,7 +12,7 @@ def match(command):
 
 @git_support
 def get_new_command(command):
-    return replace_argument(command.script, 'push', 'push --force')
+    return replace_argument(command.script, 'push', 'push --force-with-lease')
 
 
 enabled_by_default = False
"
"thefuck","9","ce6b82c92d78ae283cb3db001766b76f6647bc47","feb36ede5c518fdc3b6eddf945b2d8b1e2294d15","thefuck/rules/git_push.py","thefuck/rules/git_push.py","diff --git a/thefuck/rules/git_push.py b/thefuck/rules/git_push.py","tests/rules/test_git_push.py","","diff --git a/thefuck/rules/git_push.py b/thefuck/rules/git_push.py
index 0a624eb..f64d2ce 100644
--- a/thefuck/rules/git_push.py
+++ b/thefuck/rules/git_push.py
@@ -24,7 +24,11 @@ def get_new_command(command):
         pass
     if upstream_option_index is not -1:
         command.script_parts.pop(upstream_option_index)
-        command.script_parts.pop(upstream_option_index)
+        try:
+            command.script_parts.pop(upstream_option_index)
+        except IndexError:
+            # This happens for `git push -u`
+            pass
 
     push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]
     return replace_argument("" "".join(command.script_parts), 'push', push_upstream)
"
"thefuck","21","71dc2666ccf62e653291d9a7a08e2c6c3320425b","213791d3c2af379ffa37a140735998736b41912e","thefuck/rules/git_fix_stash.py","thefuck/rules/git_fix_stash.py","diff --git a/thefuck/rules/git_fix_stash.py b/thefuck/rules/git_fix_stash.py","tests/rules/test_git_fix_stash.py","","diff --git a/thefuck/rules/git_fix_stash.py b/thefuck/rules/git_fix_stash.py
index 8cf6ae3..944d37c 100644
--- a/thefuck/rules/git_fix_stash.py
+++ b/thefuck/rules/git_fix_stash.py
@@ -5,8 +5,12 @@ from thefuck.specific.git import git_support
 
 @git_support
 def match(command):
-    return (command.script.split()[1] == 'stash'
-            and 'usage:' in command.stderr)
+    splited_script = command.script.split()
+    if len(splited_script) > 1:
+        return (splited_script[1] == 'stash'
+                and 'usage:' in command.stderr)
+    else:
+        return False
 
 # git's output here is too complicated to be parsed (see the test file)
 stash_commands = (
"
"thefuck","30","de513cacb150049e3f95434f8d6d30b7ed1e0ea7","43fead02d3a24fef71534116c5550def0f56830c","thefuck/rules/fix_file.py","thefuck/rules/fix_file.py","diff --git a/thefuck/rules/fix_file.py b/thefuck/rules/fix_file.py","tests/rules/test_fix_file.py","","diff --git a/thefuck/rules/fix_file.py b/thefuck/rules/fix_file.py
index 5f4aafb..28b17ba 100644
--- a/thefuck/rules/fix_file.py
+++ b/thefuck/rules/fix_file.py
@@ -50,7 +50,12 @@ def _search(stderr):
 
 
 def match(command, settings):
-    return 'EDITOR' in os.environ and _search(command.stderr)
+    if 'EDITOR' not in os.environ:
+        return False
+
+    m = _search(command.stderr)
+
+    return m and os.path.isfile(m.group('file'))
 
 
 def get_new_command(command, settings):
"
"thefuck","15","3a39deb485995e67afb1919972cd1c9aaedf4c32","41707b80c61acadb7c87b0efcbf10f4186dc5937","thefuck/rules/git_add.py","thefuck/rules/git_add.py","diff --git a/thefuck/rules/git_add.py b/thefuck/rules/git_add.py","tests/rules/test_git_add.py","","diff --git a/thefuck/rules/git_add.py b/thefuck/rules/git_add.py
index 0cf1b05..a779a20 100644
--- a/thefuck/rules/git_add.py
+++ b/thefuck/rules/git_add.py
@@ -5,15 +5,14 @@ from thefuck.specific.git import git_support
 
 @git_support
 def match(command):
-    return ('did not match any file(s) known to git.' in command.stderr
-            and ""Did you forget to 'git add'?"" in command.stderr)
+    return 'did not match any file(s) known to git.' in command.stderr
 
 
 @git_support
 def get_new_command(command):
     missing_file = re.findall(
-            r""error: pathspec '([^']*)' ""
-            r""did not match any file\(s\) known to git."", command.stderr)[0]
+        r""error: pathspec '([^']*)' ""
+        r'did not match any file\(s\) known to git.', command.stderr)[0]
 
     formatme = shell.and_('git add -- {}', '{}')
     return formatme.format(missing_file, command.script)
"
"thefuck","32","cb33c912e5f2f4c2da6b70d708ff0437bfcd3b94","25cc98a21a3450a046caf418f08713c82a290805","thefuck/rules/ls_lah.py","thefuck/rules/ls_lah.py","diff --git a/thefuck/rules/ls_lah.py b/thefuck/rules/ls_lah.py","tests/rules/test_ls_lah.py","","diff --git a/thefuck/rules/ls_lah.py b/thefuck/rules/ls_lah.py
index 7eba5bd..70f6baa 100644
--- a/thefuck/rules/ls_lah.py
+++ b/thefuck/rules/ls_lah.py
@@ -1,5 +1,7 @@
 def match(command, settings):
-    return 'ls' in command.script and not ('ls -' in command.script)
+    return (command.script == 'ls'
+            or command.script.startswith('ls ')
+            and not ('ls -' in command.script))
 
 
 def get_new_command(command, settings):
"
"thefuck","11","92f3c8fb52b32b79005b4864c31a5c2d8c45f4b1","db7dffdb44ae5c7be8de088765463fbda96197d1","thefuck/rules/git_push.py","thefuck/rules/git_push.py","diff --git a/thefuck/rules/git_push.py b/thefuck/rules/git_push.py","tests/rules/test_git_push.py","","diff --git a/thefuck/rules/git_push.py b/thefuck/rules/git_push.py
index 86a2ea5..0a624eb 100644
--- a/thefuck/rules/git_push.py
+++ b/thefuck/rules/git_push.py
@@ -10,5 +10,21 @@ def match(command):
 
 @git_support
 def get_new_command(command):
+    # If --set-upstream or -u are passed, remove it and its argument. This is
+    # because the remaining arguments are concatenated onto the command suggested
+    # by git, which includes --set-upstream and its argument
+    upstream_option_index = -1
+    try:
+        upstream_option_index = command.script_parts.index('--set-upstream')
+    except ValueError:
+        pass
+    try:
+        upstream_option_index = command.script_parts.index('-u')
+    except ValueError:
+        pass
+    if upstream_option_index is not -1:
+        command.script_parts.pop(upstream_option_index)
+        command.script_parts.pop(upstream_option_index)
+
     push_upstream = command.stderr.split('\n')[-3].strip().partition('git ')[2]
-    return replace_argument(command.script, 'push', push_upstream)
+    return replace_argument("" "".join(command.script_parts), 'push', push_upstream)
"
"thefuck","6","797ca1c5647c565f62e21a8e29515c8b0fbe275f","7c858fadb3458be829d3d43666ccb46c3ed5b8a0","thefuck/rules/git_branch_exists.py","thefuck/rules/git_branch_exists.py","diff --git a/thefuck/rules/git_branch_exists.py b/thefuck/rules/git_branch_exists.py","tests/rules/test_git_branch_exists.py","","diff --git a/thefuck/rules/git_branch_exists.py b/thefuck/rules/git_branch_exists.py
index 93e11b5..4a7a822 100644
--- a/thefuck/rules/git_branch_exists.py
+++ b/thefuck/rules/git_branch_exists.py
@@ -7,14 +7,15 @@ from thefuck.utils import eager
 @git_support
 def match(command):
     return (""fatal: A branch named '"" in command.output
-            and "" already exists."" in command.output)
+            and ""' already exists."" in command.output)
 
 
 @git_support
 @eager
 def get_new_command(command):
     branch_name = re.findall(
-        r""fatal: A branch named '([^']*)' already exists."", command.output)[0]
+        r""fatal: A branch named '(.+)' already exists."", command.output)[0]
+    branch_name = branch_name.replace(""'"", r""\'"")
     new_command_templates = [['git branch -d {0}', 'git branch {0}'],
                              ['git branch -d {0}', 'git checkout -b {0}'],
                              ['git branch -D {0}', 'git branch {0}'],
"
"thefuck","17","f7f0660114a02fe49578ec5684dd02c81042d175","7ce4307c87c1e2e4106db2c961e48e249be987be","thefuck/shells/bash.py;thefuck/shells/zsh.py","thefuck/shells/bash.py;thefuck/shells/zsh.py","diff --git a/thefuck/shells/bash.py b/thefuck/shells/bash.py;diff --git a/thefuck/shells/zsh.py b/thefuck/shells/zsh.py","tests/shells/test_bash.py","","diff --git a/thefuck/shells/bash.py b/thefuck/shells/bash.py
index 9834937..d6e9b2c 100644
--- a/thefuck/shells/bash.py
+++ b/thefuck/shells/bash.py
@@ -1,7 +1,6 @@
-from subprocess import Popen, PIPE
 import os
 from ..conf import settings
-from ..utils import DEVNULL, memoize, cache
+from ..utils import memoize
 from .generic import Generic
 
 
@@ -9,7 +8,7 @@ class Bash(Generic):
     def app_alias(self, fuck):
         alias = ""TF_ALIAS={0}"" \
                 "" alias {0}='PYTHONIOENCODING=utf-8"" \
-                "" TF_CMD=$(thefuck $(fc -ln -1)) && "" \
+                "" TF_CMD=$(TF_SHELL_ALIASES=$(alias) thefuck $(fc -ln -1)) && "" \
                 "" eval $TF_CMD"".format(fuck)
 
         if settings.alter_history:
@@ -24,13 +23,10 @@ class Bash(Generic):
         return name, value
 
     @memoize
-    @cache('.bashrc', '.bash_profile')
     def get_aliases(self):
-        proc = Popen(['bash', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)
-        return dict(
-                self._parse_alias(alias)
-                for alias in proc.stdout.read().decode('utf-8').split('\n')
-                if alias and '=' in alias)
+        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\n')
+        return dict(self._parse_alias(alias)
+                    for alias in raw_aliases if alias and '=' in alias)
 
     def _get_history_file_name(self):
         return os.environ.get(""HISTFILE"",
diff --git a/thefuck/shells/zsh.py b/thefuck/shells/zsh.py
index f4e6c2e..a8c0587 100644
--- a/thefuck/shells/zsh.py
+++ b/thefuck/shells/zsh.py
@@ -26,7 +26,7 @@ class Zsh(Generic):
 
     @memoize
     def get_aliases(self):
-        raw_aliases = os.environ['TF_SHELL_ALIASES'].split('\n')
+        raw_aliases = os.environ.get('TF_SHELL_ALIASES', '').split('\n')
         return dict(self._parse_alias(alias)
                     for alias in raw_aliases if alias and '=' in alias)
 
"
"thefuck","28","88831c424f569e6a55fc98883d3eeecc7d425b18","9b30ae0424607a4e268bd26eaee8ccb91a5588f9","thefuck/rules/fix_file.py","thefuck/rules/fix_file.py","diff --git a/thefuck/rules/fix_file.py b/thefuck/rules/fix_file.py","tests/rules/test_fix_file.py","","diff --git a/thefuck/rules/fix_file.py b/thefuck/rules/fix_file.py
index a64b10c..caa59f0 100644
--- a/thefuck/rules/fix_file.py
+++ b/thefuck/rules/fix_file.py
@@ -1,9 +1,10 @@
 import re
 import os
-from thefuck.utils import memoize
+from thefuck.utils import memoize, wrap_settings
 from thefuck import shells
 
 
+# order is important: only the first match is considered
 patterns = (
         # js, node:
         '^    at {file}:{line}:{col}',
@@ -20,13 +21,13 @@ patterns = (
         # lua:
         '^lua: {file}:{line}:',
         # fish:
-        '^{file} \(line {line}\):',
+        '^{file} \\(line {line}\\):',
         # bash, sh, ssh:
         '^{file}: line {line}: ',
-        # ghc, make, ruby, zsh:
-        '^{file}:{line}:',
         # cargo, clang, gcc, go, pep8, rustc:
         '^{file}:{line}:{col}',
+        # ghc, make, ruby, zsh:
+        '^{file}:{line}:',
         # perl:
         'at {file} line {line}',
     )
@@ -56,12 +57,21 @@ def match(command, settings):
     return _search(command.stderr) or _search(command.stdout)
 
 
+@wrap_settings({'fixlinecmd': '{editor} {file} +{line}',
+                'fixcolcmd': None})
 def get_new_command(command, settings):
     m = _search(command.stderr) or _search(command.stdout)
 
     # Note: there does not seem to be a standard for columns, so they are just
-    # ignored for now
-    editor_call = '{} {} +{}'.format(os.environ['EDITOR'],
-                                     m.group('file'),
-                                     m.group('line'))
+    # ignored by default
+    if settings.fixcolcmd and 'col' in m.groupdict():
+        editor_call = settings.fixcolcmd.format(editor=os.environ['EDITOR'],
+                                                file=m.group('file'),
+                                                line=m.group('line'),
+                                                col=m.group('col'))
+    else:
+        editor_call = settings.fixlinecmd.format(editor=os.environ['EDITOR'],
+                                                 file=m.group('file'),
+                                                 line=m.group('line'))
+
     return shells.and_(editor_call, command.script)
"
"thefuck","20","0a6a3db65d2fc480c5b2f1135137f34c9f06b742","280751b36e715b006c631ba6c08de99ccc74f6d2","thefuck/rules/dirty_unzip.py","thefuck/rules/dirty_unzip.py","diff --git a/thefuck/rules/dirty_unzip.py b/thefuck/rules/dirty_unzip.py","tests/rules/test_dirty_unzip.py","","diff --git a/thefuck/rules/dirty_unzip.py b/thefuck/rules/dirty_unzip.py
index d0a1248..fced9b3 100644
--- a/thefuck/rules/dirty_unzip.py
+++ b/thefuck/rules/dirty_unzip.py
@@ -1,6 +1,7 @@
 import os
 import zipfile
 from thefuck.utils import for_app
+from thefuck.shells import quote
 
 
 def _is_bad_zip(file):
@@ -13,7 +14,7 @@ def _zip_file(command):
     # unzip [-flags] file[.zip] [file(s) ...] [-x file(s) ...]
     #                ^          ^ files to unzip from the archive
     #                archive to unzip
-    for c in command.script.split()[1:]:
+    for c in command.split_script[1:]:
         if not c.startswith('-'):
             if c.endswith('.zip'):
                 return c
@@ -28,7 +29,7 @@ def match(command):
 
 
 def get_new_command(command):
-    return '{} -d {}'.format(command.script, _zip_file(command)[:-4])
+    return '{} -d {}'.format(command.script, quote(_zip_file(command)[:-4]))
 
 
 def side_effect(old_cmd, command):
"
"thefuck","27","bc6b107066d3f1e60b4cfcaa8cf6399e98cf1b1c","1becd92b126a368d6e7d93aa8eea209414ce4aa2","thefuck/rules/open.py","thefuck/rules/open.py","diff --git a/thefuck/rules/open.py b/thefuck/rules/open.py","tests/rules/test_open.py","","diff --git a/thefuck/rules/open.py b/thefuck/rules/open.py
index 756f288..22aaea3 100644
--- a/thefuck/rules/open.py
+++ b/thefuck/rules/open.py
@@ -23,4 +23,4 @@ def match(command, settings):
 
 
 def get_new_command(command, settings):
-    return 'open http://' + command.script[5:]
+    return command.script.replace('open ', 'open http://')
"
"thefuck","12","4c2fc490f280ca3921785d8f58a37274ced35ce6","ca787a1cba3cc9b26b43919c5e60acb40ebcd919","thefuck/rules/no_command.py","thefuck/rules/no_command.py","diff --git a/thefuck/rules/no_command.py b/thefuck/rules/no_command.py","tests/rules/test_no_command.py","","diff --git a/thefuck/rules/no_command.py b/thefuck/rules/no_command.py
index 34b7a4b..174ee78 100644
--- a/thefuck/rules/no_command.py
+++ b/thefuck/rules/no_command.py
@@ -1,12 +1,12 @@
 from difflib import get_close_matches
 from thefuck.utils import get_all_executables, \
-    get_valid_history_without_current, get_closest
+    get_valid_history_without_current, get_closest, which
 from thefuck.specific.sudo import sudo_support
 
 
 @sudo_support
 def match(command):
-    return (command.script_parts
+    return (not which(command.script_parts[0])
             and 'not found' in command.stderr
             and bool(get_close_matches(command.script_parts[0],
                                        get_all_executables())))
"
"thefuck","29","4a2f869c6d6ef03d8d7ada1121cc6631d7ef979e","88831c424f569e6a55fc98883d3eeecc7d425b18","thefuck/types.py","thefuck/types.py","diff --git a/thefuck/types.py b/thefuck/types.py","tests/test_types.py;tests/test_utils.py","","diff --git a/thefuck/types.py b/thefuck/types.py
index cc48138..2e5b41a 100644
--- a/thefuck/types.py
+++ b/thefuck/types.py
@@ -23,7 +23,9 @@ class Settings(dict):
         return self.get(item)
 
     def update(self, **kwargs):
-        """"""Returns new settings with new values from `kwargs`.""""""
-        conf = dict(self)
-        conf.update(kwargs)
+        """"""
+        Returns new settings with values from `kwargs` for unset settings.
+        """"""
+        conf = dict(kwargs)
+        conf.update(self)
         return Settings(conf)
"
"thefuck","4","68949a592248913b52bfd50036893553153fcddb","8db3cf604865e559090412ce80b0640e290ad83a","thefuck/shells/fish.py","thefuck/shells/fish.py","diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py","tests/shells/test_fish.py","","diff --git a/thefuck/shells/fish.py b/thefuck/shells/fish.py
index 471df2a..5693404 100644
--- a/thefuck/shells/fish.py
+++ b/thefuck/shells/fish.py
@@ -20,9 +20,17 @@ def _get_functions(overridden):
 def _get_aliases(overridden):
     aliases = {}
     proc = Popen(['fish', '-ic', 'alias'], stdout=PIPE, stderr=DEVNULL)
-    alias_out = proc.stdout.read().decode('utf-8').strip().split('\n')
-    for alias in alias_out:
-        name, value = alias.replace('alias ', '', 1).split(' ', 1)
+    alias_out = proc.stdout.read().decode('utf-8').strip()
+    if not alias_out:
+        return aliases
+    for alias in alias_out.split('\n'):
+        for separator in (' ', '='):
+            split_alias = alias.replace('alias ', '', 1).split(separator, 1)
+            if len(split_alias) == 2:
+                name, value = split_alias
+                break
+        else:
+            continue
         if name not in overridden:
             aliases[name] = value
     return aliases
"
"thefuck","26","7cb0388ed0845545e878b29783bbf8e901a02745","feb3eee2a08f0cba4552373d728509bc90b561ab","thefuck/rules/vagrant_up.py","thefuck/rules/vagrant_up.py","diff --git a/thefuck/rules/vagrant_up.py b/thefuck/rules/vagrant_up.py","tests/rules/test_vagrant_up.py","","diff --git a/thefuck/rules/vagrant_up.py b/thefuck/rules/vagrant_up.py
index 1c27db4..9c0a1e4 100644
--- a/thefuck/rules/vagrant_up.py
+++ b/thefuck/rules/vagrant_up.py
@@ -7,7 +7,12 @@ def match(command, settings):
 
 def get_new_command(command, settings):
     cmds = command.script.split(' ')
-    machine = """"
+    machine = None
     if len(cmds) >= 3:
         machine = cmds[2]
-    return shells.and_(""vagrant up "" +  machine, command.script)
+
+    startAllInstances = shells.and_(""vagrant up"", command.script)
+    if machine is None: 
+        return startAllInstances
+    else:
+        return [ shells.and_(""vagrant up "" +  machine, command.script), startAllInstances]
"
"thefuck","5","7c858fadb3458be829d3d43666ccb46c3ed5b8a0","c205683a8df8a57e2db1e9816a5a7ce3255b08fc","thefuck/rules/git_push.py","thefuck/rules/git_push.py","diff --git a/thefuck/rules/git_push.py b/thefuck/rules/git_push.py","tests/rules/test_git_push.py","","diff --git a/thefuck/rules/git_push.py b/thefuck/rules/git_push.py
index 551b25d..cccee67 100644
--- a/thefuck/rules/git_push.py
+++ b/thefuck/rules/git_push.py
@@ -6,7 +6,7 @@ from thefuck.specific.git import git_support
 @git_support
 def match(command):
     return ('push' in command.script_parts
-            and 'set-upstream' in command.output)
+            and 'git push --set-upstream' in command.output)
 
 
 def _get_upstream_option_index(command_parts):
"
"thefuck","24","12394ca8423a438915fed996383b44471fc1139d","5d74344994da89ed01afd448f1c9d86b85e85351","thefuck/types.py","thefuck/types.py","diff --git a/thefuck/types.py b/thefuck/types.py","tests/test_types.py","","diff --git a/thefuck/types.py b/thefuck/types.py
index 6da4234..3f2155b 100644
--- a/thefuck/types.py
+++ b/thefuck/types.py
@@ -4,12 +4,31 @@ from .logs import debug
 
 Command = namedtuple('Command', ('script', 'stdout', 'stderr'))
 
-CorrectedCommand = namedtuple('CorrectedCommand', ('script', 'side_effect', 'priority'))
-
 Rule = namedtuple('Rule', ('name', 'match', 'get_new_command',
                            'enabled_by_default', 'side_effect',
                            'priority', 'requires_output'))
 
+class CorrectedCommand(object):
+    def __init__(self, script, side_effect, priority):
+        self.script = script
+        self.side_effect = side_effect
+        self.priority = priority
+
+    def __eq__(self, other):
+        """"""Ignores `priority` field.""""""
+        if isinstance(other, CorrectedCommand):
+            return (other.script, other.side_effect) ==\
+                   (self.script, self.side_effect)
+        else:
+            return False
+
+    def __hash__(self):
+        return (self.script, self.side_effect).__hash__()
+
+    def __repr__(self):
+        return 'CorrectedCommand(script={}, side_effect={}, priority={})'.format(
+            self.script, self.side_effect, self.priority)
+
 
 class RulesNamesList(list):
     """"""Wrapper a top of list for storing rules names.""""""
@@ -54,19 +73,17 @@ class SortedCorrectedCommandsSequence(object):
             return []
 
         for command in self._commands:
-            if command.script != first.script or \
-                            command.side_effect != first.side_effect:
+            if command != first:
                 return [first, command]
         return [first]
 
     def _remove_duplicates(self, corrected_commands):
         """"""Removes low-priority duplicates.""""""
-        commands = {(command.script, command.side_effect): command
+        commands = {command
                     for command in sorted(corrected_commands,
                                           key=lambda command: -command.priority)
-                    if command.script != self._cached[0].script
-                    or command.side_effect != self._cached[0].side_effect}
-        return commands.values()
+                    if command.script != self._cached[0]}
+        return commands
 
     def _realise(self):
         """"""Realises generator, removes duplicates and sorts commands.""""""
"
"luigi","22","9c4c47ae449593c55fb67ce51115d0be1fecb163","2db9768958c9665c2bb78f040054a25534205fc4","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/scheduler_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index 37ab3d91..fe5a4fe0 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -224,7 +224,7 @@ class Worker(object):
     Structure for tracking worker activity and keeping their references.
     """"""
 
-    def __init__(self, worker_id, last_active=None):
+    def __init__(self, worker_id, last_active=time.time()):
         self.id = worker_id
         self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)
         self.last_active = last_active  # seconds since epoch
"
"luigi","18","6cffbf438d023441f7f42c2019a51c62eecd9018","c521d59c5eacf6c19ce3c17a62f73e042fa0556e","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/central_planner_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index 7c4d4718..edcaa5d5 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -402,7 +402,7 @@ class SimpleTaskState(object):
                 self.re_enable(task)
 
             # don't allow workers to override a scheduler disable
-            elif task.scheduler_disable_time is not None:
+            elif task.scheduler_disable_time is not None and new_status != DISABLED:
                 return
 
         if new_status == FAILED and task.can_disable() and task.status != DISABLED:
"
"luigi","14","f7219c38121098d464011a094156d99b5b320362","43f2de2646c8e1efd6e17ffabbb11accc21e70b6","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/central_planner_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index a75c1096..9ea1bcc7 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -90,9 +90,9 @@ class scheduler(Config):
     # These disables last for disable_persist seconds.
     disable_window = parameter.IntParameter(default=3600,
                                             config_path=dict(section='scheduler', name='disable-window-seconds'))
-    disable_failures = parameter.IntParameter(default=None,
+    disable_failures = parameter.IntParameter(default=999999999,
                                               config_path=dict(section='scheduler', name='disable-num-failures'))
-    disable_hard_timeout = parameter.IntParameter(default=None,
+    disable_hard_timeout = parameter.IntParameter(default=999999999,
                                                   config_path=dict(section='scheduler', name='disable-hard-timeout'))
     disable_persist = parameter.IntParameter(default=86400,
                                              config_path=dict(section='scheduler', name='disable-persist-seconds'))
@@ -199,8 +199,7 @@ class Task(object):
         self.failures.add_failure()
 
     def has_excessive_failures(self):
-        if (self.failures.first_failure_time is not None and
-                self.disable_hard_timeout):
+        if self.failures.first_failure_time is not None:
             if (time.time() >= self.failures.first_failure_time +
                     self.disable_hard_timeout):
                 return True
@@ -210,10 +209,6 @@ class Task(object):
 
         return False
 
-    def can_disable(self):
-        return (self.disable_failures is not None or
-                self.disable_hard_timeout is not None)
-
     @property
     def pretty_id(self):
         param_str = ', '.join('{}={}'.format(key, value) for key, value in self.params.items())
@@ -383,7 +378,7 @@ class SimpleTaskState(object):
             elif task.scheduler_disable_time is not None and new_status != DISABLED:
                 return
 
-        if new_status == FAILED and task.can_disable() and task.status != DISABLED:
+        if new_status == FAILED and task.status != DISABLED:
             task.add_failure()
             if task.has_excessive_failures():
                 task.scheduler_disable_time = time.time()
"
"luigi","3","a0f1db01ddab5b4b2bda3fbe58bad09a6d94a7b4","3a0bfbff69addfb3be1107adab3d4914bcae3e4b","luigi/parameter.py","luigi/parameter.py","diff --git a/luigi/parameter.py b/luigi/parameter.py","test/parameter_test.py","","diff --git a/luigi/parameter.py b/luigi/parameter.py
index 13c6f8af..66395205 100644
--- a/luigi/parameter.py
+++ b/luigi/parameter.py
@@ -1114,8 +1114,8 @@ class TupleParameter(ListParameter):
         try:
             # loop required to parse tuple of tuples
             return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))
-        except ValueError:
-            return literal_eval(x)  # if this causes an error, let that error be raised.
+        except (ValueError, TypeError):
+            return tuple(literal_eval(x))  # if this causes an error, let that error be raised.
 
 
 class NumericalParameter(Parameter):
"
"luigi","16","e38392a1381dd8daee0f180f0ac7f651edb88e0c","96f2b5a97c2cc5f63bea0f422c57f93dcec0ebac","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/central_planner_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index 8f5beada..3a08956f 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -567,7 +567,8 @@ class CentralPlannerScheduler(Scheduler):
 
         for task in self._state.get_active_tasks():
             self._state.fail_dead_worker_task(task, self._config, assistant_ids)
-            if task.id not in necessary_tasks and self._state.prune(task, self._config):
+            removed = self._state.prune(task, self._config)
+            if removed and task.id not in necessary_tasks:
                 remove_tasks.append(task.id)
 
         self._state.inactivate_tasks(remove_tasks)
"
"luigi","2","baa54c9f4f809692d62d4c3e4497161717c76550","24e85945ae39e5975491527e00c3f0f64b42ea6e","luigi/contrib/beam_dataflow.py","luigi/contrib/beam_dataflow.py","diff --git a/luigi/contrib/beam_dataflow.py b/luigi/contrib/beam_dataflow.py","test/contrib/beam_dataflow_test.py","","diff --git a/luigi/contrib/beam_dataflow.py b/luigi/contrib/beam_dataflow.py
index dd510786..42cdc742 100644
--- a/luigi/contrib/beam_dataflow.py
+++ b/luigi/contrib/beam_dataflow.py
@@ -219,6 +219,7 @@ class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):
     def __init__(self):
         if not isinstance(self.dataflow_params, DataflowParamKeys):
             raise ValueError(""dataflow_params must be of type DataflowParamKeys"")
+        super(BeamDataflowJobTask, self).__init__()
 
     @abstractmethod
     def dataflow_executable(self):
@@ -471,9 +472,13 @@ class BeamDataflowJobTask(MixinNaiveBulkComplete, luigi.Task):
 
     @staticmethod
     def get_target_path(target):
+        """"""
+            Given a luigi Target, determine a stringly typed path to pass as a
+            Dataflow job argument.
+        """"""
         if isinstance(target, luigi.LocalTarget) or isinstance(target, gcs.GCSTarget):
             return target.path
         elif isinstance(target, bigquery.BigQueryTarget):
-            ""{}:{}.{}"".format(target.project_id, target.dataset_id, target.table_id)
+            return ""{}:{}.{}"".format(target.table.project_id, target.table.dataset_id, target.table.table_id)
         else:
-            raise ValueError(""Target not supported"")
+            raise ValueError(""Target %s not supported"" % target)
"
"luigi","8","61ee32e9968978c32be12a6af0affa3a5750e87e","8874b93165953c4f6bbe7b747804654d13290018","luigi/contrib/redshift.py","luigi/contrib/redshift.py","diff --git a/luigi/contrib/redshift.py b/luigi/contrib/redshift.py","test/contrib/redshift_test.py","","diff --git a/luigi/contrib/redshift.py b/luigi/contrib/redshift.py
index 398d5f39..a3ac8fc3 100644
--- a/luigi/contrib/redshift.py
+++ b/luigi/contrib/redshift.py
@@ -332,11 +332,11 @@ class S3CopyToTable(rdbms.CopyToTable):
         if '.' in self.table:
             query = (""select 1 as table_exists ""
                      ""from information_schema.tables ""
-                     ""where table_schema = %s and table_name = %s limit 1"")
+                     ""where table_schema = lower(%s) and table_name = lower(%s) limit 1"")
         else:
             query = (""select 1 as table_exists ""
                      ""from pg_table_def ""
-                     ""where tablename = %s limit 1"")
+                     ""where tablename = lower(%s) limit 1"")
         cursor = connection.cursor()
         try:
             cursor.execute(query, tuple(self.table.split('.')))
"
"luigi","23","c707253572deb795a900c3e07d21eee591a55fca","dc41727f4de88f86f4e77aa45be51eff4ee6b3be","luigi/interface.py;luigi/scheduler.py","luigi/interface.py;luigi/scheduler.py","diff --git a/luigi/interface.py b/luigi/interface.py;diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/worker_external_task_test.py","","diff --git a/luigi/interface.py b/luigi/interface.py
index 0a15fc69..860c1596 100644
--- a/luigi/interface.py
+++ b/luigi/interface.py
@@ -109,7 +109,7 @@ class core(task.Config):
 class WorkerSchedulerFactory(object):
 
     def create_local_scheduler(self):
-        return scheduler.CentralPlannerScheduler()
+        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)
 
     def create_remote_scheduler(self, host, port):
         return rpc.RemoteScheduler(host=host, port=port)
diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index bbd00efa..81dafb15 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -101,6 +101,8 @@ class scheduler(Config):
 
     visualization_graph = parameter.Parameter(default=""svg"", config_path=dict(section='scheduler', name='visualization-graph'))
 
+    prune_on_get_work = parameter.BoolParameter(default=False)
+
 
 def fix_time(x):
     # Backwards compatibility for a fix in Dec 2014. Prior to the fix, pickled state might store datetime objects
@@ -227,7 +229,7 @@ class Worker(object):
     def __init__(self, worker_id, last_active=None):
         self.id = worker_id
         self.reference = None  # reference to the worker in the real world. (Currently a dict containing just the host)
-        self.last_active = last_active  # seconds since epoch
+        self.last_active = last_active or time.time()  # seconds since epoch
         self.started = time.time()  # seconds since epoch
         self.tasks = set()  # task objects
         self.info = {}
@@ -710,6 +712,9 @@ class CentralPlannerScheduler(Scheduler):
         # TODO: remove tasks that can't be done, figure out if the worker has absolutely
         # nothing it can wait for
 
+        if self._config.prune_on_get_work:
+            self.prune()
+
         worker_id = kwargs['worker']
         # Return remaining tasks that have no FAILED descendents
         self.update(worker_id, {'host': host})
"
"luigi","1","1164eb6b85b8a70f596dbb99452bec513e72c12e","aec5dc2ed8db53fc282a0bd24aabe59031b6d1ba","luigi/server.py","luigi/server.py","diff --git a/luigi/server.py b/luigi/server.py","test/server_test.py","","diff --git a/luigi/server.py b/luigi/server.py
index 3cb77ede..41dc28f3 100644
--- a/luigi/server.py
+++ b/luigi/server.py
@@ -279,9 +279,10 @@ class MetricsHandler(tornado.web.RequestHandler):
         self._scheduler = scheduler
 
     def get(self):
-        metrics = self._scheduler._state._metrics_collector.generate_latest()
+        metrics_collector = self._scheduler._state._metrics_collector
+        metrics = metrics_collector.generate_latest()
         if metrics:
-            metrics.configure_http_handler(self)
+            metrics_collector.configure_http_handler(self)
             self.write(metrics)
 
 
"
"luigi","31","554850f32784537796383db5ee188d0455863ca9","c0857e9e06012b696017e0a353ae74f4f621d066","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/central_planner_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index 1c86da9a..2052663e 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -653,7 +653,7 @@ class CentralPlannerScheduler(Scheduler):
         tasks.sort(key=self._rank(), reverse=True)
 
         for task in tasks:
-            in_workers = assistant or worker in task.workers
+            in_workers = (assistant and task.workers) or worker in task.workers
             if task.status == 'RUNNING' and in_workers:
                 # Return a list of currently running tasks to the client,
                 # makes it easier to troubleshoot
"
"luigi","7","dd1f2ce0061e7787166522a3c75339ba4755dd2c","daf9ce99a3a7ed4227d1564570c5fce8848357e5","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/scheduler_api_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index fb43fc9b..b84aa849 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -823,8 +823,8 @@ class Scheduler(object):
                 for batch_task in self._state.get_batch_running_tasks(task.batch_id):
                     batch_task.expl = expl
 
-        if not (task.status in (RUNNING, BATCH_RUNNING) and status == PENDING) or new_deps:
-            # don't allow re-scheduling of task while it is running, it must either fail or succeed first
+        if not (task.status in (RUNNING, BATCH_RUNNING) and (status not in (DONE, FAILED, RUNNING) or task.worker_running != worker_id)) or new_deps:
+            # don't allow re-scheduling of task while it is running, it must either fail or succeed on the worker actually running it
             if status == PENDING or status != task.status:
                 # Update the DB only if there was a acctual change, to prevent noise.
                 # We also check for status == PENDING b/c that's the default value
"
"luigi","25","d7ec31609c88503391d12d65b6037f397feff816","040bbc9ef8d1703b64d13c60f271fded63e13601","luigi/contrib/redshift.py","luigi/contrib/redshift.py","diff --git a/luigi/contrib/redshift.py b/luigi/contrib/redshift.py","test/contrib/redshift_test.py","","diff --git a/luigi/contrib/redshift.py b/luigi/contrib/redshift.py
index af0223ea..93d5a392 100644
--- a/luigi/contrib/redshift.py
+++ b/luigi/contrib/redshift.py
@@ -163,7 +163,7 @@ class S3CopyToTable(rdbms.CopyToTable):
         if not (self.table):
             raise Exception(""table need to be specified"")
 
-        path = self.s3_load_path()
+        path = self.s3_load_path
         connection = self.output().connect()
         if not self.does_table_exist(connection):
             # try creating table
"
"luigi","10","f538d1b3d473d542a19d508e5f7e0809b1dfe5ef","3c55acd2cd5cf9c6c760bec5bb3159e0bc48a614","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/scheduler_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index 16b203b1..6cc4b87f 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -302,7 +302,7 @@ class Worker(object):
             return six.moves.filter(lambda task: task.status in [PENDING, RUNNING],
                                     self.tasks)
         else:
-            return state.get_pending_tasks()
+            return six.moves.filter(lambda task: self.id in task.workers, state.get_pending_tasks())
 
     def is_trivial_worker(self, state):
         """"""
"
"luigi","13","3c90bcdac63d978dbdaeae408420e22b963c9863","a8e64fe7f83d69702166a44c7e8cb9470ff31040","luigi/file.py","luigi/file.py","diff --git a/luigi/file.py b/luigi/file.py","test/file_test.py","","diff --git a/luigi/file.py b/luigi/file.py
index 816049f4..f87ac559 100644
--- a/luigi/file.py
+++ b/luigi/file.py
@@ -88,7 +88,7 @@ class LocalFileSystem(FileSystem):
             raise RuntimeError('Destination exists: %s' % new_path)
         d = os.path.dirname(new_path)
         if d and not os.path.exists(d):
-            self.fs.mkdir(d)
+            self.mkdir(d)
         os.rename(old_path, new_path)
 
 
"
"luigi","19","3d2f75224c7649402927a5ef57558d8c3717cd94","6cffbf438d023441f7f42c2019a51c62eecd9018","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/central_planner_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index cf0d8c40..7c4d4718 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -405,7 +405,7 @@ class SimpleTaskState(object):
             elif task.scheduler_disable_time is not None:
                 return
 
-        if new_status == FAILED and task.can_disable():
+        if new_status == FAILED and task.can_disable() and task.status != DISABLED:
             task.add_failure()
             if task.has_excessive_failures():
                 task.scheduler_disable_time = time.time()
"
"luigi","9","f7e0b7710fd8b12f27625b1efb62a8fde6e206c4","b7115974c3deadf77113686248b39567cb67e38f","luigi/execution_summary.py;test/retcodes_test.py","luigi/execution_summary.py;test/retcodes_test.py","diff --git a/luigi/execution_summary.py b/luigi/execution_summary.py;diff --git a/test/retcodes_test.py b/test/retcodes_test.py","test/execution_summary_test.py","","diff --git a/luigi/execution_summary.py b/luigi/execution_summary.py
index 27efbcb6..256f341b 100644
--- a/luigi/execution_summary.py
+++ b/luigi/execution_summary.py
@@ -43,12 +43,13 @@ def _partition_tasks(worker):
     set_tasks[""completed""] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}
     set_tasks[""already_done""] = {task for (task, status, ext) in task_history
                                  if status == 'DONE' and task not in pending_tasks and task not in set_tasks[""completed""]}
-    set_tasks[""failed""] = {task for (task, status, ext) in task_history if status == 'FAILED'}
+    set_tasks[""ever_failed""] = {task for (task, status, ext) in task_history if status == 'FAILED'}
+    set_tasks[""failed""] = set_tasks[""ever_failed""] - set_tasks[""completed""]
     set_tasks[""scheduling_error""] = {task for(task, status, ext) in task_history if status == 'UNKNOWN'}
     set_tasks[""still_pending_ext""] = {task for (task, status, ext) in task_history
-                                      if status == 'PENDING' and task not in set_tasks[""failed""] and task not in set_tasks[""completed""] and not ext}
+                                      if status == 'PENDING' and task not in set_tasks[""ever_failed""] and task not in set_tasks[""completed""] and not ext}
     set_tasks[""still_pending_not_ext""] = {task for (task, status, ext) in task_history
-                                          if status == 'PENDING' and task not in set_tasks[""failed""] and task not in set_tasks[""completed""] and ext}
+                                          if status == 'PENDING' and task not in set_tasks[""ever_failed""] and task not in set_tasks[""completed""] and ext}
     set_tasks[""run_by_other_worker""] = set()
     set_tasks[""upstream_failure""] = set()
     set_tasks[""upstream_missing_dependency""] = set()
@@ -87,7 +88,7 @@ def _depth_first_search(set_tasks, current_task, visited):
         for task in current_task._requires():
             if task not in visited:
                 _depth_first_search(set_tasks, task, visited)
-            if task in set_tasks[""failed""] or task in set_tasks[""upstream_failure""]:
+            if task in set_tasks[""ever_failed""] or task in set_tasks[""upstream_failure""]:
                 set_tasks[""upstream_failure""].add(current_task)
                 upstream_failure = True
             if task in set_tasks[""still_pending_ext""] or task in set_tasks[""upstream_missing_dependency""]:
@@ -261,6 +262,7 @@ def _get_comments(group_tasks):
 _ORDERED_STATUSES = (
     ""already_done"",
     ""completed"",
+    ""ever_failed"",
     ""failed"",
     ""scheduling_error"",
     ""still_pending"",
@@ -377,11 +379,15 @@ def _summary_format(set_tasks, worker):
         str_output += 'Did not run any tasks'
     smiley = """"
     reason = """"
-    if set_tasks[""failed""]:
-        smiley = "":(""
-        reason = ""there were failed tasks""
-        if set_tasks[""scheduling_error""]:
-            reason += "" and tasks whose scheduling failed""
+    if set_tasks[""ever_failed""]:
+        if not set_tasks[""failed""]:
+            smiley = "":)""
+            reason = ""there were failed tasks but they all suceeded in a retry""
+        else:
+            smiley = "":(""
+            reason = ""there were failed tasks""
+            if set_tasks[""scheduling_error""]:
+                reason += "" and tasks whose scheduling failed""
     elif set_tasks[""scheduling_error""]:
         smiley = "":(""
         reason = ""there were tasks whose scheduling failed""
diff --git a/test/retcodes_test.py b/test/retcodes_test.py
index e517bf21..db13fd51 100644
--- a/test/retcodes_test.py
+++ b/test/retcodes_test.py
@@ -170,3 +170,22 @@ class RetcodesTest(LuigiTestCase):
         with mock.patch('luigi.scheduler.Scheduler.add_task', new_func):
             self.run_and_expect('RequiringTask', 0)
             self.run_and_expect('RequiringTask --retcode-not-run 5', 5)
+
+    """"""
+    Test that a task once crashing and then succeeding should be counted as no failure.
+    """"""
+    def test_retry_sucess_task(self):
+        class Foo(luigi.Task):
+            run_count = 0
+
+            def run(self):
+                self.run_count += 1
+                if self.run_count == 1:
+                    raise ValueError()
+
+            def complete(self):
+                return self.run_count > 0
+
+        self.run_and_expect('Foo --scheduler-retry-delay=0', 0)
+        self.run_and_expect('Foo --scheduler-retry-delay=0 --retcode-task-failed=5', 0)
+        self.run_with_config(dict(task_failed='3'), 'Foo', 0)
"
"luigi","21","b7768da963570bd2223d97c1035f811c2eaf30b4","1a6a89d8b510089392bb407d4ec660451deb1f23","luigi/interface.py","luigi/interface.py","diff --git a/luigi/interface.py b/luigi/interface.py","test/interface_test.py","","diff --git a/luigi/interface.py b/luigi/interface.py
index 326232c3..3bcd9090 100644
--- a/luigi/interface.py
+++ b/luigi/interface.py
@@ -337,6 +337,9 @@ def run(cmdline_args=None, main_task_cls=None,
     :param use_dynamic_argparse:
     :param local_scheduler:
     """"""
+    if cmdline_args is None:
+        cmdline_args = sys.argv[1:]
+
     if use_dynamic_argparse:
         interface = DynamicArgParseInterface()
     else:
"
"luigi","30","97fa4afea3748f0d714482d2c97990bb467bc9d1","f1e3fb48fe9877e511a2d079636fd75eaaba4573","luigi/worker.py","luigi/worker.py","diff --git a/luigi/worker.py b/luigi/worker.py","test/test_event_callbacks.py","","diff --git a/luigi/worker.py b/luigi/worker.py
index 43002464..f1f20f3a 100644
--- a/luigi/worker.py
+++ b/luigi/worker.py
@@ -185,25 +185,23 @@ class TaskProcess(AbstractTaskProcess):
             self.task.trigger_event(Event.START, self.task)
             t0 = time.time()
             status = None
-            try:
-                new_deps = self._run_get_new_deps()
-                if new_deps is None:
-                    status = RUNNING
-                else:
-                    status = SUSPENDED
-                    logger.info(
-                        '[pid %s] Worker %s new requirements      %s',
-                        os.getpid(), self.worker_id, self.task.task_id)
-                    return
-            finally:
-                if status != SUSPENDED:
-                    self.task.trigger_event(
-                        Event.PROCESSING_TIME, self.task, time.time() - t0)
-                    error_message = json.dumps(self.task.on_success())
-                    logger.info('[pid %s] Worker %s done      %s', os.getpid(),
-                                self.worker_id, self.task.task_id)
-                    self.task.trigger_event(Event.SUCCESS, self.task)
-                    status = DONE
+
+            new_deps = self._run_get_new_deps()
+
+            if new_deps is None:
+                status = DONE
+                self.task.trigger_event(
+                    Event.PROCESSING_TIME, self.task, time.time() - t0)
+                error_message = json.dumps(self.task.on_success())
+                logger.info('[pid %s] Worker %s done      %s', os.getpid(),
+                            self.worker_id, self.task.task_id)
+                self.task.trigger_event(Event.SUCCESS, self.task)
+
+            else:
+                status = SUSPENDED
+                logger.info(
+                    '[pid %s] Worker %s new requirements      %s',
+                    os.getpid(), self.worker_id, self.task.task_id)
 
         except KeyboardInterrupt:
             raise
"
"luigi","15","a822f55d4d7c5adf5b9e3b64f23189d8305e9bf9","736c0f1352463c20ece84f2f651bcd37fd2b88ae","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/central_planner_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index d67beb9e..bb419af2 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -482,8 +482,8 @@ class SimpleTaskState(object):
     def get_necessary_tasks(self):
         necessary_tasks = set()
         for task in self.get_active_tasks():
-            if task.status not in (DONE, DISABLED) or \
-                    getattr(task, 'scheduler_disable_time', None) is not None:
+            if task.status not in (DONE, DISABLED, UNKNOWN) or \
+                    task.scheduler_disable_time is not None:
                 necessary_tasks.update(task.deps)
                 necessary_tasks.add(task.id)
         return necessary_tasks
"
"luigi","32","7233a0cc3e34c7c14259b1fa046f66332914f410","027ac3fbcf66d5d21554c9ac6da26eee5c2e0f3d","luigi/task_register.py","luigi/task_register.py","diff --git a/luigi/task_register.py b/luigi/task_register.py","test/instance_test.py","","diff --git a/luigi/task_register.py b/luigi/task_register.py
index ad6dfd1b..393d35d8 100644
--- a/luigi/task_register.py
+++ b/luigi/task_register.py
@@ -25,6 +25,8 @@ except ImportError:
     from ordereddict import OrderedDict
 
 from luigi import six
+import logging
+logger = logging.getLogger('luigi-interface')
 
 
 class TaskClassException(Exception):
"
"luigi","11","bf8b5cba573d5d6cd11f7f10a03b458aeaf955c1","70d8734d60e168389f425082b41b1936d63c028e","luigi/scheduler.py","luigi/scheduler.py","diff --git a/luigi/scheduler.py b/luigi/scheduler.py","test/scheduler_api_test.py","","diff --git a/luigi/scheduler.py b/luigi/scheduler.py
index 8938f565..faa6c8bd 100644
--- a/luigi/scheduler.py
+++ b/luigi/scheduler.py
@@ -894,7 +894,8 @@ class Scheduler(object):
 
             if (best_task and batched_params and task.family == best_task.family and
                     len(batched_tasks) < max_batch_size and task.is_batchable() and all(
-                    task.params.get(name) == value for name, value in unbatched_params.items())):
+                    task.params.get(name) == value for name, value in unbatched_params.items()) and
+                    self._schedulable(task)):
                 for name, params in batched_params.items():
                     params.append(task.params.get(name))
                 batched_tasks.append(task)
"
"luigi","6","d9667b7c8ce75f17efa383e8f64e27e5852e6f89","ce881b2a95743887c6147ff4ba23ce5f622b3f5e","luigi/parameter.py","luigi/parameter.py","diff --git a/luigi/parameter.py b/luigi/parameter.py","test/parameter_test.py","","diff --git a/luigi/parameter.py b/luigi/parameter.py
index 4493fb6d..7dcbe1f5 100644
--- a/luigi/parameter.py
+++ b/luigi/parameter.py
@@ -821,11 +821,21 @@ def _recursively_freeze(value):
     """"""
     if isinstance(value, Mapping):
         return _FrozenOrderedDict(((k, _recursively_freeze(v)) for k, v in value.items()))
-    elif isinstance(value, list):
+    elif isinstance(value, list) or isinstance(value, tuple):
         return tuple(_recursively_freeze(v) for v in value)
     return value
 
 
+class _DictParamEncoder(JSONEncoder):
+    """"""
+    JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.
+    """"""
+    def default(self, obj):
+        if isinstance(obj, _FrozenOrderedDict):
+            return obj.get_wrapped()
+        return json.JSONEncoder.default(self, obj)
+
+
 class DictParameter(Parameter):
     """"""
     Parameter whose value is a ``dict``.
@@ -858,16 +868,6 @@ class DictParameter(Parameter):
     tags, that are dynamically constructed outside Luigi), or you have a complex parameter containing logically related
     values (like a database connection config).
     """"""
-
-    class _DictParamEncoder(JSONEncoder):
-        """"""
-        JSON encoder for :py:class:`~DictParameter`, which makes :py:class:`~_FrozenOrderedDict` JSON serializable.
-        """"""
-        def default(self, obj):
-            if isinstance(obj, _FrozenOrderedDict):
-                return obj.get_wrapped()
-            return json.JSONEncoder.default(self, obj)
-
     def normalize(self, value):
         """"""
         Ensure that dictionary parameter is converted to a _FrozenOrderedDict so it can be hashed.
@@ -888,7 +888,7 @@ class DictParameter(Parameter):
         return json.loads(s, object_pairs_hook=_FrozenOrderedDict)
 
     def serialize(self, x):
-        return json.dumps(x, cls=DictParameter._DictParamEncoder)
+        return json.dumps(x, cls=_DictParamEncoder)
 
 
 class ListParameter(Parameter):
@@ -923,7 +923,7 @@ class ListParameter(Parameter):
     """"""
     def normalize(self, x):
         """"""
-        Ensure that list parameter is converted to a tuple so it can be hashed.
+        Ensure that struct is recursively converted to a tuple so it can be hashed.
 
         :param str x: the value to parse.
         :return: the normalized (hashable/immutable) value.
@@ -937,7 +937,7 @@ class ListParameter(Parameter):
         :param str x: the value to parse.
         :return: the parsed value.
         """"""
-        return list(json.loads(x))
+        return list(json.loads(x, object_pairs_hook=_FrozenOrderedDict))
 
     def serialize(self, x):
         """"""
@@ -947,10 +947,10 @@ class ListParameter(Parameter):
 
         :param x: the value to serialize.
         """"""
-        return json.dumps(x)
+        return json.dumps(x, cls=_DictParamEncoder)
 
 
-class TupleParameter(Parameter):
+class TupleParameter(ListParameter):
     """"""
     Parameter whose value is a ``tuple`` or ``tuple`` of tuples.
 
@@ -978,7 +978,6 @@ class TupleParameter(Parameter):
 
         $ luigi --module my_tasks MyTask --book_locations '((12,3),(4,15),(52,1))'
     """"""
-
     def parse(self, x):
         """"""
         Parse an individual value from the input.
@@ -999,20 +998,11 @@ class TupleParameter(Parameter):
         # Therefore, if json.loads(x) returns a ValueError, try ast.literal_eval(x).
         # ast.literal_eval(t_str) == t
         try:
-            return tuple(tuple(x) for x in json.loads(x))  # loop required to parse tuple of tuples
+            # loop required to parse tuple of tuples
+            return tuple(tuple(x) for x in json.loads(x, object_pairs_hook=_FrozenOrderedDict))
         except ValueError:
             return literal_eval(x)  # if this causes an error, let that error be raised.
 
-    def serialize(self, x):
-        """"""
-        Opposite of :py:meth:`parse`.
-
-        Converts the value ``x`` to a string.
-
-        :param x: the value to serialize.
-        """"""
-        return json.dumps(x)
-
 
 class NumericalParameter(Parameter):
     """"""
"
"luigi","33","a7c0662eab78fd226fd7ef6b4461d7199336cbb1","fccb631a14e1d52138d39f06004be14ca8f3337d","luigi/task.py","luigi/task.py","diff --git a/luigi/task.py b/luigi/task.py","test/parameter_test.py","","diff --git a/luigi/task.py b/luigi/task.py
index c5ddbeb6..67a786ae 100644
--- a/luigi/task.py
+++ b/luigi/task.py
@@ -331,7 +331,7 @@ class Task(object):
         exc_desc = '%s[args=%s, kwargs=%s]' % (task_name, args, kwargs)
 
         # Fill in the positional arguments
-        positional_params = [(n, p) for n, p in params if p.significant]
+        positional_params = [(n, p) for n, p in params if not p.is_global]
         for i, arg in enumerate(args):
             if i >= len(positional_params):
                 raise parameter.UnknownParameterException('%s: takes at most %d parameters (%d given)' % (exc_desc, len(positional_params), len(args)))
"
"luigi","17","c39922350cba3a93c96c2ed223283bf8cf315a7d","e38392a1381dd8daee0f180f0ac7f651edb88e0c","luigi/interface.py","luigi/interface.py","diff --git a/luigi/interface.py b/luigi/interface.py","test/scheduler_test.py","","diff --git a/luigi/interface.py b/luigi/interface.py
index 2a4de9f9..d978e41a 100644
--- a/luigi/interface.py
+++ b/luigi/interface.py
@@ -131,7 +131,7 @@ class core(task.Config):
 class _WorkerSchedulerFactory(object):
 
     def create_local_scheduler(self):
-        return scheduler.CentralPlannerScheduler(prune_on_get_work=True)
+        return scheduler.CentralPlannerScheduler(prune_on_get_work=True, record_task_history=False)
 
     def create_remote_scheduler(self, url):
         return rpc.RemoteScheduler(url)
"
"luigi","28","e37cb0ea1d97e6340840128a68c8d59bd05c28c3","e2be971226c34a193d7029c51206e488b6a037cd","luigi/contrib/hive.py","luigi/contrib/hive.py","diff --git a/luigi/contrib/hive.py b/luigi/contrib/hive.py","test/contrib/hive_test.py","","diff --git a/luigi/contrib/hive.py b/luigi/contrib/hive.py
index 3ccadd2e..02823924 100644
--- a/luigi/contrib/hive.py
+++ b/luigi/contrib/hive.py
@@ -138,7 +138,7 @@ class HiveCommandClient(HiveClient):
         if partition is None:
             stdout = run_hive_cmd('use {0}; show tables like ""{1}"";'.format(database, table))
 
-            return stdout and table in stdout
+            return stdout and table.lower() in stdout
         else:
             stdout = run_hive_cmd(""""""use %s; show partitions %s partition
                                 (%s)"""""" % (database, table, self.partition_spec(partition)))
"
"luigi","20","b958140c2ec838e590a5be02dbac7414d5d0bf17","c3d685e2b03369aab6f4d86ed1c95169c1c2c217","luigi/task.py","luigi/task.py","diff --git a/luigi/task.py b/luigi/task.py","test/task_test.py","","diff --git a/luigi/task.py b/luigi/task.py
index 396768d4..469e97a0 100644
--- a/luigi/task.py
+++ b/luigi/task.py
@@ -306,8 +306,7 @@ class Task(object):
         params_str = {}
         params = dict(self.get_params())
         for param_name, param_value in six.iteritems(self.param_kwargs):
-            if params[param_name].significant:
-                params_str[param_name] = params[param_name].serialize(param_value)
+            params_str[param_name] = params[param_name].serialize(param_value)
 
         return params_str
 
"
"luigi","27","69dfec33dc7c34d551ddc71742fa9c847295b01f","fa17292ebb54c8b83db8cf0995618a2a057103a6","luigi/parameter.py","luigi/parameter.py","diff --git a/luigi/parameter.py b/luigi/parameter.py","test/parameter_test.py","","diff --git a/luigi/parameter.py b/luigi/parameter.py
index 6d12f2b7..9b742d8f 100644
--- a/luigi/parameter.py
+++ b/luigi/parameter.py
@@ -279,7 +279,7 @@ class Parameter(object):
             return [str(v) for v in x]
         return str(x)
 
-    def parse_from_input(self, param_name, x):
+    def parse_from_input(self, param_name, x, task_name=None):
         """"""
         Parses the parameter value from input ``x``, handling defaults and is_list.
 
@@ -289,8 +289,8 @@ class Parameter(object):
         :raises MissingParameterException: if x is false-y and no default is specified.
         """"""
         if not x:
-            if self.has_value:
-                return self.value
+            if self.has_task_value(param_name=param_name, task_name=task_name):
+                return self.task_value(param_name=param_name, task_name=task_name)
             elif self.is_bool:
                 return False
             elif self.is_list:
@@ -333,8 +333,9 @@ class Parameter(object):
             description.append('for all instances of class %s' % task_name)
         elif self.description:
             description.append(self.description)
-        if self.has_value:
-            description.append("" [default: %s]"" % (self.value,))
+        if self.has_task_value(param_name=param_name, task_name=task_name):
+            value = self.task_value(param_name=param_name, task_name=task_name)
+            description.append("" [default: %s]"" % (value,))
 
         if self.is_list:
             action = ""append""
@@ -356,7 +357,7 @@ class Parameter(object):
         dest = self.parser_dest(param_name, task_name, glob=False)
         if dest is not None:
             value = getattr(args, dest, None)
-            params[param_name] = self.parse_from_input(param_name, value)
+            params[param_name] = self.parse_from_input(param_name, value, task_name=task_name)
 
     def set_global_from_args(self, param_name, task_name, args, is_without_section=False):
         # Note: side effects
@@ -364,7 +365,7 @@ class Parameter(object):
         if dest is not None:
             value = getattr(args, dest, None)
             if value:
-                self.set_global(self.parse_from_input(param_name, value))
+                self.set_global(self.parse_from_input(param_name, value, task_name=task_name))
             else:  # either False (bools) or None (everything else)
                 self.reset_global()
 
"
"luigi","12","c3119757c9ad4141fb446554109fa09cbd31173c","b3e9ad57f8502a390686957b69070105fddcfd49","luigi/contrib/hdfs/clients.py","luigi/contrib/hdfs/clients.py","diff --git a/luigi/contrib/hdfs/clients.py b/luigi/contrib/hdfs/clients.py","test/hdfs_client_test.py","","diff --git a/luigi/contrib/hdfs/clients.py b/luigi/contrib/hdfs/clients.py
index f7ece64d..2e8399ba 100644
--- a/luigi/contrib/hdfs/clients.py
+++ b/luigi/contrib/hdfs/clients.py
@@ -19,33 +19,42 @@
 The implementations of the hdfs clients. The hadoop cli client and the
 snakebite client.
 """"""
-
+import logging
+import threading
 
 from luigi.contrib.hdfs import config as hdfs_config
 from luigi.contrib.hdfs import snakebite_client as hdfs_snakebite_client
 from luigi.contrib.hdfs import webhdfs_client as hdfs_webhdfs_client
 from luigi.contrib.hdfs import hadoopcli_clients as hdfs_hadoopcli_clients
 import luigi.contrib.target
-import logging
 
 logger = logging.getLogger('luigi-interface')
 
+_AUTOCONFIG_CLIENT = threading.local()
+
 
-def get_autoconfig_client():
+def get_autoconfig_client(client_cache=_AUTOCONFIG_CLIENT):
     """"""
     Creates the client as specified in the `luigi.cfg` configuration.
     """"""
-    configured_client = hdfs_config.get_configured_hdfs_client()
-    if configured_client == ""webhdfs"":
-        return hdfs_webhdfs_client.WebHdfsClient()
-    if configured_client == ""snakebite"":
-        return hdfs_snakebite_client.SnakebiteHdfsClient()
-    if configured_client == ""snakebite_with_hadoopcli_fallback"":
-        return luigi.contrib.target.CascadingClient([hdfs_snakebite_client.SnakebiteHdfsClient(),
-                                                     hdfs_hadoopcli_clients.create_hadoopcli_client()])
-    if configured_client == ""hadoopcli"":
-        return hdfs_hadoopcli_clients.create_hadoopcli_client()
-    raise Exception(""Unknown hdfs client "" + configured_client)
+    try:
+        return client_cache.client
+    except AttributeError:
+        configured_client = hdfs_config.get_configured_hdfs_client()
+        if configured_client == ""webhdfs"":
+            client_cache.client = hdfs_webhdfs_client.WebHdfsClient()
+        elif configured_client == ""snakebite"":
+            client_cache.client = hdfs_snakebite_client.SnakebiteHdfsClient()
+        elif configured_client == ""snakebite_with_hadoopcli_fallback"":
+            client_cache.client = luigi.contrib.target.CascadingClient([
+                hdfs_snakebite_client.SnakebiteHdfsClient(),
+                hdfs_hadoopcli_clients.create_hadoopcli_client(),
+            ])
+        elif configured_client == ""hadoopcli"":
+            client_cache.client = hdfs_hadoopcli_clients.create_hadoopcli_client()
+        else:
+            raise Exception(""Unknown hdfs client "" + configured_client)
+        return client_cache.client
 
 
 def _with_ac(method_name):
"
"luigi","29","07efdd9136966ffe143ea57e15b1a57b5698fd5a","2898a2d2d178499435892d7a69bb1dfc90c70b29","luigi/task_register.py;test/cmdline_test.py","luigi/task_register.py;test/cmdline_test.py","diff --git a/luigi/task_register.py b/luigi/task_register.py;diff --git a/test/cmdline_test.py b/test/cmdline_test.py","test/task_test.py","","diff --git a/luigi/task_register.py b/luigi/task_register.py
index 85843024..15d18f70 100644
--- a/luigi/task_register.py
+++ b/luigi/task_register.py
@@ -135,8 +135,6 @@ class Register(abc.ABCMeta):
         # We return this in a topologically sorted list of inheritance: this is useful in some cases (#822)
         reg = OrderedDict()
         for cls in cls._reg:
-            if cls.run == NotImplemented:
-                continue
             name = cls.task_family
 
             if name in reg and reg[name] != cls and \
diff --git a/test/cmdline_test.py b/test/cmdline_test.py
index 64e56f52..229f365c 100644
--- a/test/cmdline_test.py
+++ b/test/cmdline_test.py
@@ -53,16 +53,6 @@ class AmbiguousClass(luigi.Task):
     pass
 
 
-class NonAmbiguousClass(luigi.ExternalTask):
-    pass
-
-
-class NonAmbiguousClass(luigi.Task):
-
-    def run(self):
-        NonAmbiguousClass.has_run = True
-
-
 class TaskWithSameName(luigi.Task):
 
     def run(self):
@@ -115,12 +105,6 @@ class CmdlineTest(unittest.TestCase):
     def test_cmdline_ambiguous_class(self, logger):
         self.assertRaises(Exception, luigi.run, ['--local-scheduler', '--no-lock', 'AmbiguousClass'])
 
-    @mock.patch(""logging.getLogger"")
-    @mock.patch(""warnings.warn"")
-    def test_cmdline_non_ambiguous_class(self, warn, logger):
-        luigi.run(['--local-scheduler', '--no-lock', 'NonAmbiguousClass'])
-        self.assertTrue(NonAmbiguousClass.has_run)
-
     @mock.patch(""logging.getLogger"")
     @mock.patch(""logging.StreamHandler"")
     def test_setup_interface_logging(self, handler, logger):
"
"luigi","4","ffa51b50103a3adaf3c4d0569fdb037a7ba01e8e","8501e5dbb8d3040453a89bb0d3562526086d51e5","luigi/contrib/redshift.py","luigi/contrib/redshift.py","diff --git a/luigi/contrib/redshift.py b/luigi/contrib/redshift.py","test/contrib/redshift_test.py","","diff --git a/luigi/contrib/redshift.py b/luigi/contrib/redshift.py
index 5901685c..0ab50dc6 100644
--- a/luigi/contrib/redshift.py
+++ b/luigi/contrib/redshift.py
@@ -353,7 +353,7 @@ class S3CopyToTable(rdbms.CopyToTable, _CredentialsMixin):
         """"""
         logger.info(""Inserting file: %s"", f)
         colnames = ''
-        if len(self.columns) > 0:
+        if self.columns and len(self.columns) > 0:
             colnames = "","".join([x[0] for x in self.columns])
             colnames = '({})'.format(colnames)
 
"
"luigi","26","ed351ca3c3baf3657de584db08dfe0414fa000a3","13673fd488c25325db633b1d49e664fb937fabc2","luigi/contrib/hadoop_jar.py","luigi/contrib/hadoop_jar.py","diff --git a/luigi/contrib/hadoop_jar.py b/luigi/contrib/hadoop_jar.py","test/contrib/hadoop_jar_test.py","","diff --git a/luigi/contrib/hadoop_jar.py b/luigi/contrib/hadoop_jar.py
index 1419c54b..3b352b2c 100644
--- a/luigi/contrib/hadoop_jar.py
+++ b/luigi/contrib/hadoop_jar.py
@@ -83,7 +83,9 @@ class HadoopJarJobRunner(luigi.contrib.hadoop.JobRunner):
             arglist.append('{}@{}'.format(username, host))
         else:
             arglist = []
-            if not job.jar() or not os.path.exists(job.jar()):
+            if not job.jar():
+                raise HadoopJarJobError(""Jar not defined"")
+            if not os.path.exists(job.jar()):
                 logger.error(""Can't find jar: %s, full path %s"", job.jar(), os.path.abspath(job.jar()))
                 raise HadoopJarJobError(""job jar does not exist"")
 
"
"luigi","5","45b5711d1900184ccd66c65216d28c0d69d10d4a","1fbec18ceb7c5de352e6d1df12960c61f09e67c2","luigi/util.py","luigi/util.py","diff --git a/luigi/util.py b/luigi/util.py","test/util_test.py","","diff --git a/luigi/util.py b/luigi/util.py
index 97feb8f5..f6be2002 100644
--- a/luigi/util.py
+++ b/luigi/util.py
@@ -274,18 +274,19 @@ class inherits(object):
         self.task_to_inherit = task_to_inherit
 
     def __call__(self, task_that_inherits):
+        # Get all parameter objects from the underlying task
         for param_name, param_obj in self.task_to_inherit.get_params():
+            # Check if the parameter exists in the inheriting task
             if not hasattr(task_that_inherits, param_name):
+                # If not, add it to the inheriting task
                 setattr(task_that_inherits, param_name, param_obj)
 
-        # Modify task_that_inherits by subclassing it and adding methods
-        @task._task_wraps(task_that_inherits)
-        class Wrapped(task_that_inherits):
+        # Modify task_that_inherits by adding methods
+        def clone_parent(_self, **args):
+            return _self.clone(cls=self.task_to_inherit, **args)
+        task_that_inherits.clone_parent = clone_parent
 
-            def clone_parent(_self, **args):
-                return _self.clone(cls=self.task_to_inherit, **args)
-
-        return Wrapped
+        return task_that_inherits
 
 
 class requires(object):
@@ -300,14 +301,12 @@ class requires(object):
     def __call__(self, task_that_requires):
         task_that_requires = self.inherit_decorator(task_that_requires)
 
-        # Modify task_that_requres by subclassing it and adding methods
-        @task._task_wraps(task_that_requires)
-        class Wrapped(task_that_requires):
+        # Modify task_that_requres by adding methods
+        def requires(_self):
+            return _self.clone_parent()
+        task_that_requires.requires = requires
 
-            def requires(_self):
-                return _self.clone_parent()
-
-        return Wrapped
+        return task_that_requires
 
 
 class copies(object):
"
"luigi","24","572fce617a3b8133983cdee2b2cc336a65af5abe","8a4f73296f237fcf8182c342e62c2cb201c717df","luigi/contrib/spark.py","luigi/contrib/spark.py","diff --git a/luigi/contrib/spark.py b/luigi/contrib/spark.py","test/contrib/spark_test.py","","diff --git a/luigi/contrib/spark.py b/luigi/contrib/spark.py
index c5c6afba..34c3e615 100644
--- a/luigi/contrib/spark.py
+++ b/luigi/contrib/spark.py
@@ -269,7 +269,7 @@ class SparkSubmitTask(luigi.Task):
         command = []
         if value and isinstance(value, dict):
             for prop, value in value.items():
-                command += [name, '""{0}={1}""'.format(prop, value)]
+                command += [name, '{0}={1}'.format(prop, value)]
         return command
 
     def _flag_arg(self, name, value):
"
"ansible","18","bb1256ca9aa4c22225dbeef0ef23a20fa9388b2f","056aac1e3012d0e65991b73f2478b844b7cbdfc8","lib/ansible/cli/galaxy.py","lib/ansible/cli/galaxy.py","diff --git a/lib/ansible/cli/galaxy.py b/lib/ansible/cli/galaxy.py","test/units/cli/test_galaxy.py","","diff --git a/lib/ansible/cli/galaxy.py b/lib/ansible/cli/galaxy.py
index 4ff114ec70..66c45a723c 100644
--- a/lib/ansible/cli/galaxy.py
+++ b/lib/ansible/cli/galaxy.py
@@ -55,7 +55,7 @@ class GalaxyCLI(CLI):
         ''' create an options parser for bin/ansible '''
 
         super(GalaxyCLI, self).init_parser(
-            desc=""Perform various Role related operations."",
+            desc=""Perform various Role and Collection related operations."",
         )
 
         # common
@@ -413,7 +413,7 @@ class GalaxyCLI(CLI):
         obj_name = context.CLIARGS['{0}_name'.format(galaxy_type)]
 
         inject_data = dict(
-            description='your description',
+            description='your {0} description'.format(galaxy_type),
             ansible_plugin_list_dir=get_versioned_doclink('plugins/plugins.html'),
         )
         if galaxy_type == 'role':
@@ -525,7 +525,7 @@ class GalaxyCLI(CLI):
                 if not os.path.exists(b_dir_path):
                     os.makedirs(b_dir_path)
 
-        display.display(""- %s was created successfully"" % obj_name)
+        display.display(""- %s %s was created successfully"" % (galaxy_type.title(), obj_name))
 
     def execute_info(self):
         """"""
"
"ansible","14","a1ab093ddbd32f1002cbf6d6f184c7d0041d890d","7acae62fa849481b2a5e2e2d56961c5e1dcea96c","lib/ansible/galaxy/api.py","lib/ansible/galaxy/api.py","diff --git a/lib/ansible/galaxy/api.py b/lib/ansible/galaxy/api.py","test/units/galaxy/test_api.py","","diff --git a/lib/ansible/galaxy/api.py b/lib/ansible/galaxy/api.py
index da53f2e70c..ed9be32af3 100644
--- a/lib/ansible/galaxy/api.py
+++ b/lib/ansible/galaxy/api.py
@@ -21,6 +21,12 @@ from ansible.module_utils.urls import open_url
 from ansible.utils.display import Display
 from ansible.utils.hashing import secure_hash_s
 
+try:
+    from urllib.parse import urlparse
+except ImportError:
+    # Python 2
+    from urlparse import urlparse
+
 display = Display()
 
 
@@ -289,14 +295,20 @@ class GalaxyAPI:
             data = self._call_galaxy(url)
             results = data['results']
             done = (data.get('next_link', None) is None)
+
+            # https://github.com/ansible/ansible/issues/64355
+            # api_server contains part of the API path but next_link includes the the /api part so strip it out.
+            url_info = urlparse(self.api_server)
+            base_url = ""%s://%s/"" % (url_info.scheme, url_info.netloc)
+
             while not done:
-                url = _urljoin(self.api_server, data['next_link'])
+                url = _urljoin(base_url, data['next_link'])
                 data = self._call_galaxy(url)
                 results += data['results']
                 done = (data.get('next_link', None) is None)
         except Exception as e:
-            display.vvvv(""Unable to retrive role (id=%s) data (%s), but this is not fatal so we continue: %s""
-                         % (role_id, related, to_text(e)))
+            display.warning(""Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s""
+                            % (role_id, related, to_text(e)))
         return results
 
     @g_connect(['v1'])
"
"ansible","3","70219df9056ffb1e2766f572fbe71f7a1800c9f5","9d48884e36fb4fd9551f000b87d264383de74e75","lib/ansible/module_utils/facts/system/distribution.py","lib/ansible/module_utils/facts/system/distribution.py","diff --git a/lib/ansible/module_utils/facts/system/distribution.py b/lib/ansible/module_utils/facts/system/distribution.py","test/units/module_utils/test_distribution_version.py","","diff --git a/lib/ansible/module_utils/facts/system/distribution.py b/lib/ansible/module_utils/facts/system/distribution.py
index 606dbc19bd..528b0e2b9e 100644
--- a/lib/ansible/module_utils/facts/system/distribution.py
+++ b/lib/ansible/module_utils/facts/system/distribution.py
@@ -320,7 +320,8 @@ class DistributionFiles:
         elif 'SteamOS' in data:
             debian_facts['distribution'] = 'SteamOS'
             # nothing else to do, SteamOS gets correct info from python functions
-        elif path == '/etc/lsb-release' and 'Kali' in data:
+        elif path in ('/etc/lsb-release', '/etc/os-release') and 'Kali' in data:
+            # Kali does not provide /etc/lsb-release anymore
             debian_facts['distribution'] = 'Kali'
             release = re.search('DISTRIB_RELEASE=(.*)', data)
             if release:
"
"ansible","16","2a9964ede8b2b77a62a005f6f5abc964b2819b0e","93d9d640380252084855885ad27873b4377898ec","lib/ansible/module_utils/facts/hardware/linux.py;test/units/module_utils/facts/hardware/linux_data.py","lib/ansible/module_utils/facts/hardware/linux.py;test/units/module_utils/facts/hardware/linux_data.py","diff --git a/lib/ansible/module_utils/facts/hardware/linux.py b/lib/ansible/module_utils/facts/hardware/linux.py;diff --git a/test/units/module_utils/facts/hardware/linux_data.py b/test/units/module_utils/facts/hardware/linux_data.py","test/units/module_utils/facts/hardware/test_linux_get_cpu_info.py","","diff --git a/lib/ansible/module_utils/facts/hardware/linux.py b/lib/ansible/module_utils/facts/hardware/linux.py
index befc2fb5e7..503a6e3b73 100644
--- a/lib/ansible/module_utils/facts/hardware/linux.py
+++ b/lib/ansible/module_utils/facts/hardware/linux.py
@@ -242,8 +242,9 @@ class LinuxHardware(Hardware):
 
         # The fields for ARM CPUs do not always include 'vendor_id' or 'model name',
         # and sometimes includes both 'processor' and 'Processor'.
-        # Always use 'processor' count for ARM systems
-        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch')):
+        # The fields for Power CPUs include 'processor' and 'cpu'.
+        # Always use 'processor' count for ARM and Power systems
+        if collected_facts.get('ansible_architecture', '').startswith(('armv', 'aarch', 'ppc')):
             i = processor_occurence
 
         # FIXME
diff --git a/test/units/module_utils/facts/hardware/linux_data.py b/test/units/module_utils/facts/hardware/linux_data.py
index ba2e528d7a..05dc0e6513 100644
--- a/test/units/module_utils/facts/hardware/linux_data.py
+++ b/test/units/module_utils/facts/hardware/linux_data.py
@@ -495,9 +495,9 @@ CPU_INFO_TEST_SCENARIOS = [
                 '7', 'POWER7 (architected), altivec supported'
             ],
             'processor_cores': 1,
-            'processor_count': 16,
+            'processor_count': 8,
             'processor_threads_per_core': 1,
-            'processor_vcpus': 16
+            'processor_vcpus': 8
         },
     },
     {
@@ -531,9 +531,9 @@ CPU_INFO_TEST_SCENARIOS = [
                 '23', 'POWER8 (architected), altivec supported',
             ],
             'processor_cores': 1,
-            'processor_count': 48,
+            'processor_count': 24,
             'processor_threads_per_core': 1,
-            'processor_vcpus': 48
+            'processor_vcpus': 24
         },
     },
     {
"
"ansible","2","de59b17c7f69d5cfb72479b71776cc8b97e29a6b","5b9418c06ca6d51507468124250bb58046886be6","lib/ansible/utils/version.py","lib/ansible/utils/version.py","diff --git a/lib/ansible/utils/version.py b/lib/ansible/utils/version.py","test/units/utils/test_version.py","","diff --git a/lib/ansible/utils/version.py b/lib/ansible/utils/version.py
index 0dc6687ed9..d69723b473 100644
--- a/lib/ansible/utils/version.py
+++ b/lib/ansible/utils/version.py
@@ -72,14 +72,14 @@ class _Alpha:
 
         raise ValueError
 
-    def __gt__(self, other):
-        return not self.__lt__(other)
-
     def __le__(self, other):
         return self.__lt__(other) or self.__eq__(other)
 
+    def __gt__(self, other):
+        return not self.__le__(other)
+
     def __ge__(self, other):
-        return self.__gt__(other) or self.__eq__(other)
+        return not self.__lt__(other)
 
 
 class _Numeric:
@@ -115,14 +115,14 @@ class _Numeric:
 
         raise ValueError
 
-    def __gt__(self, other):
-        return not self.__lt__(other)
-
     def __le__(self, other):
         return self.__lt__(other) or self.__eq__(other)
 
+    def __gt__(self, other):
+        return not self.__le__(other)
+
     def __ge__(self, other):
-        return self.__gt__(other) or self.__eq__(other)
+        return not self.__lt__(other)
 
 
 class SemanticVersion(Version):
"
"ansible","8","81378b3e744cd0d13b33d18a4f8a38aeb8a6e97a","fc7980af9a42676913b4054163570ee438b82e9c","lib/ansible/plugins/shell/powershell.py","lib/ansible/plugins/shell/powershell.py","diff --git a/lib/ansible/plugins/shell/powershell.py b/lib/ansible/plugins/shell/powershell.py","test/units/plugins/shell/test_powershell.py","","diff --git a/lib/ansible/plugins/shell/powershell.py b/lib/ansible/plugins/shell/powershell.py
index ee23147cc5..ca2d5ebf5b 100644
--- a/lib/ansible/plugins/shell/powershell.py
+++ b/lib/ansible/plugins/shell/powershell.py
@@ -22,6 +22,7 @@ import re
 import shlex
 import pkgutil
 import xml.etree.ElementTree as ET
+import ntpath
 
 from ansible.errors import AnsibleError
 from ansible.module_utils._text import to_bytes, to_text
@@ -93,14 +94,13 @@ class ShellModule(ShellBase):
         return """"
 
     def join_path(self, *args):
-        parts = []
-        for arg in args:
-            arg = self._unquote(arg).replace('/', '\\')
-            parts.extend([a for a in arg.split('\\') if a])
-        path = '\\'.join(parts)
-        if path.startswith('~'):
-            return path
-        return path
+        # use normpath() to remove doubled slashed and convert forward to backslashes
+        parts = [ntpath.normpath(self._unquote(arg)) for arg in args]
+
+        # Becuase ntpath.join treats any component that begins with a backslash as an absolute path,
+        # we have to strip slashes from at least the beginning, otherwise join will ignore all previous
+        # path components except for the drive.
+        return ntpath.join(parts[0], *[part.strip('\\') for part in parts[1:]])
 
     def get_remote_filename(self, pathname):
         # powershell requires that script files end with .ps1
"
"ansible","1","25c5388fdec9e56517a93feb5e8d485680946c25","343ffaa18b63c92e182b16c3ad84b8d81ca4df69","lib/ansible/galaxy/collection.py","lib/ansible/galaxy/collection.py","diff --git a/lib/ansible/galaxy/collection.py b/lib/ansible/galaxy/collection.py","test/units/galaxy/test_collection.py","","diff --git a/lib/ansible/galaxy/collection.py b/lib/ansible/galaxy/collection.py
index a055b08e71..856e54666f 100644
--- a/lib/ansible/galaxy/collection.py
+++ b/lib/ansible/galaxy/collection.py
@@ -668,6 +668,11 @@ def verify_collections(collections, search_paths, apis, validate_certs, ignore_e
                     for search_path in search_paths:
                         b_search_path = to_bytes(os.path.join(search_path, namespace, name), errors='surrogate_or_strict')
                         if os.path.isdir(b_search_path):
+                            if not os.path.isfile(os.path.join(to_text(b_search_path, errors='surrogate_or_strict'), 'MANIFEST.json')):
+                                raise AnsibleError(
+                                    message=""Collection %s does not appear to have a MANIFEST.json. "" % collection_name +
+                                            ""A MANIFEST.json is expected if the collection has been built and installed via ansible-galaxy.""
+                                )
                             local_collection = CollectionRequirement.from_path(b_search_path, False)
                             break
                     if local_collection is None:
"
"ansible","7","cd146b836e032df785ecd9eb711c6ef23c2228b8","4ec1437212b2fb3c313e44ed5a76b105f2151622","lib/ansible/module_utils/network/eos/config/vlans/vlans.py","lib/ansible/module_utils/network/eos/config/vlans/vlans.py","diff --git a/lib/ansible/module_utils/network/eos/config/vlans/vlans.py b/lib/ansible/module_utils/network/eos/config/vlans/vlans.py","test/units/modules/network/eos/test_eos_vlans.py","","diff --git a/lib/ansible/module_utils/network/eos/config/vlans/vlans.py b/lib/ansible/module_utils/network/eos/config/vlans/vlans.py
index 99cb37cd07..c2a701637d 100644
--- a/lib/ansible/module_utils/network/eos/config/vlans/vlans.py
+++ b/lib/ansible/module_utils/network/eos/config/vlans/vlans.py
@@ -208,16 +208,17 @@ def generate_commands(vlan_id, to_set, to_remove):
     if ""vlan_id"" in to_remove:
         return [""no vlan {0}"".format(vlan_id)]
 
+    for key in to_remove:
+        if key in to_set.keys():
+            continue
+        commands.append(""no {0}"".format(key))
+
     for key, value in to_set.items():
         if key == ""vlan_id"" or value is None:
             continue
 
         commands.append(""{0} {1}"".format(key, value))
 
-    for key in to_remove:
-        commands.append(""no {0}"".format(key))
-
     if commands:
         commands.insert(0, ""vlan {0}"".format(vlan_id))
-
     return commands
"
"ansible","10","e368f788f71c338cd3f049d5d6bdc643a51c0514","a4b59d021368285490f7cda50c11ac4f7a8030b5","lib/ansible/modules/system/pamd.py","lib/ansible/modules/system/pamd.py","diff --git a/lib/ansible/modules/system/pamd.py b/lib/ansible/modules/system/pamd.py","test/units/modules/system/test_pamd.py","","diff --git a/lib/ansible/modules/system/pamd.py b/lib/ansible/modules/system/pamd.py
index 0d8e32b5ae..50da1fcf9e 100644
--- a/lib/ansible/modules/system/pamd.py
+++ b/lib/ansible/modules/system/pamd.py
@@ -351,6 +351,8 @@ class PamdRule(PamdLine):
     valid_control_actions = ['ignore', 'bad', 'die', 'ok', 'done', 'reset']
 
     def __init__(self, rule_type, rule_control, rule_path, rule_args=None):
+        self.prev = None
+        self.next = None
         self._control = None
         self._args = None
         self.rule_type = rule_type
@@ -478,7 +480,8 @@ class PamdService(object):
             if current_line.matches(rule_type, rule_control, rule_path):
                 if current_line.prev is not None:
                     current_line.prev.next = current_line.next
-                    current_line.next.prev = current_line.prev
+                    if current_line.next is not None:
+                        current_line.next.prev = current_line.prev
                 else:
                     self._head = current_line.next
                     current_line.next.prev = None
"
"ansible","13","41472ee3878be215af8b933b2b04b4a72b9165ca","694ef5660d45fcb97c9beea5b2750f6eadcf5e93","lib/ansible/cli/galaxy.py;lib/ansible/galaxy/collection.py","lib/ansible/cli/galaxy.py;lib/ansible/galaxy/collection.py","diff --git a/lib/ansible/cli/galaxy.py b/lib/ansible/cli/galaxy.py;diff --git a/lib/ansible/galaxy/collection.py b/lib/ansible/galaxy/collection.py","test/units/cli/test_galaxy.py","","diff --git a/lib/ansible/cli/galaxy.py b/lib/ansible/cli/galaxy.py
index 487f21d0db..9cf3a8e87e 100644
--- a/lib/ansible/cli/galaxy.py
+++ b/lib/ansible/cli/galaxy.py
@@ -29,12 +29,14 @@ from ansible.galaxy.role import GalaxyRole
 from ansible.galaxy.token import BasicAuthToken, GalaxyToken, KeycloakToken, NoTokenSentinel
 from ansible.module_utils.ansible_release import __version__ as ansible_version
 from ansible.module_utils._text import to_bytes, to_native, to_text
+from ansible.module_utils import six
 from ansible.parsing.yaml.loader import AnsibleLoader
 from ansible.playbook.role.requirement import RoleRequirement
 from ansible.utils.display import Display
 from ansible.utils.plugin_docs import get_versioned_doclink
 
 display = Display()
+urlparse = six.moves.urllib.parse.urlparse
 
 
 class GalaxyCLI(CLI):
@@ -805,7 +807,13 @@ class GalaxyCLI(CLI):
             else:
                 requirements = []
                 for collection_input in collections:
-                    name, dummy, requirement = collection_input.partition(':')
+                    requirement = None
+                    if os.path.isfile(to_bytes(collection_input, errors='surrogate_or_strict')) or \
+                            urlparse(collection_input).scheme.lower() in ['http', 'https']:
+                        # Arg is a file path or URL to a collection
+                        name = collection_input
+                    else:
+                        name, dummy, requirement = collection_input.partition(':')
                     requirements.append((name, requirement or '*', None))
 
             output_path = GalaxyCLI._resolve_path(output_path)
diff --git a/lib/ansible/galaxy/collection.py b/lib/ansible/galaxy/collection.py
index 0569605a3d..ede9492251 100644
--- a/lib/ansible/galaxy/collection.py
+++ b/lib/ansible/galaxy/collection.py
@@ -827,9 +827,13 @@ def _get_collection_info(dep_map, existing_collections, collection, requirement,
     if os.path.isfile(to_bytes(collection, errors='surrogate_or_strict')):
         display.vvvv(""Collection requirement '%s' is a tar artifact"" % to_text(collection))
         b_tar_path = to_bytes(collection, errors='surrogate_or_strict')
-    elif urlparse(collection).scheme:
+    elif urlparse(collection).scheme.lower() in ['http', 'https']:
         display.vvvv(""Collection requirement '%s' is a URL to a tar artifact"" % collection)
-        b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)
+        try:
+            b_tar_path = _download_file(collection, b_temp_path, None, validate_certs)
+        except urllib_error.URLError as err:
+            raise AnsibleError(""Failed to download collection tar from '%s': %s""
+                               % (to_native(collection), to_native(err)))
 
     if b_tar_path:
         req = CollectionRequirement.from_tar(b_tar_path, force, parent=parent)
"
"ansible","9","9276dd2007a73172ed99ab5a56cded4298d3cd2b","6f1bb37feb81acd99157f5ba0933fecd747015a2","lib/ansible/modules/packaging/os/redhat_subscription.py","lib/ansible/modules/packaging/os/redhat_subscription.py","diff --git a/lib/ansible/modules/packaging/os/redhat_subscription.py b/lib/ansible/modules/packaging/os/redhat_subscription.py","test/units/modules/packaging/os/test_redhat_subscription.py","","diff --git a/lib/ansible/modules/packaging/os/redhat_subscription.py b/lib/ansible/modules/packaging/os/redhat_subscription.py
index bc0bcc3ee7..d3cca7beb0 100644
--- a/lib/ansible/modules/packaging/os/redhat_subscription.py
+++ b/lib/ansible/modules/packaging/os/redhat_subscription.py
@@ -515,7 +515,9 @@ class Rhsm(RegistrationBase):
 
         for pool_id, quantity in sorted(pool_ids.items()):
             if pool_id in available_pool_ids:
-                args = [SUBMAN_CMD, 'attach', '--pool', pool_id, '--quantity', quantity]
+                args = [SUBMAN_CMD, 'attach', '--pool', pool_id]
+                if quantity is not None:
+                    args.extend(['--quantity', to_native(quantity)])
                 rc, stderr, stdout = self.module.run_command(args, check_rc=True)
             else:
                 self.module.fail_json(msg='Pool ID: %s not in list of available pools' % pool_id)
@@ -839,8 +841,8 @@ def main():
                 module.fail_json(msg='Unable to parse pool_ids option.')
             pool_id, quantity = list(value.items())[0]
         else:
-            pool_id, quantity = value, 1
-        pool_ids[pool_id] = str(quantity)
+            pool_id, quantity = value, None
+        pool_ids[pool_id] = quantity
     consumer_type = module.params[""consumer_type""]
     consumer_name = module.params[""consumer_name""]
     consumer_id = module.params[""consumer_id""]
"
"ansible","15","b1e8a6c1cbd2a668b462995487b819ef7dd8ba4b","68de182555b185737353e780882159a3d213908c","lib/ansible/modules/network/eos/eos_eapi.py","lib/ansible/modules/network/eos/eos_eapi.py","diff --git a/lib/ansible/modules/network/eos/eos_eapi.py b/lib/ansible/modules/network/eos/eos_eapi.py","test/units/modules/network/eos/test_eos_eapi.py","","diff --git a/lib/ansible/modules/network/eos/eos_eapi.py b/lib/ansible/modules/network/eos/eos_eapi.py
index 07eeaf93eb..d3825a30df 100644
--- a/lib/ansible/modules/network/eos/eos_eapi.py
+++ b/lib/ansible/modules/network/eos/eos_eapi.py
@@ -264,7 +264,7 @@ def map_obj_to_commands(updates, module, warnings):
         else:
             add('protocol unix-socket')
 
-    if needs_update('state') and not needs_update('vrf'):
+    if needs_update('state'):
         if want['state'] == 'stopped':
             add('shutdown')
         elif want['state'] == 'started':
"
"ansible","11","da07b98b7a433493728ddb7ac7efbd20b8988776","52f3ce8a808f943561803bd664e695fed1841fe8","lib/ansible/modules/network/ios/ios_banner.py","lib/ansible/modules/network/ios/ios_banner.py","diff --git a/lib/ansible/modules/network/ios/ios_banner.py b/lib/ansible/modules/network/ios/ios_banner.py","test/units/modules/network/ios/test_ios_banner.py","","diff --git a/lib/ansible/modules/network/ios/ios_banner.py b/lib/ansible/modules/network/ios/ios_banner.py
index be85781058..c28838407c 100644
--- a/lib/ansible/modules/network/ios/ios_banner.py
+++ b/lib/ansible/modules/network/ios/ios_banner.py
@@ -89,10 +89,9 @@ commands:
     - string
 """"""
 from ansible.module_utils.basic import AnsibleModule
-from ansible.module_utils.connection import exec_command
-from ansible.module_utils.network.ios.ios import load_config
+from ansible.module_utils.network.ios.ios import get_config, load_config
 from ansible.module_utils.network.ios.ios import ios_argument_spec
-import re
+from re import search, M
 
 
 def map_obj_to_commands(updates, module):
@@ -107,7 +106,7 @@ def map_obj_to_commands(updates, module):
         if want['text'] and (want['text'] != have.get('text')):
             banner_cmd = 'banner %s' % module.params['banner']
             banner_cmd += ' @\n'
-            banner_cmd += want['text'].strip()
+            banner_cmd += want['text'].strip('\n')
             banner_cmd += '\n@'
             commands.append(banner_cmd)
 
@@ -115,17 +114,21 @@ def map_obj_to_commands(updates, module):
 
 
 def map_config_to_obj(module):
-    rc, out, err = exec_command(module, 'show banner %s' % module.params['banner'])
-    if rc == 0:
-        output = out
-    else:
-        rc, out, err = exec_command(module,
-                                    'show running-config | begin banner %s'
-                                    % module.params['banner'])
-        if out:
-            output = re.search(r'\^C(.*?)\^C', out, re.S).group(1).strip()
+    """"""
+    This function gets the banner config without stripping any whitespaces,
+    and then fetches the required banner from it.
+    :param module:
+    :return: banner config dict object.
+    """"""
+    out = get_config(module, flags='| begin banner %s' % module.params['banner'])
+    if out:
+        regex = 'banner ' + module.params['banner'] + ' ^C\n'
+        if search('banner ' + module.params['banner'], out, M):
+            output = str((out.split(regex))[1].split(""^C\n"")[0])
         else:
             output = None
+    else:
+        output = None
     obj = {'banner': module.params['banner'], 'state': 'absent'}
     if output:
         obj['text'] = output
@@ -135,9 +138,6 @@ def map_config_to_obj(module):
 
 def map_params_to_obj(module):
     text = module.params['text']
-    if text:
-        text = str(text).strip()
-
     return {
         'banner': module.params['banner'],
         'text': text,
"
"ansible","6","90898132e456ee1993db99a1531379f1b98ee915","4881af2e7e0506ada0225fd764e874e20569d5b2","lib/ansible/galaxy/collection.py","lib/ansible/galaxy/collection.py","diff --git a/lib/ansible/galaxy/collection.py b/lib/ansible/galaxy/collection.py","test/units/galaxy/test_collection_install.py","","diff --git a/lib/ansible/galaxy/collection.py b/lib/ansible/galaxy/collection.py
index 24b13d97a1..4d7b327613 100644
--- a/lib/ansible/galaxy/collection.py
+++ b/lib/ansible/galaxy/collection.py
@@ -219,12 +219,15 @@ class CollectionRequirement:
                 requirement = req
                 op = operator.eq
 
-                # In the case we are checking a new requirement on a base requirement (parent != None) we can't accept
-                # version as '*' (unknown version) unless the requirement is also '*'.
-                if parent and version == '*' and requirement != '*':
-                    break
-                elif requirement == '*' or version == '*':
-                    continue
+            # In the case we are checking a new requirement on a base requirement (parent != None) we can't accept
+            # version as '*' (unknown version) unless the requirement is also '*'.
+            if parent and version == '*' and requirement != '*':
+                display.warning(""Failed to validate the collection requirement '%s:%s' for %s when the existing ""
+                                ""install does not have a version set, the collection may not work.""
+                                % (to_text(self), req, parent))
+                continue
+            elif requirement == '*' or version == '*':
+                continue
 
             if not op(LooseVersion(version), LooseVersion(requirement)):
                 break
@@ -286,7 +289,13 @@ class CollectionRequirement:
             manifest = info['manifest_file']['collection_info']
             namespace = manifest['namespace']
             name = manifest['name']
-            version = manifest['version']
+            version = to_text(manifest['version'], errors='surrogate_or_strict')
+
+            if not hasattr(LooseVersion(version), 'version'):
+                display.warning(""Collection at '%s' does not have a valid version set, falling back to '*'. Found ""
+                                ""version: '%s'"" % (to_text(b_path), version))
+                version = '*'
+
             dependencies = manifest['dependencies']
         else:
             display.warning(""Collection at '%s' does not have a MANIFEST.json file, cannot detect version.""
@@ -877,7 +886,7 @@ def _get_collection_info(dep_map, existing_collections, collection, requirement,
     existing = [c for c in existing_collections if to_text(c) == to_text(collection_info)]
     if existing and not collection_info.force:
         # Test that the installed collection fits the requirement
-        existing[0].add_requirement(to_text(collection_info), requirement)
+        existing[0].add_requirement(parent, requirement)
         collection_info = existing[0]
 
     dep_map[to_text(collection_info)] = collection_info
"
"ansible","17","9cb47832d15c61884b30d70f9d4e0f816b064b05","b38cb37728df76e0529243bdce694b18ca0e1163","lib/ansible/module_utils/facts/hardware/linux.py","lib/ansible/module_utils/facts/hardware/linux.py","diff --git a/lib/ansible/module_utils/facts/hardware/linux.py b/lib/ansible/module_utils/facts/hardware/linux.py","test/units/module_utils/facts/test_facts.py","","diff --git a/lib/ansible/module_utils/facts/hardware/linux.py b/lib/ansible/module_utils/facts/hardware/linux.py
index 19ca6e4799..befc2fb5e7 100644
--- a/lib/ansible/module_utils/facts/hardware/linux.py
+++ b/lib/ansible/module_utils/facts/hardware/linux.py
@@ -79,6 +79,9 @@ class LinuxHardware(Hardware):
     # regex used against mtab content to find entries that are bind mounts
     MTAB_BIND_MOUNT_RE = re.compile(r'.*bind.*""')
 
+    # regex used for replacing octal escape sequences
+    OCTAL_ESCAPE_RE = re.compile(r'\\[0-9]{3}')
+
     def populate(self, collected_facts=None):
         hardware_facts = {}
         self.module.run_command_environ_update = {'LANG': 'C', 'LC_ALL': 'C', 'LC_NUMERIC': 'C'}
@@ -460,6 +463,14 @@ class LinuxHardware(Hardware):
             mtab_entries.append(fields)
         return mtab_entries
 
+    @staticmethod
+    def _replace_octal_escapes_helper(match):
+        # Convert to integer using base8 and then convert to character
+        return chr(int(match.group()[1:], 8))
+
+    def _replace_octal_escapes(self, value):
+        return self.OCTAL_ESCAPE_RE.sub(self._replace_octal_escapes_helper, value)
+
     def get_mount_info(self, mount, device, uuids):
 
         mount_size = get_mount_size(mount)
@@ -485,6 +496,8 @@ class LinuxHardware(Hardware):
         pool = ThreadPool(processes=min(len(mtab_entries), cpu_count()))
         maxtime = globals().get('GATHER_TIMEOUT') or timeout.DEFAULT_GATHER_TIMEOUT
         for fields in mtab_entries:
+            # Transform octal escape sequences
+            fields = [self._replace_octal_escapes(field) for field in fields]
 
             device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]
 
"
"ansible","12","05e2e1806162393d76542a75c2520c7d61c2d855","2fa8f9cfd80daf32c7d222190edf7cfc7234582a","lib/ansible/plugins/lookup/env.py","lib/ansible/plugins/lookup/env.py","diff --git a/lib/ansible/plugins/lookup/env.py b/lib/ansible/plugins/lookup/env.py","test/units/plugins/lookup/test_env.py","","diff --git a/lib/ansible/plugins/lookup/env.py b/lib/ansible/plugins/lookup/env.py
index 6892feb8c5..5926bfeea4 100644
--- a/lib/ansible/plugins/lookup/env.py
+++ b/lib/ansible/plugins/lookup/env.py
@@ -27,9 +27,8 @@ RETURN = """"""
       - values from the environment variables.
     type: list
 """"""
-import os
-
 from ansible.plugins.lookup import LookupBase
+from ansible.utils import py3compat
 
 
 class LookupModule(LookupBase):
@@ -39,6 +38,6 @@ class LookupModule(LookupBase):
         ret = []
         for term in terms:
             var = term.split()[0]
-            ret.append(os.getenv(var, ''))
+            ret.append(py3compat.environ.get(var, ''))
 
         return ret
"
"ansible","4","d91658ec0c8434c82c3ef98bfe9eb4e1027a43a3","18a66e291dad71128a32d662aa808213acefe0e9","lib/ansible/playbook/collectionsearch.py","lib/ansible/playbook/collectionsearch.py","diff --git a/lib/ansible/playbook/collectionsearch.py b/lib/ansible/playbook/collectionsearch.py","test/units/playbook/test_collectionsearch.py","","diff --git a/lib/ansible/playbook/collectionsearch.py b/lib/ansible/playbook/collectionsearch.py
index 93b80a8665..994e2e13e4 100644
--- a/lib/ansible/playbook/collectionsearch.py
+++ b/lib/ansible/playbook/collectionsearch.py
@@ -7,6 +7,10 @@ __metaclass__ = type
 from ansible.module_utils.six import string_types
 from ansible.playbook.attribute import FieldAttribute
 from ansible.utils.collection_loader import AnsibleCollectionLoader
+from ansible.template import is_template, Environment
+from ansible.utils.display import Display
+
+display = Display()
 
 
 def _ensure_default_collection(collection_list=None):
@@ -32,7 +36,8 @@ def _ensure_default_collection(collection_list=None):
 class CollectionSearch:
 
     # this needs to be populated before we can resolve tasks/roles/etc
-    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection)
+    _collections = FieldAttribute(isa='list', listof=string_types, priority=100, default=_ensure_default_collection,
+                                  always_post_validate=True, static=True)
 
     def _load_collections(self, attr, ds):
         # this will only be called if someone specified a value; call the shared value
@@ -41,4 +46,13 @@ class CollectionSearch:
         if not ds:  # don't return an empty collection list, just return None
             return None
 
+        # This duplicates static attr checking logic from post_validate()
+        # because if the user attempts to template a collection name, it will
+        # error before it ever gets to the post_validate() warning.
+        env = Environment()
+        for collection_name in ds:
+            if is_template(collection_name, env):
+                display.warning('""collections"" is not templatable, but we found: %s, '
+                                'it will not be templated and will be used ""as is"".' % (collection_name))
+
         return ds
"
"ansible","5","2af76f16be8cf2239daaec4c2f31c3dcb4e3469e","3c3ffc09c203d1b2262f6a319cceadd727749761","lib/ansible/module_utils/common/validation.py;test/units/module_utils/common/validation/test_check_mutually_exclusive.py","lib/ansible/module_utils/common/validation.py;test/units/module_utils/common/validation/test_check_mutually_exclusive.py","diff --git a/lib/ansible/module_utils/common/validation.py b/lib/ansible/module_utils/common/validation.py;diff --git a/test/units/module_utils/common/validation/test_check_mutually_exclusive.py b/test/units/module_utils/common/validation/test_check_mutually_exclusive.py","test/units/module_utils/common/validation/test_check_required_arguments.py","","diff --git a/lib/ansible/module_utils/common/validation.py b/lib/ansible/module_utils/common/validation.py
index 4c29c8b234..fc13f4d0aa 100644
--- a/lib/ansible/module_utils/common/validation.py
+++ b/lib/ansible/module_utils/common/validation.py
@@ -189,7 +189,7 @@ def check_required_arguments(argument_spec, module_parameters):
             missing.append(k)
 
     if missing:
-        msg = ""missing required arguments: %s"" % "", "".join(missing)
+        msg = ""missing required arguments: %s"" % "", "".join(sorted(missing))
         raise TypeError(to_native(msg))
 
     return missing
diff --git a/test/units/module_utils/common/validation/test_check_mutually_exclusive.py b/test/units/module_utils/common/validation/test_check_mutually_exclusive.py
index 5d44f85151..7bf90760b1 100644
--- a/test/units/module_utils/common/validation/test_check_mutually_exclusive.py
+++ b/test/units/module_utils/common/validation/test_check_mutually_exclusive.py
@@ -34,11 +34,12 @@ def test_check_mutually_exclusive_found(mutually_exclusive_terms):
         'fox': 'red',
         'socks': 'blue',
     }
-    expected = ""TypeError('parameters are mutually exclusive: string1|string2, box|fox|socks',)""
+    expected = ""parameters are mutually exclusive: string1|string2, box|fox|socks""
 
     with pytest.raises(TypeError) as e:
         check_mutually_exclusive(mutually_exclusive_terms, params)
-        assert e.value == expected
+
+    assert to_native(e.value) == expected
 
 
 def test_check_mutually_exclusive_none():
@@ -53,4 +54,4 @@ def test_check_mutually_exclusive_none():
 def test_check_mutually_exclusive_no_params(mutually_exclusive_terms):
     with pytest.raises(TypeError) as te:
         check_mutually_exclusive(mutually_exclusive_terms, None)
-        assert ""TypeError: 'NoneType' object is not iterable"" in to_native(te.error)
+    assert ""'NoneType' object is not iterable"" in to_native(te.value)
"
"spacy","3","dac70f29eb3b1f21ae9e2c6346666bf6a46307b6","663333c3b2bad90915d1a48a626ca1275b7ef077","bin/wiki_entity_linking/wikipedia_processor.py","bin/wiki_entity_linking/wikipedia_processor.py","diff --git a/bin/wiki_entity_linking/wikipedia_processor.py b/bin/wiki_entity_linking/wikipedia_processor.py","spacy/tests/regression/test_issue5314.py","","diff --git a/bin/wiki_entity_linking/wikipedia_processor.py b/bin/wiki_entity_linking/wikipedia_processor.py
index ed3c35c43..649d48fe5 100644
--- a/bin/wiki_entity_linking/wikipedia_processor.py
+++ b/bin/wiki_entity_linking/wikipedia_processor.py
@@ -30,7 +30,8 @@ logger = logging.getLogger(__name__)
 
 title_regex = re.compile(r""(?<=<title>).*(?=</title>)"")
 id_regex = re.compile(r""(?<=<id>)\d*(?=</id>)"")
-text_regex = re.compile(r""(?<=<text xml:space=\""preserve\"">).*(?=</text)"")
+text_tag_regex = re.compile(r""(?<=<text).*?(?=>)"")
+text_regex = re.compile(r""(?<=<text>).*(?=</text)"")
 info_regex = re.compile(r""{[^{]*?}"")
 html_regex = re.compile(r""&lt;!--[^-]*--&gt;"")
 ref_regex = re.compile(r""&lt;ref.*?&gt;"")  # non-greedy
@@ -285,7 +286,8 @@ def _process_wp_text(article_title, article_text, wp_to_id):
         return None, None
 
     # remove the text tags
-    text_search = text_regex.search(article_text)
+    text_search = text_tag_regex.sub("""", article_text)
+    text_search = text_regex.search(text_search)
     if text_search is None:
         return None, None
     text = text_search.group(0)
"
"spacy","2","efec28ce70a0ff69471cc379867deebe7eb881e0","cfdaf99b8029d6762730c5d5bd2b6f6c173c1241","spacy/util.py","spacy/util.py","diff --git a/spacy/util.py b/spacy/util.py","spacy/tests/regression/test_issue5137.py","","diff --git a/spacy/util.py b/spacy/util.py
index 609c0b572..d4cdca4e0 100644
--- a/spacy/util.py
+++ b/spacy/util.py
@@ -208,6 +208,7 @@ def load_model_from_path(model_path, meta=False, **overrides):
     for name in pipeline:
         if name not in disable:
             config = meta.get(""pipeline_args"", {}).get(name, {})
+            config.update(overrides)
             factory = factories.get(name, name)
             component = nlp.create_pipe(factory, config=config)
             nlp.add_pipe(component, name=name)
"
"spacy","8","fa95c030a511337935d1a2e930cb954c7a4cd376","5efae495f18f37316bd641a05ca26e62cb78e242","spacy/errors.py","spacy/errors.py","diff --git a/spacy/errors.py b/spacy/errors.py","spacy/tests/matcher/test_matcher_logic.py","","diff --git a/spacy/errors.py b/spacy/errors.py
index 5d4d4298e..d75b1cec8 100644
--- a/spacy/errors.py
+++ b/spacy/errors.py
@@ -498,6 +498,7 @@ class Errors(object):
             ""details: https://spacy.io/api/lemmatizer#init"")
     E174 = (""Architecture '{name}' not found in registry. Available ""
             ""names: {names}"")
+    E175 = (""Can't remove rule for unknown match pattern ID: {key}"")
 
 
 @add_codes
"
"spacy","1","9ce059dd067ecc3f097d04023e3cfa0d70d35bb8","a987e9e45d4084f30964a4cec9914ae6ed25a73c","spacy/errors.py","spacy/errors.py","diff --git a/spacy/errors.py b/spacy/errors.py","spacy/tests/test_errors.py","","diff --git a/spacy/errors.py b/spacy/errors.py
index 32ccd3df7..b97ef3a8e 100644
--- a/spacy/errors.py
+++ b/spacy/errors.py
@@ -7,8 +7,11 @@ def add_codes(err_cls):
 
     class ErrorsWithCodes(object):
         def __getattribute__(self, code):
-            msg = getattr(err_cls, code)
-            return ""[{code}] {msg}"".format(code=code, msg=msg)
+            if not code.startswith('__'):
+                msg = getattr(err_cls, code)
+                return ""[{code}] {msg}"".format(code=code, msg=msg)
+            else:
+                return super().__getattribute__(code)
 
     return ErrorsWithCodes()
 
"
"spacy","7","da6e0de34f4947fdebc839df3969c641014cfa97","6f54e59fe7ccb3bacce896ed33d36b39f11cbfaf","examples/information_extraction/entity_relations.py;spacy/util.py","examples/information_extraction/entity_relations.py;spacy/util.py","diff --git a/examples/information_extraction/entity_relations.py b/examples/information_extraction/entity_relations.py;diff --git a/spacy/util.py b/spacy/util.py","spacy/tests/doc/test_span.py","","diff --git a/examples/information_extraction/entity_relations.py b/examples/information_extraction/entity_relations.py
index 1b3ba1d27..c40a3c10d 100644
--- a/examples/information_extraction/entity_relations.py
+++ b/examples/information_extraction/entity_relations.py
@@ -7,7 +7,7 @@ dependency tree to find the noun phrase they are referring to – for example:
 $9.4 million --> Net income.
 
 Compatible with: spaCy v2.0.0+
-Last tested with: v2.1.0
+Last tested with: v2.2.1
 """"""
 from __future__ import unicode_literals, print_function
 
@@ -38,14 +38,17 @@ def main(model=""en_core_web_sm""):
 
 def filter_spans(spans):
     # Filter a sequence of spans so they don't contain overlaps
-    get_sort_key = lambda span: (span.end - span.start, span.start)
+    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()
+    get_sort_key = lambda span: (span.end - span.start, -span.start)
     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)
     result = []
     seen_tokens = set()
     for span in sorted_spans:
+        # Check for end - 1 here because boundaries are inclusive
         if span.start not in seen_tokens and span.end - 1 not in seen_tokens:
             result.append(span)
-            seen_tokens.update(range(span.start, span.end))
+        seen_tokens.update(range(span.start, span.end))
+    result = sorted(result, key=lambda span: span.start)
     return result
 
 
diff --git a/spacy/util.py b/spacy/util.py
index 9798ff11b..fa8111d67 100644
--- a/spacy/util.py
+++ b/spacy/util.py
@@ -666,7 +666,7 @@ def filter_spans(spans):
     spans (iterable): The spans to filter.
     RETURNS (list): The filtered spans.
     """"""
-    get_sort_key = lambda span: (span.end - span.start, span.start)
+    get_sort_key = lambda span: (span.end - span.start, -span.start)
     sorted_spans = sorted(spans, key=get_sort_key, reverse=True)
     result = []
     seen_tokens = set()
"
"spacy","10","38de08c7a99d5d8c490223126071afe7dd4f4b67","52904b72700a3f301a26563d3f94493bad96a446","spacy/errors.py","spacy/errors.py","diff --git a/spacy/errors.py b/spacy/errors.py","spacy/tests/matcher/test_matcher_api.py","","diff --git a/spacy/errors.py b/spacy/errors.py
index a6b199a50..02656e0e7 100644
--- a/spacy/errors.py
+++ b/spacy/errors.py
@@ -476,6 +476,8 @@ class Errors(object):
     E168 = (""Unknown field: {field}"")
     E169 = (""Can't find module: {module}"")
     E170 = (""Cannot apply transition {name}: invalid for the current state."")
+    E171 = (""Matcher.add received invalid on_match callback argument: expected ""
+            ""callable or None, but got: {arg_type}"")
 
 
 @add_codes
"
"spacy","9","bc7e7db208d351fae2982afbcdff7633f9636779","3297a19545027c8d8550b1ae793ce290567eff32","spacy/errors.py","spacy/errors.py","diff --git a/spacy/errors.py b/spacy/errors.py","spacy/tests/pipeline/test_tagger.py","","diff --git a/spacy/errors.py b/spacy/errors.py
index 30c7a5f48..93d42aa4c 100644
--- a/spacy/errors.py
+++ b/spacy/errors.py
@@ -88,6 +88,13 @@ class Warnings(object):
             ""loaded. (Shape: {shape})"")
     W021 = (""Unexpected hash collision in PhraseMatcher. Matches may be ""
             ""incorrect. Modify PhraseMatcher._terminal_hash to fix."")
+    W022 = (""Training a new part-of-speech tagger using a model with no ""
+            ""lemmatization rules or data. This means that the trained model ""
+            ""may not be able to lemmatize correctly. If this is intentional ""
+            ""or the language you're using doesn't have lemmatization data, ""
+            ""you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. ""
+            ""If this is surprising, make sure you have the spacy-lookups-data ""
+            ""package installed."")
 
 
 @add_codes
"
"spacy","6","6b874ef09611ac32ad038203423d44087cbeb3ae","afe4a428f78abe45d6104d74ef42a066570fa43d","spacy/language.py","spacy/language.py","diff --git a/spacy/language.py b/spacy/language.py","spacy/tests/pipeline/test_analysis.py","","diff --git a/spacy/language.py b/spacy/language.py
index 05838f21b..d53710f58 100644
--- a/spacy/language.py
+++ b/spacy/language.py
@@ -402,9 +402,10 @@ class Language(object):
         """"""
         if name not in self.pipe_names:
             raise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))
+        removed = self.pipeline.pop(self.pipe_names.index(name))
         if ENABLE_PIPELINE_ANALYSIS:
             analyze_all_pipes(self.pipeline)
-        return self.pipeline.pop(self.pipe_names.index(name))
+        return removed
 
     def __call__(self, text, disable=[], component_cfg=None):
         """"""Apply the pipeline to some text. The text can span multiple sentences,
"
"spacy","4","abd5c06374eab5db0cf897b73543b1f3eb007e12","9fa9d7f2cb52ce6a70c264d4e57c7f190d7686bf","spacy/cli/converters/conllu2json.py","spacy/cli/converters/conllu2json.py","diff --git a/spacy/cli/converters/conllu2json.py b/spacy/cli/converters/conllu2json.py","spacy/tests/regression/test_issue4665.py","","diff --git a/spacy/cli/converters/conllu2json.py b/spacy/cli/converters/conllu2json.py
index e66a8c50e..3de4dcc30 100644
--- a/spacy/cli/converters/conllu2json.py
+++ b/spacy/cli/converters/conllu2json.py
@@ -70,7 +70,7 @@ def read_conllx(input_data, use_morphology=False, n=0):
                     continue
                 try:
                     id_ = int(id_) - 1
-                    head = (int(head) - 1) if head != ""0"" else id_
+                    head = (int(head) - 1) if head not in [""0"", ""_""] else id_
                     dep = ""ROOT"" if dep == ""root"" else dep
                     tag = pos if tag == ""_"" else tag
                     tag = tag + ""__"" + morph if use_morphology else tag
"
"spacy","5","bdfb696677a7591ced018e7597c00929e97c6837","3bd15055ce74b04dcaf3b9abe2adeb01fb595776","spacy/language.py","spacy/language.py","diff --git a/spacy/language.py b/spacy/language.py","spacy/tests/test_language.py","","diff --git a/spacy/language.py b/spacy/language.py
index 72044a0c5..266a1727d 100644
--- a/spacy/language.py
+++ b/spacy/language.py
@@ -678,7 +678,7 @@ class Language(object):
             kwargs = component_cfg.get(name, {})
             kwargs.setdefault(""batch_size"", batch_size)
             if not hasattr(pipe, ""pipe""):
-                docs = _pipe(pipe, docs, kwargs)
+                docs = _pipe(docs, pipe, kwargs)
             else:
                 docs = pipe.pipe(docs, **kwargs)
         for doc, gold in zip(docs, golds):
"
"youtube-dl","22","1bd05345ea4b91598ec04b8e0d33fd14f9e2eddc","db13c16ef8968613680e2bbc85f373c3e74faf98","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_YoutubeDL.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 1279a9042..07c07be6f 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2383,6 +2383,7 @@ def _match_one(filter_part, dct):
         \s*(?P<op>%s)(?P<none_inclusive>\s*\?)?\s*
         (?:
             (?P<intval>[0-9.]+(?:[kKmMgGtTpPeEzZyY]i?[Bb]?)?)|
+            (?P<quote>[""\'])(?P<quotedstrval>(?:\\.|(?!(?P=quote)|\\).)+?)(?P=quote)|
             (?P<strval>(?![0-9.])[a-z0-9A-Z]*)
         )
         \s*$
@@ -2391,7 +2392,8 @@ def _match_one(filter_part, dct):
     if m:
         op = COMPARISON_OPERATORS[m.group('op')]
         actual_value = dct.get(m.group('key'))
-        if (m.group('strval') is not None or
+        if (m.group('quotedstrval') is not None or
+            m.group('strval') is not None or
             # If the original field is a string and matching comparisonvalue is
             # a number we should respect the origin of the original field
             # and process comparison value as a string (see
@@ -2401,7 +2403,10 @@ def _match_one(filter_part, dct):
             if m.group('op') not in ('=', '!='):
                 raise ValueError(
                     'Operator %s does not support string values!' % m.group('op'))
-            comparison_value = m.group('strval') or m.group('intval')
+            comparison_value = m.group('quotedstrval') or m.group('strval') or m.group('intval')
+            quote = m.group('quote')
+            if quote is not None:
+                comparison_value = comparison_value.replace(r'\%s' % quote, quote)
         else:
             try:
                 comparison_value = int(m.group('intval'))
"
"youtube-dl","18","dc6520aa3d1fe7afc52613e392f15dde90af4844","0396806f671e5828c2abdeb8048acf8b654507b6","youtube_dl/YoutubeDL.py","youtube_dl/YoutubeDL.py","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py","test/test_YoutubeDL.py","","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py
index 89c07be29..f94836d06 100755
--- a/youtube_dl/YoutubeDL.py
+++ b/youtube_dl/YoutubeDL.py
@@ -860,7 +860,7 @@ class YoutubeDL(object):
 
             force_properties = dict(
                 (k, v) for k, v in ie_result.items() if v is not None)
-            for f in ('_type', 'url', 'ie_key'):
+            for f in ('_type', 'url', 'id', 'extractor', 'extractor_key', 'ie_key'):
                 if f in force_properties:
                     del force_properties[f]
             new_result = info.copy()
"
"youtube-dl","14","562de77f41d0c08df9dbb08cfa86ba6c7d239c5a","84213ea8d41d5fe1608333a16ac578dccdf9a915","youtube_dl/extractor/youtube.py","youtube_dl/extractor/youtube.py","diff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py","test/test_youtube_chapters.py","","diff --git a/youtube_dl/extractor/youtube.py b/youtube_dl/extractor/youtube.py
index fec17987b..54ec76db5 100644
--- a/youtube_dl/extractor/youtube.py
+++ b/youtube_dl/extractor/youtube.py
@@ -1652,8 +1652,63 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
         video_id = mobj.group(2)
         return video_id
 
+    def _extract_chapters_from_json(self, webpage, video_id, duration):
+        if not webpage:
+            return
+        player = self._parse_json(
+            self._search_regex(
+                r'RELATED_PLAYER_ARGS[""\']\s*:\s*({.+})\s*,?\s*\n', webpage,
+                'player args', default='{}'),
+            video_id, fatal=False)
+        if not player or not isinstance(player, dict):
+            return
+        watch_next_response = player.get('watch_next_response')
+        if not isinstance(watch_next_response, compat_str):
+            return
+        response = self._parse_json(watch_next_response, video_id, fatal=False)
+        if not response or not isinstance(response, dict):
+            return
+        chapters_list = try_get(
+            response,
+            lambda x: x['playerOverlays']
+                       ['playerOverlayRenderer']
+                       ['decoratedPlayerBarRenderer']
+                       ['decoratedPlayerBarRenderer']
+                       ['playerBar']
+                       ['chapteredPlayerBarRenderer']
+                       ['chapters'],
+            list)
+        if not chapters_list:
+            return
+
+        def chapter_time(chapter):
+            return float_or_none(
+                try_get(
+                    chapter,
+                    lambda x: x['chapterRenderer']['timeRangeStartMillis'],
+                    int),
+                scale=1000)
+        chapters = []
+        for next_num, chapter in enumerate(chapters_list, start=1):
+            start_time = chapter_time(chapter)
+            if start_time is None:
+                continue
+            end_time = (chapter_time(chapters_list[next_num])
+                        if next_num < len(chapters_list) else duration)
+            if end_time is None:
+                continue
+            title = try_get(
+                chapter, lambda x: x['chapterRenderer']['title']['simpleText'],
+                compat_str)
+            chapters.append({
+                'start_time': start_time,
+                'end_time': end_time,
+                'title': title,
+            })
+        return chapters
+
     @staticmethod
-    def _extract_chapters(description, duration):
+    def _extract_chapters_from_description(description, duration):
         if not description:
             return None
         chapter_lines = re.findall(
@@ -1687,6 +1742,10 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
             })
         return chapters
 
+    def _extract_chapters(self, webpage, description, video_id, duration):
+        return (self._extract_chapters_from_json(webpage, video_id, duration)
+                or self._extract_chapters_from_description(description, duration))
+
     def _real_extract(self, url):
         url, smuggled_data = unsmuggle_url(url, {})
 
@@ -2324,7 +2383,7 @@ class YoutubeIE(YoutubeBaseInfoExtractor):
                     errnote='Unable to download video annotations', fatal=False,
                     data=urlencode_postdata({xsrf_field_name: xsrf_token}))
 
-        chapters = self._extract_chapters(description_original, video_duration)
+        chapters = self._extract_chapters(video_webpage, description_original, video_id, video_duration)
 
         # Look for the DASH manifest
         if self._downloader.params.get('youtube_include_dash_manifest', True):
"
"youtube-dl","3","f5469da9e6e259c1690c7ef54f1da1c19f65036f","95f3f7c20a05e7ac490e768b8470b20538ef8581","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index c9cbd5842..2554a2abd 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -596,7 +596,7 @@ def unescapeHTML(s):
     assert type(s) == compat_str
 
     return re.sub(
-        r'&([^;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)
+        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)
 
 
 def get_subprocess_encoding():
"
"youtube-dl","16","68d43a61b552007a718894967b869c0f1d8ff00f","3869028ffb6be6ab719e5cf1004276dfdfd1216d","youtube_dl/postprocessor/ffmpeg.py;youtube_dl/utils.py","youtube_dl/postprocessor/ffmpeg.py;youtube_dl/utils.py","diff --git a/youtube_dl/postprocessor/ffmpeg.py b/youtube_dl/postprocessor/ffmpeg.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/postprocessor/ffmpeg.py b/youtube_dl/postprocessor/ffmpeg.py
index 51256a3fb..f71d413b5 100644
--- a/youtube_dl/postprocessor/ffmpeg.py
+++ b/youtube_dl/postprocessor/ffmpeg.py
@@ -585,7 +585,7 @@ class FFmpegSubtitlesConvertorPP(FFmpegPostProcessor):
                 dfxp_file = old_file
                 srt_file = subtitles_filename(filename, lang, 'srt')
 
-                with io.open(dfxp_file, 'rt', encoding='utf-8') as f:
+                with open(dfxp_file, 'rb') as f:
                     srt_data = dfxp2srt(f.read())
 
                 with io.open(srt_file, 'wt', encoding='utf-8') as f:
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 9e4492d40..b724e0b70 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2572,14 +2572,18 @@ def srt_subtitles_timecode(seconds):
 
 
 def dfxp2srt(dfxp_data):
+    '''
+    @param dfxp_data A bytes-like object containing DFXP data
+    @returns A unicode object containing converted SRT data
+    '''
     LEGACY_NAMESPACES = (
-        ('http://www.w3.org/ns/ttml', [
-            'http://www.w3.org/2004/11/ttaf1',
-            'http://www.w3.org/2006/04/ttaf1',
-            'http://www.w3.org/2006/10/ttaf1',
+        (b'http://www.w3.org/ns/ttml', [
+            b'http://www.w3.org/2004/11/ttaf1',
+            b'http://www.w3.org/2006/04/ttaf1',
+            b'http://www.w3.org/2006/10/ttaf1',
         ]),
-        ('http://www.w3.org/ns/ttml#styling', [
-            'http://www.w3.org/ns/ttml#style',
+        (b'http://www.w3.org/ns/ttml#styling', [
+            b'http://www.w3.org/ns/ttml#style',
         ]),
     )
 
@@ -2674,7 +2678,7 @@ def dfxp2srt(dfxp_data):
         for ns in v:
             dfxp_data = dfxp_data.replace(ns, k)
 
-    dfxp = compat_etree_fromstring(dfxp_data.encode('utf-8'))
+    dfxp = compat_etree_fromstring(dfxp_data)
     out = []
     paras = dfxp.findall(_x('.//ttml:p')) or dfxp.findall('.//p')
 
"
"youtube-dl","2","84f085d4bdb66ee025fb337bcd571eab7469da97","9d6ac71c27b1dfb662c795ef598dbfd0286682da","youtube_dl/extractor/common.py","youtube_dl/extractor/common.py","diff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py","test/test_InfoExtractor.py","","diff --git a/youtube_dl/extractor/common.py b/youtube_dl/extractor/common.py
index 3b79b8cb4..35d427eec 100644
--- a/youtube_dl/extractor/common.py
+++ b/youtube_dl/extractor/common.py
@@ -2007,16 +2007,14 @@ class InfoExtractor(object):
                                     f['url'] = initialization_url
                                 f['fragments'].append({location_key(initialization_url): initialization_url})
                             f['fragments'].extend(representation_ms_info['fragments'])
-                        try:
-                            existing_format = next(
-                                fo for fo in formats
-                                if fo['format_id'] == representation_id)
-                        except StopIteration:
-                            full_info = formats_dict.get(representation_id, {}).copy()
-                            full_info.update(f)
-                            formats.append(full_info)
-                        else:
-                            existing_format.update(f)
+                        # According to [1, 5.3.5.2, Table 7, page 35] @id of Representation
+                        # is not necessarily unique within a Period thus formats with
+                        # the same `format_id` are quite possible. There are numerous examples
+                        # of such manifests (see https://github.com/rg3/youtube-dl/issues/15111,
+                        # https://github.com/rg3/youtube-dl/issues/13919)
+                        full_info = formats_dict.get(representation_id, {}).copy()
+                        full_info.update(f)
+                        formats.append(full_info)
                     else:
                         self.report_warning('Unknown MIME type %s in DASH manifest' % mime_type)
         return formats
"
"youtube-dl","38","94eae04c94a43847e8ce7c9bf3d88dd029ef62f6","b74fa8cd2c9deb412ac277c6cc44847c3839b844","youtube_dl/extractor/facebook.py;youtube_dl/utils.py","youtube_dl/extractor/facebook.py;youtube_dl/utils.py","diff --git a/youtube_dl/extractor/facebook.py b/youtube_dl/extractor/facebook.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/extractor/facebook.py b/youtube_dl/extractor/facebook.py
index f6b5f589a..848a2c263 100644
--- a/youtube_dl/extractor/facebook.py
+++ b/youtube_dl/extractor/facebook.py
@@ -11,6 +11,7 @@ from ..utils import (
     compat_urllib_error,
     compat_urllib_parse,
     compat_urllib_request,
+    urlencode_postdata,
 
     ExtractorError,
 )
@@ -51,8 +52,8 @@ class FacebookIE(InfoExtractor):
 
         login_page_req = compat_urllib_request.Request(self._LOGIN_URL)
         login_page_req.add_header('Cookie', 'locale=en_US')
-        self.report_login()
-        login_page = self._download_webpage(login_page_req, None, note=False,
+        login_page = self._download_webpage(login_page_req, None,
+            note='Downloading login page',
             errnote='Unable to download login page')
         lsd = self._search_regex(
             r'<input type=""hidden"" name=""lsd"" value=""([^""]*)""',
@@ -70,23 +71,25 @@ class FacebookIE(InfoExtractor):
             'timezone': '-60',
             'trynum': '1',
             }
-        request = compat_urllib_request.Request(self._LOGIN_URL, compat_urllib_parse.urlencode(login_form))
+        request = compat_urllib_request.Request(self._LOGIN_URL, urlencode_postdata(login_form))
         request.add_header('Content-Type', 'application/x-www-form-urlencoded')
         try:
-            login_results = compat_urllib_request.urlopen(request).read()
+            login_results = self._download_webpage(request, None,
+                note='Logging in', errnote='unable to fetch login page')
             if re.search(r'<form(.*)name=""login""(.*)</form>', login_results) is not None:
                 self._downloader.report_warning('unable to log in: bad username/password, or exceded login rate limit (~3/min). Check credentials or wait.')
                 return
 
             check_form = {
-                'fb_dtsg': self._search_regex(r'""fb_dtsg"":""(.*?)""', login_results, 'fb_dtsg'),
+                'fb_dtsg': self._search_regex(r'name=""fb_dtsg"" value=""(.+?)""', login_results, 'fb_dtsg'),
                 'nh': self._search_regex(r'name=""nh"" value=""(\w*?)""', login_results, 'nh'),
                 'name_action_selected': 'dont_save',
-                'submit[Continue]': self._search_regex(r'<input value=""(.*?)"" name=""submit\[Continue\]""', login_results, 'continue'),
+                'submit[Continue]': self._search_regex(r'<button[^>]+value=""(.*?)""[^>]+name=""submit\[Continue\]""', login_results, 'continue'),
             }
-            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, compat_urllib_parse.urlencode(check_form))
+            check_req = compat_urllib_request.Request(self._CHECKPOINT_URL, urlencode_postdata(check_form))
             check_req.add_header('Content-Type', 'application/x-www-form-urlencoded')
-            check_response = compat_urllib_request.urlopen(check_req).read()
+            check_response = self._download_webpage(check_req, None,
+                note='Confirming login')
             if re.search(r'id=""checkpointSubmitButton""', check_response) is not None:
                 self._downloader.report_warning('Unable to confirm login, you have to login in your brower and authorize the login.')
         except (compat_urllib_error.URLError, compat_http_client.HTTPException, socket.error) as err:
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 02b8f7c45..d4abd4031 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1263,3 +1263,7 @@ def read_batch_urls(batch_fd):
 
     with contextlib.closing(batch_fd) as fd:
         return [url for url in map(fixup, fd) if url]
+
+
+def urlencode_postdata(*args, **kargs):
+    return compat_urllib_parse.urlencode(*args, **kargs).encode('ascii')
"
"youtube-dl","8","cf2ac6df6896dac4d23918867bb86fac1e1088d9","f5f4a27a964b41646303921104f4d6d6fd2098e4","youtube_dl/YoutubeDL.py","youtube_dl/YoutubeDL.py","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py","test/test_YoutubeDL.py","","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py
index 5deb4848e..5a79e5f1d 100755
--- a/youtube_dl/YoutubeDL.py
+++ b/youtube_dl/YoutubeDL.py
@@ -958,8 +958,7 @@ class YoutubeDL(object):
                     elif string == '/':
                         first_choice = current_selector
                         second_choice = _parse_format_selection(tokens, inside_choice=True)
-                        current_selector = None
-                        selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))
+                        current_selector = FormatSelector(PICKFIRST, (first_choice, second_choice), [])
                     elif string == '[':
                         if not current_selector:
                             current_selector = FormatSelector(SINGLE, 'best', [])
"
"youtube-dl","23","a22b2fd19bd8c08d50f884d1903486d4f00f76ec","b3ee552e4b918fb720111b23147e24fa5475a74b","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index cf46711b9..6c462625b 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2107,7 +2107,7 @@ def js_to_json(code):
         v = m.group(0)
         if v in ('true', 'false', 'null'):
             return v
-        elif v.startswith('/*') or v == ',':
+        elif v.startswith('/*') or v.startswith('//') or v == ',':
             return """"
 
         if v[0] in (""'"", '""'):
@@ -2134,7 +2134,7 @@ def js_to_json(code):
     return re.sub(r'''(?sx)
         ""(?:[^""\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^""\\]*""|
         '(?:[^'\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^'\\]*'|
-        /\*.*?\*/|,(?=\s*[\]}])|
+        /\*.*?\*/|//[^\n]*|,(?=\s*[\]}])|
         [a-zA-Z_][.a-zA-Z_0-9]*|
         \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|
         [0-9]+(?=\s*:)
"
"youtube-dl","1","99036a1298089068dcf80c0985bfcc3f8c24f281","1cc47c667419e0eadc0a6989256ab7b276852adf","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 027d12785..574284e94 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2574,8 +2574,8 @@ def _match_one(filter_part, dct):
         return op(actual_value, comparison_value)
 
     UNARY_OPERATORS = {
-        '': lambda v: v is not None,
-        '!': lambda v: v is None,
+        '': lambda v: (v is True) if isinstance(v, bool) else (v is not None),
+        '!': lambda v: (v is False) if isinstance(v, bool) else (v is None),
     }
     operator_rex = re.compile(r'''(?x)\s*
         (?P<op>%s)\s*(?P<key>[a-z_]+)
"
"youtube-dl","31","ab07963b5cc79812c6fb7e4f9e363533d8123830","e8df5cee12378acd708b6686130a73c5edc06f0e","youtube_dl/extractor/minhateca.py;youtube_dl/utils.py","youtube_dl/extractor/minhateca.py;youtube_dl/utils.py","diff --git a/youtube_dl/extractor/minhateca.py b/youtube_dl/extractor/minhateca.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/extractor/minhateca.py b/youtube_dl/extractor/minhateca.py
index 077c9b19d..14934b7ec 100644
--- a/youtube_dl/extractor/minhateca.py
+++ b/youtube_dl/extractor/minhateca.py
@@ -8,6 +8,7 @@ from ..compat import (
 )
 from ..utils import (
     int_or_none,
+    parse_duration,
     parse_filesize,
 )
 
@@ -52,8 +53,8 @@ class MinhatecaIE(InfoExtractor):
         filesize_approx = parse_filesize(self._html_search_regex(
             r'<p class=""fileSize"">(.*?)</p>',
             webpage, 'file size approximation', fatal=False))
-        duration = int_or_none(self._html_search_regex(
-            r'(?s)<p class=""fileLeng[ht][th]"">.*?([0-9]+)\s*s',
+        duration = parse_duration(self._html_search_regex(
+            r'(?s)<p class=""fileLeng[ht][th]"">.*?class=""bold"">(.*?)<',
             webpage, 'duration', fatal=False))
         view_count = int_or_none(self._html_search_regex(
             r'<p class=""downloadsCounter"">([0-9]+)</p>',
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 5e9ae7a42..5efb4c59a 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1206,18 +1206,29 @@ def parse_duration(s):
 
     m = re.match(
         r'''(?ix)T?
+        (?:
+            (?P<only_mins>[0-9.]+)\s*(?:mins?|minutes?)\s*|
+            (?P<only_hours>[0-9.]+)\s*(?:hours?)|
+
             (?:
                 (?:(?P<hours>[0-9]+)\s*(?:[:h]|hours?)\s*)?
                 (?P<mins>[0-9]+)\s*(?:[:m]|mins?|minutes?)\s*
             )?
-            (?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?\s*(?:s|secs?|seconds?)?$''', s)
+            (?P<secs>[0-9]+)(?P<ms>\.[0-9]+)?\s*(?:s|secs?|seconds?)?
+        )$''', s)
     if not m:
         return None
-    res = int(m.group('secs'))
+    res = 0
+    if m.group('only_mins'):
+        return float_or_none(m.group('only_mins'), invscale=60)
+    if m.group('only_hours'):
+        return float_or_none(m.group('only_hours'), invscale=60 * 60)
+    if m.group('secs'):
+        res += int(m.group('secs'))
     if m.group('mins'):
         res += int(m.group('mins')) * 60
-        if m.group('hours'):
-            res += int(m.group('hours')) * 60 * 60
+    if m.group('hours'):
+        res += int(m.group('hours')) * 60 * 60
     if m.group('ms'):
         res += float(m.group('ms'))
     return res
"
"youtube-dl","43","cecaaf3f58ad9f544dbb79af1e565d9353fa2b2d","d6c7a367e88096bb17e323954002c084477fe908","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index a249c7ec1..2d12e2df9 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1087,7 +1087,7 @@ def remove_start(s, start):
 
 
 def url_basename(url):
-    m = re.match(r'(?:https?:|)//[^/]+/(?:[^/?#]+/)?([^/?#]+)/?(?:[?#]|$)', url)
+    m = re.match(r'(?:https?:|)//[^/]+/(?:[^?#]+/)?([^/?#]+)/?(?:[?#]|$)', url)
     if not m:
         return u''
     return m.group(1)
"
"youtube-dl","39","b04c8f735805ea2671429ac8d683c2887a6b4db8","a020a0dc20ced6468ec46214c394f6f360735b1d","youtube_dl/extractor/facebook.py;youtube_dl/utils.py","youtube_dl/extractor/facebook.py;youtube_dl/utils.py","diff --git a/youtube_dl/extractor/facebook.py b/youtube_dl/extractor/facebook.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/extractor/facebook.py b/youtube_dl/extractor/facebook.py
index d675a939d..60e68d98a 100644
--- a/youtube_dl/extractor/facebook.py
+++ b/youtube_dl/extractor/facebook.py
@@ -12,8 +12,8 @@ from ..utils import (
     compat_urllib_parse,
     compat_urllib_request,
     urlencode_postdata,
-
     ExtractorError,
+    limit_length,
 )
 
 
@@ -37,6 +37,14 @@ class FacebookIE(InfoExtractor):
             'duration': 38,
             'title': 'Did you know Kei Nishikori is the first Asian man to ever reach a Grand Slam fin...',
         }
+    }, {
+        'note': 'Video without discernible title',
+        'url': 'https://www.facebook.com/video.php?v=274175099429670',
+        'info_dict': {
+            'id': '274175099429670',
+            'ext': 'mp4',
+            'title': 'Facebook video #274175099429670',
+        }
     }, {
         'url': 'https://www.facebook.com/video.php?v=10204634152394104',
         'only_matching': True,
@@ -131,8 +139,7 @@ class FacebookIE(InfoExtractor):
             video_title = self._html_search_regex(
                 r'(?s)<span class=""fbPhotosPhotoCaption"".*?id=""fbPhotoPageCaption""><span class=""hasCaption"">(.*?)</span>',
                 webpage, 'alternative title', default=None)
-            if len(video_title) > 80 + 3:
-                video_title = video_title[:80] + '...'
+            video_title = limit_length(video_title, 80)
         if not video_title:
             video_title = 'Facebook video #%s' % video_id
 
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 247788078..3ac0f1f54 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1571,3 +1571,13 @@ except AttributeError:
         if ret:
             raise subprocess.CalledProcessError(ret, p.args, output=output)
         return output
+
+
+def limit_length(s, length):
+    """""" Add ellipses to overly long strings """"""
+    if s is None:
+        return None
+    ELLIPSES = '...'
+    if len(s) > length:
+        return s[:length - len(ELLIPSES)] + ELLIPSES
+    return s
"
"youtube-dl","7","63a64948342ebfe46db8c258765e698a04a61904","d01949dc89feb2441f251e42e8a6bfa4711b9715","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index db5b3698e..a61e47646 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1701,8 +1701,8 @@ def js_to_json(code):
         if v in ('true', 'false', 'null'):
             return v
         if v.startswith('""'):
-            return v
-        if v.startswith(""'""):
+            v = re.sub(r""\\'"", ""'"", v[1:-1])
+        elif v.startswith(""'""):
             v = v[1:-1]
             v = re.sub(r""\\\\|\\'|\"""", lambda m: {
                 '\\\\': '\\\\',
"
"youtube-dl","25","9e5751b9fe72f7425e4cb3f22a56b6a95b59e41d","e4659b45474acb563db0ab4284abdfc80837307e","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 35362e767..0c36c1b80 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2038,14 +2038,14 @@ def js_to_json(code):
             }.get(m.group(0), m.group(0)), v[1:-1])
 
         INTEGER_TABLE = (
-            (r'^0[xX][0-9a-fA-F]+', 16),
-            (r'^0+[0-7]+', 8),
+            (r'^(0[xX][0-9a-fA-F]+)\s*:?$', 16),
+            (r'^(0+[0-7]+)\s*:?$', 8),
         )
 
         for regex, base in INTEGER_TABLE:
             im = re.match(regex, v)
             if im:
-                i = int(im.group(0), base)
+                i = int(im.group(1), base)
                 return '""%d"":' % i if v.endswith(':') else '%d' % i
 
         return '""%s""' % v
"
"youtube-dl","10","85d586617750d38d742a24f141b099f6b898d269","d305dd73a3d6927f0a2c63d08662a183fa173833","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 3eb6bc6d4..4358137a0 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1560,8 +1560,8 @@ def js_to_json(code):
         return '""%s""' % v
 
     res = re.sub(r'''(?x)
-        ""(?:[^""\\]*(?:\\\\|\\"")?)*""|
-        '(?:[^'\\]*(?:\\\\|\\')?)*'|
+        ""(?:[^""\\]*(?:\\\\|\\['""nu]))*[^""\\]*""|
+        '(?:[^'\\]*(?:\\\\|\\['""nu]))*[^'\\]*'|
         [a-zA-Z_][.a-zA-Z_0-9]*
         ''', fix_kv, code)
     res = re.sub(r',(\s*\])', lambda m: m.group(1), res)
"
"youtube-dl","13","6945b9e78f38284eb4e440b7badea2fc60b66c2f","fad4ceb53404227f471af2f3544c4c14a5df4acb","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index d2d3c1a9f..d0cb65814 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1868,7 +1868,7 @@ def urljoin(base, path):
         path = path.decode('utf-8')
     if not isinstance(path, compat_str) or not path:
         return None
-    if re.match(r'^(?:https?:)?//', path):
+    if re.match(r'^(?:[a-zA-Z][a-zA-Z0-9+-.]*:)?//', path):
         return path
     if isinstance(base, bytes):
         base = base.decode('utf-8')
"
"youtube-dl","41","81c2f20b5386d89a62dc27293654d75b77f47473","026fcc04956f2077a50cd4b4e9b87f45d2bcddea","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index fa8f80e02..67c6af507 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -756,9 +756,9 @@ def unified_strdate(date_str):
     """"""Return a string with the date in the format YYYYMMDD""""""
     upload_date = None
     #Replace commas
-    date_str = date_str.replace(',',' ')
+    date_str = date_str.replace(',', ' ')
     # %z (UTC offset) is only supported in python>=3.2
-    date_str = re.sub(r' ?(\+|-)[0-9:]*$', '', date_str)
+    date_str = re.sub(r' ?(\+|-)[0-9]{2}:?[0-9]{2}$', '', date_str)
     format_expressions = [
         '%d %B %Y',
         '%B %d %Y',
"
"youtube-dl","42","b853d2e1555dbb4a09fe3d7857c6d2bc044646f4","5aafe895fce2a7be9595cb2e56b7bd73a748e6b6","youtube_dl/extractor/clipsyndicate.py;youtube_dl/extractor/metacritic.py;youtube_dl/extractor/mtv.py;youtube_dl/utils.py","youtube_dl/extractor/clipsyndicate.py;youtube_dl/extractor/metacritic.py;youtube_dl/extractor/mtv.py;youtube_dl/utils.py","diff --git a/youtube_dl/extractor/clipsyndicate.py b/youtube_dl/extractor/clipsyndicate.py;diff --git a/youtube_dl/extractor/metacritic.py b/youtube_dl/extractor/metacritic.py;diff --git a/youtube_dl/extractor/mtv.py b/youtube_dl/extractor/mtv.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/extractor/clipsyndicate.py b/youtube_dl/extractor/clipsyndicate.py
index c60089ad3..9ab6a4ab6 100644
--- a/youtube_dl/extractor/clipsyndicate.py
+++ b/youtube_dl/extractor/clipsyndicate.py
@@ -3,7 +3,7 @@ import re
 from .common import InfoExtractor
 from ..utils import (
     find_xpath_attr,
-    fix_xml_all_ampersand,
+    fix_xml_ampersands
 )
 
 
@@ -33,7 +33,7 @@ class ClipsyndicateIE(InfoExtractor):
         pdoc = self._download_xml(
             'http://eplayer.clipsyndicate.com/osmf/playlist?%s' % flvars,
             video_id, u'Downloading video info',
-            transform_source=fix_xml_all_ampersand) 
+            transform_source=fix_xml_ampersands)
 
         track_doc = pdoc.find('trackList/track')
         def find_param(name):
diff --git a/youtube_dl/extractor/metacritic.py b/youtube_dl/extractor/metacritic.py
index f3ff0e8bb..465ac4916 100644
--- a/youtube_dl/extractor/metacritic.py
+++ b/youtube_dl/extractor/metacritic.py
@@ -4,7 +4,7 @@ import re
 
 from .common import InfoExtractor
 from ..utils import (
-    fix_xml_all_ampersand,
+    fix_xml_ampersands,
 )
 
 
@@ -27,7 +27,7 @@ class MetacriticIE(InfoExtractor):
         webpage = self._download_webpage(url, video_id)
         # The xml is not well formatted, there are raw '&'
         info = self._download_xml('http://www.metacritic.com/video_data?video=' + video_id,
-            video_id, 'Downloading info xml', transform_source=fix_xml_all_ampersand)
+            video_id, 'Downloading info xml', transform_source=fix_xml_ampersands)
 
         clip = next(c for c in info.findall('playList/clip') if c.find('id').text == video_id)
         formats = []
diff --git a/youtube_dl/extractor/mtv.py b/youtube_dl/extractor/mtv.py
index f1cf41e2d..c4fa16fb6 100644
--- a/youtube_dl/extractor/mtv.py
+++ b/youtube_dl/extractor/mtv.py
@@ -5,6 +5,7 @@ from .common import InfoExtractor
 from ..utils import (
     compat_urllib_parse,
     ExtractorError,
+    fix_xml_ampersands,
 )
 
 def _media_xml_tag(tag):
@@ -83,12 +84,9 @@ class MTVServicesInfoExtractor(InfoExtractor):
         video_id = self._id_from_uri(uri)
         data = compat_urllib_parse.urlencode({'uri': uri})
 
-        def fix_ampersand(s):
-            """""" Fix unencoded ampersand in XML """"""
-            return s.replace(u'& ', '&amp; ')
         idoc = self._download_xml(
             self._FEED_URL + '?' + data, video_id,
-            u'Downloading info', transform_source=fix_ampersand)
+            u'Downloading info', transform_source=fix_xml_ampersands)
         return [self._get_video_info(item) for item in idoc.findall('.//item')]
 
 
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 73fe1ad0a..70f284149 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1092,9 +1092,12 @@ def month_by_name(name):
         return None
 
 
-def fix_xml_all_ampersand(xml_str):
+def fix_xml_ampersands(xml_str):
     """"""Replace all the '&' by '&amp;' in XML""""""
-    return xml_str.replace(u'&', u'&amp;')
+    return re.sub(
+        r'&(?!amp;|lt;|gt;|apos;|quot;|#x[0-9a-fA-F]{,4};|#[0-9]{,4};)',
+        u'&amp;',
+        xml_str)
 
 
 def setproctitle(title):
"
"youtube-dl","19","9a0942ad55bba714d6eaeb9ee4f66a138ec85e17","15da37c7dc8cf14ba5ce880aa1805fceaa71fc44","youtube_dl/YoutubeDL.py","youtube_dl/YoutubeDL.py","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py","test/test_YoutubeDL.py","","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py
index 60ee4b7d8..8730d32ef 100755
--- a/youtube_dl/YoutubeDL.py
+++ b/youtube_dl/YoutubeDL.py
@@ -20,6 +20,7 @@ import re
 import shutil
 import subprocess
 import socket
+import string
 import sys
 import time
 import tokenize
@@ -674,7 +675,19 @@ class YoutubeDL(object):
                         FORMAT_RE.format(numeric_field),
                         r'%({0})s'.format(numeric_field), outtmpl)
 
-            filename = expand_path(outtmpl % template_dict)
+            # expand_path translates '%%' into '%' and '$$' into '$'
+            # correspondingly that is not what we want since we need to keep
+            # '%%' intact for template dict substitution step. Working around
+            # with boundary-alike separator hack.
+            sep = ''.join([random.choice(string.ascii_letters) for _ in range(32)])
+            outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))
+
+            # outtmpl should be expand_path'ed before template dict substitution
+            # because meta fields may contain env variables we don't want to
+            # be expanded. For example, for outtmpl ""%(title)s.%(ext)s"" and
+            # title ""Hello $PATH"", we don't want `$PATH` to be expanded.
+            filename = expand_path(outtmpl).replace(sep, '') % template_dict
+
             # Temporary fix for #4787
             # 'Treat' all problem characters by passing filename through preferredencoding
             # to workaround encoding issues with subprocess on python2 @ Windows
"
"youtube-dl","9","0130afb76e5cb6f470f39f127c8d09eea3e82d0d","cf2ac6df6896dac4d23918867bb86fac1e1088d9","youtube_dl/YoutubeDL.py","youtube_dl/YoutubeDL.py","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py","test/test_YoutubeDL.py","","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py
index e5b46f87e..5deb4848e 100755
--- a/youtube_dl/YoutubeDL.py
+++ b/youtube_dl/YoutubeDL.py
@@ -931,7 +931,7 @@ class YoutubeDL(object):
                 else:
                     filter_parts.append(string)
 
-        def _parse_format_selection(tokens, endwith=[]):
+        def _parse_format_selection(tokens, inside_merge=False, inside_choice=False, inside_group=False):
             selectors = []
             current_selector = None
             for type, string, start, _, _ in tokens:
@@ -941,18 +941,23 @@ class YoutubeDL(object):
                 elif type in [tokenize.NAME, tokenize.NUMBER]:
                     current_selector = FormatSelector(SINGLE, string, [])
                 elif type == tokenize.OP:
-                    if string in endwith:
+                    if string == ')':
+                        if not inside_group:
+                            # ')' will be handled by the parentheses group
+                            tokens.restore_last_token()
                         break
-                    elif string == ')':
-                        # ')' will be handled by the parentheses group
+                    elif inside_merge and string in ['/', ',']:
                         tokens.restore_last_token()
                         break
-                    if string == ',':
+                    elif inside_choice and string == ',':
+                        tokens.restore_last_token()
+                        break
+                    elif string == ',':
                         selectors.append(current_selector)
                         current_selector = None
                     elif string == '/':
                         first_choice = current_selector
-                        second_choice = _parse_format_selection(tokens, [','])
+                        second_choice = _parse_format_selection(tokens, inside_choice=True)
                         current_selector = None
                         selectors.append(FormatSelector(PICKFIRST, (first_choice, second_choice), []))
                     elif string == '[':
@@ -963,12 +968,12 @@ class YoutubeDL(object):
                     elif string == '(':
                         if current_selector:
                             raise syntax_error('Unexpected ""(""', start)
-                        current_selector = FormatSelector(GROUP, _parse_format_selection(tokens, [')']), [])
+                        group = _parse_format_selection(tokens, inside_group=True)
+                        current_selector = FormatSelector(GROUP, group, [])
                     elif string == '+':
                         video_selector = current_selector
-                        audio_selector = _parse_format_selection(tokens, [','])
-                        current_selector = None
-                        selectors.append(FormatSelector(MERGE, (video_selector, audio_selector), []))
+                        audio_selector = _parse_format_selection(tokens, inside_merge=True)
+                        current_selector = FormatSelector(MERGE, (video_selector, audio_selector), [])
                     else:
                         raise syntax_error('Operator not recognized: ""{0}""'.format(string), start)
                 elif type == tokenize.ENDMARKER:
"
"youtube-dl","21","96182695e4e37795a30ab143129c91dab18a9865","4b5de77bdb7765df4797bf068592926285ba709a","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 8738aa249..d293c7498 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1748,11 +1748,16 @@ def base_url(url):
 
 
 def urljoin(base, path):
+    if isinstance(path, bytes):
+        path = path.decode('utf-8')
     if not isinstance(path, compat_str) or not path:
         return None
     if re.match(r'^(?:https?:)?//', path):
         return path
-    if not isinstance(base, compat_str) or not re.match(r'^(?:https?:)?//', base):
+    if isinstance(base, bytes):
+        base = base.decode('utf-8')
+    if not isinstance(base, compat_str) or not re.match(
+            r'^(?:https?:)?//', base):
         return None
     return compat_urlparse.urljoin(base, path)
 
"
"youtube-dl","30","f5f4a27a964b41646303921104f4d6d6fd2098e4","bb8e55366289e0c129ef85abb8c1ac1cbae86a66","youtube_dl/YoutubeDL.py","youtube_dl/YoutubeDL.py","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py","test/test_YoutubeDL.py","","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py
index 5a79e5f1d..6478d05dc 100755
--- a/youtube_dl/YoutubeDL.py
+++ b/youtube_dl/YoutubeDL.py
@@ -1005,6 +1005,9 @@ class YoutubeDL(object):
                 format_spec = selector.selector
 
                 def selector_function(formats):
+                    formats = list(formats)
+                    if not formats:
+                        return
                     if format_spec == 'all':
                         for f in formats:
                             yield f
"
"youtube-dl","15","e7f3529f68ee7c8ca78366d37f851cb31fa00f31","c384d537f882efab10a78a56ce6dcb0a30f54b47","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 386897a85..2fe9cf585 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2267,7 +2267,7 @@ def js_to_json(code):
         ""(?:[^""\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^""\\]*""|
         '(?:[^'\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^'\\]*'|
         {comment}|,(?={skip}[\]}}])|
-        [a-zA-Z_][.a-zA-Z_0-9]*|
+        (?:(?<![0-9])[eE]|[a-df-zA-DF-Z_])[.a-zA-Z_0-9]*|
         \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:{skip}:)?|
         [0-9]+(?={skip}:)
         '''.format(comment=COMMENT_RE, skip=SKIP_RE), fix_kv, code)
"
"youtube-dl","32","bf951c5e29548cfed80480389762edd29fcc8825","609a61e3e6fffce3d45e845f33ae2c5fa2d432ac","youtube_dl/extractor/npo.py;youtube_dl/utils.py","youtube_dl/extractor/npo.py;youtube_dl/utils.py","diff --git a/youtube_dl/extractor/npo.py b/youtube_dl/extractor/npo.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/extractor/npo.py b/youtube_dl/extractor/npo.py
index f36d446d2..ce31694a5 100644
--- a/youtube_dl/extractor/npo.py
+++ b/youtube_dl/extractor/npo.py
@@ -7,6 +7,7 @@ from ..utils import (
     unified_strdate,
     parse_duration,
     qualities,
+    strip_jsonp,
     url_basename,
 )
 
@@ -63,7 +64,7 @@ class NPOIE(InfoExtractor):
             'http://e.omroep.nl/metadata/aflevering/%s' % video_id,
             video_id,
             # We have to remove the javascript callback
-            transform_source=lambda j: re.sub(r'parseMetadata\((.*?)\);\n//.*$', r'\1', j)
+            transform_source=strip_jsonp,
         )
         token_page = self._download_webpage(
             'http://ida.omroep.nl/npoplayer/i.js',
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 0b2ba39e2..5786fa699 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1331,7 +1331,8 @@ def parse_age_limit(s):
 
 
 def strip_jsonp(code):
-    return re.sub(r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?\s*$', r'\1', code)
+    return re.sub(
+        r'(?s)^[a-zA-Z0-9_]+\s*\(\s*(.*)\);?\s*?(?://[^\n]*)*$', r'\1', code)
 
 
 def js_to_json(code):
"
"youtube-dl","11","b568561eba6f4aceb87419e21aba11567c5de7da","348c6bf1c1a00eec323d6e21ff7b9b12699afe04","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index b14603d8a..328f037a8 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -3519,8 +3519,8 @@ def str_or_none(v, default=None):
 
 def str_to_int(int_str):
     """""" A more relaxed version of int_or_none """"""
-    if int_str is None:
-        return None
+    if not isinstance(int_str, compat_str):
+        return int_str
     int_str = re.sub(r'[,\.\+]', '', int_str)
     return int(int_str)
 
"
"youtube-dl","6","4f29fa99069760dc47ef9ca5dbf607a567d2982f","d631d5f9f27f93767226192e4288990413fa9dbd","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 91917fc96..ee20c3d9b 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1976,7 +1976,7 @@ def match_filter_func(filter_str):
 
 def parse_dfxp_time_expr(time_expr):
     if not time_expr:
-        return 0.0
+        return
 
     mobj = re.match(r'^(?P<time_offset>\d+(?:\.\d+)?)s?$', time_expr)
     if mobj:
@@ -2020,10 +2020,15 @@ def dfxp2srt(dfxp_data):
         raise ValueError('Invalid dfxp/TTML subtitle')
 
     for para, index in zip(paras, itertools.count(1)):
-        begin_time = parse_dfxp_time_expr(para.attrib['begin'])
+        begin_time = parse_dfxp_time_expr(para.attrib.get('begin'))
         end_time = parse_dfxp_time_expr(para.attrib.get('end'))
+        dur = parse_dfxp_time_expr(para.attrib.get('dur'))
+        if begin_time is None:
+            continue
         if not end_time:
-            end_time = begin_time + parse_dfxp_time_expr(para.attrib['dur'])
+            if not dur:
+                continue
+            end_time = begin_time + dur
         out.append('%d\n%s --> %s\n%s\n\n' % (
             index,
             srt_subtitles_timecode(begin_time),
"
"youtube-dl","35","89294b5f50462ede8ba83463ff262eb2c5219e1b","99b67fecc5ab6c57eada1e1678034dd71c57e338","youtube_dl/extractor/arte.py;youtube_dl/utils.py","youtube_dl/extractor/arte.py;youtube_dl/utils.py","diff --git a/youtube_dl/extractor/arte.py b/youtube_dl/extractor/arte.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/extractor/arte.py b/youtube_dl/extractor/arte.py
index 957d35979..c3d02f85e 100644
--- a/youtube_dl/extractor/arte.py
+++ b/youtube_dl/extractor/arte.py
@@ -86,11 +86,15 @@ class ArteTVPlus7IE(InfoExtractor):
         info = self._download_json(json_url, video_id)
         player_info = info['videoJsonPlayer']
 
+        upload_date_str = player_info.get('shootingDate')
+        if not upload_date_str:
+            upload_date_str = player_info.get('VDA', '').split(' ')[0]
+
         info_dict = {
             'id': player_info['VID'],
             'title': player_info['VTI'],
             'description': player_info.get('VDE'),
-            'upload_date': unified_strdate(player_info.get('VDA', '').split(' ')[0]),
+            'upload_date': unified_strdate(upload_date_str),
             'thumbnail': player_info.get('programImage') or player_info.get('VTU', {}).get('IUR'),
         }
 
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 950cd1a7a..f05747097 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -890,6 +890,7 @@ def unified_strdate(date_str):
         '%d/%m/%Y',
         '%d/%m/%y',
         '%Y/%m/%d %H:%M:%S',
+        '%d/%m/%Y %H:%M:%S',
         '%Y-%m-%d %H:%M:%S',
         '%d.%m.%Y %H:%M',
         '%d.%m.%Y %H.%M',
"
"youtube-dl","33","50c8266ef0b2b6d011257a909f47fd623dda8eb2","6ad4013d40e839211e2896129eed05ccd40ee963","youtube_dl/extractor/drtv.py;youtube_dl/utils.py","youtube_dl/extractor/drtv.py;youtube_dl/utils.py","diff --git a/youtube_dl/extractor/drtv.py b/youtube_dl/extractor/drtv.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/extractor/drtv.py b/youtube_dl/extractor/drtv.py
index 9d6ce1f48..93b3c9f36 100644
--- a/youtube_dl/extractor/drtv.py
+++ b/youtube_dl/extractor/drtv.py
@@ -1,7 +1,5 @@
 from __future__ import unicode_literals
 
-import re
-
 from .subtitles import SubtitlesInfoExtractor
 from .common import ExtractorError
 from ..utils import parse_iso8601
@@ -25,8 +23,7 @@ class DRTVIE(SubtitlesInfoExtractor):
     }
 
     def _real_extract(self, url):
-        mobj = re.match(self._VALID_URL, url)
-        video_id = mobj.group('id')
+        video_id = self._match_id(url)
 
         programcard = self._download_json(
             'http://www.dr.dk/mu/programcard/expanded/%s' % video_id, video_id, 'Downloading video JSON')
@@ -35,7 +32,7 @@ class DRTVIE(SubtitlesInfoExtractor):
 
         title = data['Title']
         description = data['Description']
-        timestamp = parse_iso8601(data['CreatedTime'][:-5])
+        timestamp = parse_iso8601(data['CreatedTime'])
 
         thumbnail = None
         duration = None
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 6c0c39ca5..2864e5142 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -925,7 +925,7 @@ def parse_iso8601(date_str, delimiter='T'):
         return None
 
     m = re.search(
-        r'Z$| ?(?P<sign>\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$',
+        r'(\.[0-9]+)?(?:Z$| ?(?P<sign>\+|-)(?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})$)',
         date_str)
     if not m:
         timezone = datetime.timedelta()
@@ -938,7 +938,7 @@ def parse_iso8601(date_str, delimiter='T'):
             timezone = datetime.timedelta(
                 hours=sign * int(m.group('hours')),
                 minutes=sign * int(m.group('minutes')))
-    date_format =  '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)
+    date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)
     dt = datetime.datetime.strptime(date_str, date_format) - timezone
     return calendar.timegm(dt.timetuple())
 
"
"youtube-dl","17","4bf22f7a1014c55e3358b5a419945071b152eafc","5b232f46dcbdc805507c02edd4fd598f31d544d5","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index fdf5e29e7..c9cbd5842 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2733,6 +2733,8 @@ def cli_option(params, command_option, param):
 
 def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):
     param = params.get(param)
+    if param is None:
+        return []
     assert isinstance(param, bool)
     if separator:
         return [command_option + separator + (true_value if param else false_value)]
"
"youtube-dl","28","bd1512d19649c280197729814766d590ea6c023b","7aefc49c4013efb5056b2c1237e22c52cb5d3c49","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index d39f313a4..b7013a6aa 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -396,7 +396,11 @@ def _htmlentity_transform(entity):
             numstr = '0%s' % numstr
         else:
             base = 10
-        return compat_chr(int(numstr, base))
+        # See https://github.com/rg3/youtube-dl/issues/7518
+        try:
+            return compat_chr(int(numstr, base))
+        except ValueError:
+            pass
 
     # Unknown entity in name, return its literal representation
     return ('&%s;' % entity)
"
"youtube-dl","20","b6c9fe416243373bcb59eb8aa5ef0baca8f3c97c","609ff8ca19f1c4c168a81121074b91cc0f0d4c47","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 39860e9d1..fdf5e29e7 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -365,9 +365,9 @@ def get_elements_by_attribute(attribute, value, html, escape_value=True):
     retlist = []
     for m in re.finditer(r'''(?xs)
         <([a-zA-Z0-9:._-]+)
-         (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'))*?
+         (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'|))*?
          \s+%s=['""]?%s['""]?
-         (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'))*?
+         (?:\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=""[^""]*""|='[^']*'|))*?
         \s*>
         (?P<content>.*?)
         </\1>
"
"youtube-dl","37","98b7cf1acefe398f792ca6ff4c5f84f1b7785fcb","676eb3f2dd542be3e84780b18388253382d3e465","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 5f1f664c8..92fee966f 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2,6 +2,7 @@
 # -*- coding: utf-8 -*-
 
 import calendar
+import codecs
 import contextlib
 import ctypes
 import datetime
@@ -1263,9 +1264,11 @@ class PagedList(object):
 
 
 def uppercase_escape(s):
+    unicode_escape = codecs.getdecoder('unicode_escape')
     return re.sub(
         r'\\U[0-9a-fA-F]{8}',
-        lambda m: m.group(0).decode('unicode-escape'), s)
+        lambda m: unicode_escape(m.group(0))[0],
+        s)
 
 try:
     struct.pack(u'!I', 0)
"
"youtube-dl","27","d631d5f9f27f93767226192e4288990413fa9dbd","db2fe38b5508cbd28b89893219d9cccd41406851","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index ee20c3d9b..5b396ede8 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1982,9 +1982,9 @@ def parse_dfxp_time_expr(time_expr):
     if mobj:
         return float(mobj.group('time_offset'))
 
-    mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:\.\d+)?)$', time_expr)
+    mobj = re.match(r'^(\d+):(\d\d):(\d\d(?:(?:\.|:)\d+)?)$', time_expr)
     if mobj:
-        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3))
+        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))
 
 
 def srt_subtitles_timecode(seconds):
"
"youtube-dl","12","435e382423f860aca82a58d7c3db58cbfa242b40","e118a8794ffe5a3a414afd489726f34d753b0b23","youtube_dl/YoutubeDL.py","youtube_dl/YoutubeDL.py","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py","test/test_YoutubeDL.py","","diff --git a/youtube_dl/YoutubeDL.py b/youtube_dl/YoutubeDL.py
index a827414dc..80ed8d7e5 100755
--- a/youtube_dl/YoutubeDL.py
+++ b/youtube_dl/YoutubeDL.py
@@ -1078,7 +1078,7 @@ class YoutubeDL(object):
                 comparison_value = m.group('value')
                 str_op = STR_OPERATORS[m.group('op')]
                 if m.group('negation'):
-                    op = lambda attr, value: not str_op
+                    op = lambda attr, value: not str_op(attr, value)
                 else:
                     op = str_op
 
"
"youtube-dl","29","c514b0ec655b23e7804eb18df04daa863d973f32","6a750402787dfc1f39a9ad347f2d78ae1c94c52c","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 7b3f79141..d39f313a4 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -911,7 +911,8 @@ def unified_strdate(date_str, day_first=True):
         timetuple = email.utils.parsedate_tz(date_str)
         if timetuple:
             upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')
-    return compat_str(upload_date)
+    if upload_date is not None:
+        return compat_str(upload_date)
 
 
 def determine_ext(url, default_ext='unknown_video'):
"
"youtube-dl","34","07e764439a1cdd3a3b95fbf21acc6a517c6a889e","410f3e73ab268f74a455798ee39de5caba90caea","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index f05747097..59851a8c0 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -673,6 +673,8 @@ class ExtractorError(Exception):
             expected = True
         if video_id is not None:
             msg = video_id + ': ' + msg
+        if cause:
+            msg += u' (caused by %r)' % cause
         if not expected:
             msg = msg + u'; please report this issue on https://yt-dl.org/bug . Be sure to call youtube-dl with the --verbose flag and include its complete output. Make sure you are using the latest version; type  youtube-dl -U  to update.'
         super(ExtractorError, self).__init__(msg)
@@ -1598,7 +1600,9 @@ def js_to_json(code):
             ([{,]\s*)
             (""[^""]*""|\'[^\']*\'|[a-z0-9A-Z]+)
             (:\s*)
-            ([0-9.]+|true|false|""[^""]*""|\'[^\']*\'|\[|\{)
+            ([0-9.]+|true|false|""[^""]*""|\'[^\']*\'|
+                (?=\[|\{)
+            )
         ''', fix_kv, code)
     res = re.sub(r',(\s*\])', lambda m: m.group(1), res)
     return res
"
"youtube-dl","4","bc40b3a5ba44006c23daf7fe0ed872af5e33bdc5","189935f15960300d316e8b07108b076ac6c2186a","youtube_dl/jsinterp.py","youtube_dl/jsinterp.py","diff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py","test/test_jsinterp.py","","diff --git a/youtube_dl/jsinterp.py b/youtube_dl/jsinterp.py
index 9737f7002..a8df4aef0 100644
--- a/youtube_dl/jsinterp.py
+++ b/youtube_dl/jsinterp.py
@@ -198,12 +198,12 @@ class JSInterpreter(object):
             return opfunc(x, y)
 
         m = re.match(
-            r'^(?P<func>%s)\((?P<args>[a-zA-Z0-9_$,]+)\)$' % _NAME_RE, expr)
+            r'^(?P<func>%s)\((?P<args>[a-zA-Z0-9_$,]*)\)$' % _NAME_RE, expr)
         if m:
             fname = m.group('func')
             argvals = tuple([
                 int(v) if v.isdigit() else local_vars[v]
-                for v in m.group('args').split(',')])
+                for v in m.group('args').split(',')]) if len(m.group('args')) > 0 else tuple()
             if fname not in self._functions:
                 self._functions[fname] = self.extract_function(fname)
             return self._functions[fname](argvals)
"
"youtube-dl","26","4c93ee8d14dc081d413304d2d2eb694cb62cc71a","47212f7bcbd59af40f91796562a6b72ba0439ac4","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index c8308ba3a..82f67f6cd 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1970,7 +1970,7 @@ def js_to_json(code):
         '(?:[^'\\]*(?:\\\\|\\['""nurtbfx/\n]))*[^'\\]*'|
         /\*.*?\*/|,(?=\s*[\]}])|
         [a-zA-Z_][.a-zA-Z_0-9]*|
-        (?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|
+        \b(?:0[xX][0-9a-fA-F]+|0+[0-7]+)(?:\s*:)?|
         [0-9]+(?=\s*:)
         ''', fix_kv, code)
 
"
"youtube-dl","5","b02b960c6bba834d9e7199ac53430c7933079dc8","7dc2a74e0ac9cfa74cc9de6f586ffd5cc8bac0d9","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index f5cd6819b..97ddd9883 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -1101,7 +1101,7 @@ def unified_timestamp(date_str, day_first=True):
 
     date_str = date_str.replace(',', ' ')
 
-    pm_delta = datetime.timedelta(hours=12 if re.search(r'(?i)PM', date_str) else 0)
+    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0
     timezone, date_str = extract_timezone(date_str)
 
     # Remove AM/PM + timezone
@@ -1109,13 +1109,13 @@ def unified_timestamp(date_str, day_first=True):
 
     for expression in date_formats(day_first):
         try:
-            dt = datetime.datetime.strptime(date_str, expression) - timezone + pm_delta
+            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)
             return calendar.timegm(dt.timetuple())
         except ValueError:
             pass
     timetuple = email.utils.parsedate_tz(date_str)
     if timetuple:
-        return calendar.timegm(timetuple.timetuple())
+        return calendar.timegm(timetuple) + pm_delta * 3600
 
 
 def determine_ext(url, default_ext='unknown_video'):
"
"youtube-dl","40","6a7a38967976ea0d0b911c2965aaa74bed2976d7","b53466e1680db3d710415329674c887d38af46c5","youtube_dl/downloader/f4m.py;youtube_dl/utils.py","youtube_dl/downloader/f4m.py;youtube_dl/utils.py","diff --git a/youtube_dl/downloader/f4m.py b/youtube_dl/downloader/f4m.py;diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_utils.py","","diff --git a/youtube_dl/downloader/f4m.py b/youtube_dl/downloader/f4m.py
index 9a6c03556..052751106 100644
--- a/youtube_dl/downloader/f4m.py
+++ b/youtube_dl/downloader/f4m.py
@@ -4,13 +4,14 @@ import base64
 import io
 import itertools
 import os
-from struct import unpack, pack
 import time
 import xml.etree.ElementTree as etree
 
 from .common import FileDownloader
 from .http import HttpFD
 from ..utils import (
+    struct_pack,
+    struct_unpack,
     compat_urllib_request,
     compat_urlparse,
     format_bytes,
@@ -27,13 +28,13 @@ class FlvReader(io.BytesIO):
 
     # Utility functions for reading numbers and strings
     def read_unsigned_long_long(self):
-        return unpack('!Q', self.read(8))[0]
+        return struct_unpack('!Q', self.read(8))[0]
 
     def read_unsigned_int(self):
-        return unpack('!I', self.read(4))[0]
+        return struct_unpack('!I', self.read(4))[0]
 
     def read_unsigned_char(self):
-        return unpack('!B', self.read(1))[0]
+        return struct_unpack('!B', self.read(1))[0]
 
     def read_string(self):
         res = b''
@@ -196,7 +197,7 @@ def write_flv_header(stream, metadata):
     # Script data
     stream.write(b'\x12')
     # Size of the metadata with 3 bytes
-    stream.write(pack('!L', len(metadata))[1:])
+    stream.write(struct_pack('!L', len(metadata))[1:])
     stream.write(b'\x00\x00\x00\x00\x00\x00\x00')
     stream.write(metadata)
     # Magic numbers extracted from the output files produced by AdobeHDS.php
diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 67c6af507..dd03f058f 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -17,6 +17,7 @@ import platform
 import re
 import ssl
 import socket
+import struct
 import subprocess
 import sys
 import traceback
@@ -1220,3 +1221,20 @@ def uppercase_escape(s):
     return re.sub(
         r'\\U([0-9a-fA-F]{8})',
         lambda m: compat_chr(int(m.group(1), base=16)), s)
+
+try:
+    struct.pack(u'!I', 0)
+except TypeError:
+    # In Python 2.6 (and some 2.7 versions), struct requires a bytes argument
+    def struct_pack(spec, *args):
+        if isinstance(spec, compat_str):
+            spec = spec.encode('ascii')
+        return struct.pack(spec, *args)
+
+    def struct_unpack(spec, *args):
+        if isinstance(spec, compat_str):
+            spec = spec.encode('ascii')
+        return struct.unpack(spec, *args)
+else:
+    struct_pack = struct.pack
+    struct_unpack = struct.unpack
"
"youtube-dl","24","2c6da7df4a4d69ec933688e3c53795fd3436a1c6","e5a088dc4be4fdcc96927a9f1b7284d4cd49c415","youtube_dl/utils.py","youtube_dl/utils.py","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py","test/test_YoutubeDL.py","","diff --git a/youtube_dl/utils.py b/youtube_dl/utils.py
index 2770c5f1c..1a5ce8688 100644
--- a/youtube_dl/utils.py
+++ b/youtube_dl/utils.py
@@ -2345,11 +2345,18 @@ def _match_one(filter_part, dct):
     m = operator_rex.search(filter_part)
     if m:
         op = COMPARISON_OPERATORS[m.group('op')]
-        if m.group('strval') is not None:
+        actual_value = dct.get(m.group('key'))
+        if (m.group('strval') is not None or
+            # If the original field is a string and matching comparisonvalue is
+            # a number we should respect the origin of the original field
+            # and process comparison value as a string (see
+            # https://github.com/rg3/youtube-dl/issues/11082).
+            actual_value is not None and m.group('intval') is not None and
+                isinstance(actual_value, compat_str)):
             if m.group('op') not in ('=', '!='):
                 raise ValueError(
                     'Operator %s does not support string values!' % m.group('op'))
-            comparison_value = m.group('strval')
+            comparison_value = m.group('strval') or m.group('intval')
         else:
             try:
                 comparison_value = int(m.group('intval'))
@@ -2361,7 +2368,6 @@ def _match_one(filter_part, dct):
                     raise ValueError(
                         'Invalid integer value %r in filter part %r' % (
                             m.group('intval'), filter_part))
-        actual_value = dct.get(m.group('key'))
         if actual_value is None:
             return m.group('none_inclusive')
         return op(actual_value, comparison_value)
"
"youtube-dl","36","173a7026d59bacfbfe7a8eea92e10ef6e89d1798","a6da7b6b9657f621a927cb4c7bc46cf7c6c27b11","youtube_dl/extractor/facebook.py","youtube_dl/extractor/facebook.py","diff --git a/youtube_dl/extractor/facebook.py b/youtube_dl/extractor/facebook.py","test/test_all_urls.py","","diff --git a/youtube_dl/extractor/facebook.py b/youtube_dl/extractor/facebook.py
index f0cd8f156..f7cf700b5 100644
--- a/youtube_dl/extractor/facebook.py
+++ b/youtube_dl/extractor/facebook.py
@@ -20,7 +20,7 @@ from ..utils import (
 class FacebookIE(InfoExtractor):
     _VALID_URL = r'''(?x)
         https?://(?:\w+\.)?facebook\.com/
-        (?:[^#?]*\#!/)?
+        (?:[^#]*?\#!/)?
         (?:video/video\.php|photo\.php|video/embed)\?(?:.*?)
         (?:v|video_id)=(?P<id>[0-9]+)
         (?:.*)'''
"
"scrapy","22","a35aec71e96b0c0288c370afa425e8e700dca8b3","bb2cf7c0d7199fffe0aa100e5c8a51c6b4b82fc2","scrapy/exporters.py","scrapy/exporters.py","diff --git a/scrapy/exporters.py b/scrapy/exporters.py","tests/test_exporters.py","","diff --git a/scrapy/exporters.py b/scrapy/exporters.py
index 35f50838..360007c0 100644
--- a/scrapy/exporters.py
+++ b/scrapy/exporters.py
@@ -144,8 +144,10 @@ class XmlItemExporter(BaseItemExporter):
         elif is_listlike(serialized_value):
             for value in serialized_value:
                 self._export_xml_field('value', value)
-        else:
+        elif isinstance(serialized_value, six.text_type):
             self._xg_characters(serialized_value)
+        else:
+            self._xg_characters(str(serialized_value))
         self.xg.endElement(name)
 
     # Workaround for http://bugs.python.org/issue17606
"
"scrapy","18","41588397c04356f2b0c393b61ed68271a08d6ccd","cabed6f183cfb2ab778c57be8c75802fec5e54d4","scrapy/responsetypes.py","scrapy/responsetypes.py","diff --git a/scrapy/responsetypes.py b/scrapy/responsetypes.py","tests/test_responsetypes.py","","diff --git a/scrapy/responsetypes.py b/scrapy/responsetypes.py
index 4880cc7b..c667b141 100644
--- a/scrapy/responsetypes.py
+++ b/scrapy/responsetypes.py
@@ -59,7 +59,8 @@ class ResponseTypes(object):
 
     def from_content_disposition(self, content_disposition):
         try:
-            filename = to_native_str(content_disposition).split(';')[1].split('=')[1]
+            filename = to_native_str(content_disposition,
+                encoding='latin-1', errors='replace').split(';')[1].split('=')[1]
             filename = filename.strip('""\'')
             return self.from_filename(filename)
         except IndexError:
"
"scrapy","14","b7553d921afe356ec858bb1d2e5b1702df05ea24","d43a35735a062a4260b002cfbcd3236c77ef9399","scrapy/utils/gz.py","scrapy/utils/gz.py","diff --git a/scrapy/utils/gz.py b/scrapy/utils/gz.py","tests/test_utils_gz.py","","diff --git a/scrapy/utils/gz.py b/scrapy/utils/gz.py
index d035f9fd..cfb65214 100644
--- a/scrapy/utils/gz.py
+++ b/scrapy/utils/gz.py
@@ -7,7 +7,7 @@ except ImportError:
 from gzip import GzipFile
 
 import six
-
+import re
 
 # - Python>=3.5 GzipFile's read() has issues returning leftover
 #   uncompressed data when input is corrupted
@@ -50,8 +50,9 @@ def gunzip(data):
                 raise
     return output
 
+_is_gzipped_re = re.compile(br'^application/(x-)?gzip\b', re.I)
 
 def is_gzipped(response):
     """"""Return True if the response is gzipped, or False otherwise""""""
     ctype = response.headers.get('Content-Type', b'')
-    return ctype in (b'application/x-gzip', b'application/gzip')
+    return _is_gzipped_re.search(ctype) is not None
"
"scrapy","3","be2e910dd06ba4904e7b10eb5a7e3251e8dab099","66cbceeb0a9104fc0fa238898e38d0d9ce9cbcf6","scrapy/downloadermiddlewares/redirect.py","scrapy/downloadermiddlewares/redirect.py","diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py","tests/test_downloadermiddleware_redirect.py","","diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py
index 49468a2e..b73f864d 100644
--- a/scrapy/downloadermiddlewares/redirect.py
+++ b/scrapy/downloadermiddlewares/redirect.py
@@ -1,5 +1,5 @@
 import logging
-from six.moves.urllib.parse import urljoin
+from six.moves.urllib.parse import urljoin, urlparse
 
 from w3lib.url import safe_url_string
 
@@ -70,7 +70,10 @@ class RedirectMiddleware(BaseRedirectMiddleware):
         if 'Location' not in response.headers or response.status not in allowed_status:
             return response
 
-        location = safe_url_string(response.headers['location'])
+        location = safe_url_string(response.headers['Location'])
+        if response.headers['Location'].startswith(b'//'):
+            request_scheme = urlparse(request.url).scheme
+            location = request_scheme + '://' + location.lstrip('/')
 
         redirected_url = urljoin(request.url, location)
 
"
"scrapy","16","73a5571c6044d5aea47b4f973e325c2f3d4e25dc","68dedf54cb27847f6d035099b61aa06226549fad","scrapy/utils/url.py","scrapy/utils/url.py","diff --git a/scrapy/utils/url.py b/scrapy/utils/url.py","tests/test_utils_url.py","","diff --git a/scrapy/utils/url.py b/scrapy/utils/url.py
index adef4a80..71651364 100644
--- a/scrapy/utils/url.py
+++ b/scrapy/utils/url.py
@@ -7,15 +7,18 @@ to the w3lib.url module. Always import those from there instead.
 """"""
 import posixpath
 import re
+import six
 from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,
                                     urlparse, parse_qsl, urlencode,
-                                    unquote)
+                                    quote, unquote)
+if six.PY3:
+    from urllib.parse import unquote_to_bytes
 
 # scrapy.utils.url was moved to w3lib.url and import * ensures this
 # move doesn't break old code
 from w3lib.url import *
 from w3lib.url import _safe_chars
-from scrapy.utils.python import to_native_str
+from scrapy.utils.python import to_bytes, to_native_str, to_unicode
 
 
 def url_is_from_any_domain(url, domains):
@@ -37,42 +40,114 @@ def url_has_any_extension(url, extensions):
     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions
 
 
+def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):
+    return (
+        to_native_str(parts.scheme),
+        to_native_str(parts.netloc.encode('idna')),
+
+        # default encoding for path component SHOULD be UTF-8
+        quote(to_bytes(parts.path, path_encoding), _safe_chars),
+        quote(to_bytes(parts.params, path_encoding), _safe_chars),
+
+        # encoding of query and fragment follows page encoding
+        # or form-charset (if known and passed)
+        quote(to_bytes(parts.query, encoding), _safe_chars),
+        quote(to_bytes(parts.fragment, encoding), _safe_chars)
+    )
+
+
 def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,
                      encoding=None):
     """"""Canonicalize the given url by applying the following procedures:
 
     - sort query arguments, first by key, then by value
-    - percent encode paths and query arguments. non-ASCII characters are
-      percent-encoded using UTF-8 (RFC-3986)
+    - percent encode paths ; non-ASCII characters are percent-encoded
+      using UTF-8 (RFC-3986)
+    - percent encode query arguments ; non-ASCII characters are percent-encoded
+      using passed `encoding` (UTF-8 by default)
     - normalize all spaces (in query arguments) '+' (plus symbol)
     - normalize percent encodings case (%2f -> %2F)
-    - remove query arguments with blank values (unless keep_blank_values is True)
-    - remove fragments (unless keep_fragments is True)
+    - remove query arguments with blank values (unless `keep_blank_values` is True)
+    - remove fragments (unless `keep_fragments` is True)
 
-    The url passed can be a str or unicode, while the url returned is always a
-    str.
+    The url passed can be bytes or unicode, while the url returned is
+    always a native str (bytes in Python 2, unicode in Python 3).
 
     For examples see the tests in tests/test_utils_url.py
     """"""
+    # If supplied `encoding` is not compatible with all characters in `url`,
+    # fallback to UTF-8 as safety net.
+    # UTF-8 can handle all Unicode characters,
+    # so we should be covered regarding URL normalization,
+    # if not for proper URL expected by remote website.
+    try:
+        scheme, netloc, path, params, query, fragment = _safe_ParseResult(
+            parse_url(url), encoding=encoding)
+    except UnicodeError as e:
+        if encoding != 'utf8':
+            scheme, netloc, path, params, query, fragment = _safe_ParseResult(
+                parse_url(url), encoding='utf8')
+        else:
+            raise
 
-    scheme, netloc, path, params, query, fragment = parse_url(url)
-    keyvals = parse_qsl(query, keep_blank_values)
+    # 1. decode query-string as UTF-8 (or keep raw bytes),
+    #    sort values,
+    #    and percent-encode them back
+    if not six.PY2:
+        # Python3's urllib.parse.parse_qsl does not work as wanted
+        # for percent-encoded characters that do not match passed encoding,
+        # they get lost.
+        #
+        # e.g., 'q=b%a3' becomes [('q', 'b\ufffd')]
+        # (ie. with 'REPLACEMENT CHARACTER' (U+FFFD),
+        #      instead of \xa3 that you get with Python2's parse_qsl)
+        #
+        # what we want here is to keep raw bytes, and percent encode them
+        # so as to preserve whatever encoding what originally used.
+        #
+        # See https://tools.ietf.org/html/rfc3987#section-6.4:
+        #
+        # For example, it is possible to have a URI reference of
+        # ""http://www.example.org/r%E9sum%E9.xml#r%C3%A9sum%C3%A9"", where the
+        # document name is encoded in iso-8859-1 based on server settings, but
+        # where the fragment identifier is encoded in UTF-8 according to
+        # [XPointer]. The IRI corresponding to the above URI would be (in XML
+        # notation)
+        # ""http://www.example.org/r%E9sum%E9.xml#r&#xE9;sum&#xE9;"".
+        # Similar considerations apply to query parts.  The functionality of
+        # IRIs (namely, to be able to include non-ASCII characters) can only be
+        # used if the query part is encoded in UTF-8.
+        keyvals = parse_qsl_to_bytes(query, keep_blank_values)
+    else:
+        keyvals = parse_qsl(query, keep_blank_values)
     keyvals.sort()
     query = urlencode(keyvals)
 
-    # XXX: copied from w3lib.url.safe_url_string to add encoding argument
-    # path = to_native_str(path, encoding)
-    # path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'
+    # 2. decode percent-encoded sequences in path as UTF-8 (or keep raw bytes)
+    #    and percent-encode path again (this normalizes to upper-case %XX)
+    uqp = _unquotepath(path)
+    path = quote(uqp, _safe_chars) or '/'
 
-    path = safe_url_string(_unquotepath(path)) or '/'
     fragment = '' if not keep_fragments else fragment
+
+    # every part should be safe already
     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))
 
 
 def _unquotepath(path):
     for reserved in ('2f', '2F', '3f', '3F'):
         path = path.replace('%' + reserved, '%25' + reserved.upper())
-    return unquote(path)
+
+    if six.PY3:
+        # standard lib's unquote() does not work in Python 3
+        # for non-UTF-8 percent-escaped characters, they get lost.
+        # e.g., '%a3' becomes 'REPLACEMENT CHARACTER' (U+FFFD)
+        #
+        # unquote_to_bytes() returns raw bytes instead
+        return unquote_to_bytes(path)
+    else:
+        # in Python 2, '%a3' becomes '\xa3', which is what we want
+        return unquote(path)
 
 
 def parse_url(url, encoding=None):
@@ -81,7 +156,60 @@ def parse_url(url, encoding=None):
     """"""
     if isinstance(url, ParseResult):
         return url
-    return urlparse(to_native_str(url, encoding))
+    return urlparse(to_unicode(url, encoding))
+
+
+if six.PY3:
+    from urllib.parse import _coerce_args, unquote_to_bytes
+
+    def parse_qsl_to_bytes(qs, keep_blank_values=False, strict_parsing=False):
+        """"""Parse a query given as a string argument.
+
+        Data are returned as a list of name, value pairs as bytes.
+
+        Arguments:
+
+        qs: percent-encoded query string to be parsed
+
+        keep_blank_values: flag indicating whether blank values in
+            percent-encoded queries should be treated as blank strings.  A
+            true value indicates that blanks should be retained as blank
+            strings.  The default false value indicates that blank values
+            are to be ignored and treated as if they were  not included.
+
+        strict_parsing: flag indicating what to do with parsing errors. If
+            false (the default), errors are silently ignored. If true,
+            errors raise a ValueError exception.
+
+        """"""
+        # This code is the same as Python3's parse_qsl()
+        # (at https://hg.python.org/cpython/rev/c38ac7ab8d9a)
+        # except for the unquote(s, encoding, errors) calls replaced
+        # with unquote_to_bytes(s)
+        qs, _coerce_result = _coerce_args(qs)
+        pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]
+        r = []
+        for name_value in pairs:
+            if not name_value and not strict_parsing:
+                continue
+            nv = name_value.split('=', 1)
+            if len(nv) != 2:
+                if strict_parsing:
+                    raise ValueError(""bad query field: %r"" % (name_value,))
+                # Handle case of a control-name with no equal sign
+                if keep_blank_values:
+                    nv.append('')
+                else:
+                    continue
+            if len(nv[1]) or keep_blank_values:
+                name = nv[0].replace('+', ' ')
+                name = unquote_to_bytes(name)
+                name = _coerce_result(name)
+                value = nv[1].replace('+', ' ')
+                value = unquote_to_bytes(value)
+                value = _coerce_result(value)
+                r.append((name, value))
+        return r
 
 
 def escape_ajax(url):
"
"scrapy","2","f02c3d1dcf3e4880388d19e961e7911be5dc54ff","439a3e59b8e858441f8d97dbc32f398db392330d","scrapy/utils/datatypes.py","scrapy/utils/datatypes.py","diff --git a/scrapy/utils/datatypes.py b/scrapy/utils/datatypes.py","tests/test_utils_datatypes.py","","diff --git a/scrapy/utils/datatypes.py b/scrapy/utils/datatypes.py
index df2b99c2..f7e3240c 100644
--- a/scrapy/utils/datatypes.py
+++ b/scrapy/utils/datatypes.py
@@ -315,8 +315,9 @@ class LocalCache(collections.OrderedDict):
         self.limit = limit
 
     def __setitem__(self, key, value):
-        while len(self) >= self.limit:
-            self.popitem(last=False)
+        if self.limit:
+            while len(self) >= self.limit:
+                self.popitem(last=False)
         super(LocalCache, self).__setitem__(key, value)
 
 
"
"scrapy","38","6cc6bbb5fc5c102271829a554772effb0444023c","6c3970e6722191b642fd99c6c1bfed0d93010cab","scrapy/http/request/form.py","scrapy/http/request/form.py","diff --git a/scrapy/http/request/form.py b/scrapy/http/request/form.py","tests/test_http_request.py","","diff --git a/scrapy/http/request/form.py b/scrapy/http/request/form.py
index d9d178a3..95b38e99 100644
--- a/scrapy/http/request/form.py
+++ b/scrapy/http/request/form.py
@@ -170,9 +170,8 @@ def _get_clickable(clickdata, form):
     """"""
     clickables = [
         el for el in form.xpath(
-            'descendant::*[(self::input or self::button)'
-            ' and re:test(@type, ""^submit$"", ""i"")]'
-            '|descendant::button[not(@type)]',
+            'descendant::input[re:test(@type, ""^(submit|image)$"", ""i"")]'
+            '|descendant::button[not(@type) or re:test(@type, ""^submit$"", ""i"")]',
             namespaces={""re"": ""http://exslt.org/regular-expressions""})
         ]
     if not clickables:
"
"scrapy","8","f2f9350c47db73bdfb60773601b59b0633e1595a","4e765acaed7a914630ee5320fa6f6523890a2b9d","scrapy/item.py","scrapy/item.py","diff --git a/scrapy/item.py b/scrapy/item.py","tests/test_item.py","","diff --git a/scrapy/item.py b/scrapy/item.py
index 138728a9..aa05e9c6 100644
--- a/scrapy/item.py
+++ b/scrapy/item.py
@@ -25,6 +25,7 @@ class Field(dict):
 class ItemMeta(ABCMeta):
 
     def __new__(mcs, class_name, bases, attrs):
+        classcell = attrs.pop('__classcell__', None)
         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))
         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)
 
@@ -39,6 +40,8 @@ class ItemMeta(ABCMeta):
 
         new_attrs['fields'] = fields
         new_attrs['_class'] = _class
+        if classcell is not None:
+            new_attrs['__classcell__'] = classcell
         return super(ItemMeta, mcs).__new__(mcs, class_name, bases, new_attrs)
 
 
"
"scrapy","23","c9e046d11dc63fbdc40effce4e6d15bcecd44593","f042ad0f39594d59a1a2032e6294ff1890638138","scrapy/downloadermiddlewares/httpproxy.py;tests/test_downloadermiddleware_retry.py","scrapy/downloadermiddlewares/httpproxy.py;tests/test_downloadermiddleware_retry.py","diff --git a/scrapy/downloadermiddlewares/httpproxy.py b/scrapy/downloadermiddlewares/httpproxy.py;diff --git a/tests/test_downloadermiddleware_retry.py b/tests/test_downloadermiddleware_retry.py","tests/test_downloadermiddleware_httpproxy.py","","diff --git a/scrapy/downloadermiddlewares/httpproxy.py b/scrapy/downloadermiddlewares/httpproxy.py
index dda6a3d2..8c3514fd 100644
--- a/scrapy/downloadermiddlewares/httpproxy.py
+++ b/scrapy/downloadermiddlewares/httpproxy.py
@@ -9,7 +9,7 @@ from six.moves.urllib.parse import urlunparse
 
 from scrapy.utils.httpobj import urlparse_cached
 from scrapy.exceptions import NotConfigured
-
+from scrapy.utils.python import to_bytes
 
 class HttpProxyMiddleware(object):
 
@@ -26,7 +26,7 @@ class HttpProxyMiddleware(object):
         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))
 
         if user:
-            user_pass = '%s:%s' % (unquote(user), unquote(password))
+            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))
             creds = base64.b64encode(user_pass).strip()
         else:
             creds = None
@@ -52,4 +52,4 @@ class HttpProxyMiddleware(object):
         creds, proxy = self.proxies[scheme]
         request.meta['proxy'] = proxy
         if creds:
-            request.headers['Proxy-Authorization'] = 'Basic ' + creds
+            request.headers['Proxy-Authorization'] = b'Basic ' + creds
diff --git a/tests/test_downloadermiddleware_retry.py b/tests/test_downloadermiddleware_retry.py
index 20561e77..3de9399c 100644
--- a/tests/test_downloadermiddleware_retry.py
+++ b/tests/test_downloadermiddleware_retry.py
@@ -21,20 +21,20 @@ class RetryTest(unittest.TestCase):
 
     def test_priority_adjust(self):
         req = Request('http://www.scrapytest.org/503')
-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)
+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
         req2 = self.mw.process_response(req, rsp, self.spider)
         assert req2.priority < req.priority
 
     def test_404(self):
         req = Request('http://www.scrapytest.org/404')
-        rsp = Response('http://www.scrapytest.org/404', body='', status=404)
+        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)
 
         # dont retry 404s
         assert self.mw.process_response(req, rsp, self.spider) is rsp
 
     def test_dont_retry(self):
         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})
-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)
+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
 
         # first retry
         r = self.mw.process_response(req, rsp, self.spider)
@@ -56,7 +56,7 @@ class RetryTest(unittest.TestCase):
 
     def test_503(self):
         req = Request('http://www.scrapytest.org/503')
-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)
+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)
 
         # first retry
         req = self.mw.process_response(req, rsp, self.spider)
"
"scrapy","1","c57512fa669e6f6b1b766a7639206a380f0d10ce","9d9dea0d69709ef0f7aef67ddba1bd7bda25d273","scrapy/spidermiddlewares/offsite.py","scrapy/spidermiddlewares/offsite.py","diff --git a/scrapy/spidermiddlewares/offsite.py b/scrapy/spidermiddlewares/offsite.py","tests/test_spidermiddleware_offsite.py","","diff --git a/scrapy/spidermiddlewares/offsite.py b/scrapy/spidermiddlewares/offsite.py
index 232e96cb..36f80969 100644
--- a/scrapy/spidermiddlewares/offsite.py
+++ b/scrapy/spidermiddlewares/offsite.py
@@ -54,12 +54,16 @@ class OffsiteMiddleware(object):
         if not allowed_domains:
             return re.compile('')  # allow all by default
         url_pattern = re.compile(""^https?://.*$"")
+        domains = []
         for domain in allowed_domains:
-            if url_pattern.match(domain):
+            if domain is None:
+                continue
+            elif url_pattern.match(domain):
                 message = (""allowed_domains accepts only domains, not URLs. ""
                            ""Ignoring URL entry %s in allowed_domains."" % domain)
                 warnings.warn(message, URLWarning)
-        domains = [re.escape(d) for d in allowed_domains if d is not None]
+            else:
+                domains.append(re.escape(domain))
         regex = r'^(.*\.)?(%s)$' % '|'.join(domains)
         return re.compile(regex)
 
"
"scrapy","31","5f02ef82e8560242eb34b336f385addfdef3211d","dba7e39f61cbe2c22d3c9064f32f6e36d74f14b2","scrapy/http/cookies.py","scrapy/http/cookies.py","diff --git a/scrapy/http/cookies.py b/scrapy/http/cookies.py","tests/test_downloadermiddleware_cookies.py","","diff --git a/scrapy/http/cookies.py b/scrapy/http/cookies.py
index 740f21d2..e92c3fe7 100644
--- a/scrapy/http/cookies.py
+++ b/scrapy/http/cookies.py
@@ -149,11 +149,13 @@ class WrappedRequest(object):
         return name in self.request.headers
 
     def get_header(self, name, default=None):
-        return to_native_str(self.request.headers.get(name, default))
+        return to_native_str(self.request.headers.get(name, default),
+                             errors='replace')
 
     def header_items(self):
         return [
-            (to_native_str(k), [to_native_str(x) for x in v])
+            (to_native_str(k, errors='replace'),
+             [to_native_str(x, errors='replace') for x in v])
             for k, v in self.request.headers.items()
         ]
 
@@ -171,6 +173,7 @@ class WrappedResponse(object):
 
     # python3 cookiejars calls get_all
     def get_all(self, name, default=None):
-        return [to_native_str(v) for v in self.response.headers.getlist(name)]
+        return [to_native_str(v, errors='replace')
+                for v in self.response.headers.getlist(name)]
     # python2 cookiejars calls getheaders
     getheaders = get_all
"
"scrapy","39","692975acb40c6394424dfb728b1ffa46b3b3c55d","a1e8a8525d2312842c7e1cca8ba6e4e1a83084b7","scrapy/spiders/__init__.py","scrapy/spiders/__init__.py","diff --git a/scrapy/spiders/__init__.py b/scrapy/spiders/__init__.py","tests/test_spider.py","","diff --git a/scrapy/spiders/__init__.py b/scrapy/spiders/__init__.py
index 138cdbc6..80768b1b 100644
--- a/scrapy/spiders/__init__.py
+++ b/scrapy/spiders/__init__.py
@@ -66,11 +66,14 @@ class Spider(object_ref):
         crawler.signals.connect(self.close, signals.spider_closed)
 
     def start_requests(self):
-        if self.make_requests_from_url is not Spider.make_requests_from_url:
+        cls = self.__class__
+        if cls.make_requests_from_url is not Spider.make_requests_from_url:
             warnings.warn(
-                ""Spider.make_requests_from_url method is deprecated; ""
-                ""it won't be called in future Scrapy releases. ""
-                ""Please override start_requests method instead.""
+                ""Spider.make_requests_from_url method is deprecated; it ""
+                ""won't be called in future Scrapy releases. Please ""
+                ""override Spider.start_requests method instead (see %s.%s)."" % (
+                    cls.__module__, cls.__name__
+                ),
             )
             for url in self.start_urls:
                 yield self.make_requests_from_url(url)
"
"scrapy","7","e1ceaf3b5fa29326f032c4ed3f50943384b9e63d","074caf434e255bc96f106e57e3e288028f372485","scrapy/http/request/form.py","scrapy/http/request/form.py","diff --git a/scrapy/http/request/form.py b/scrapy/http/request/form.py","tests/test_http_request.py","","diff --git a/scrapy/http/request/form.py b/scrapy/http/request/form.py
index 2862dc09..905d8412 100644
--- a/scrapy/http/request/form.py
+++ b/scrapy/http/request/form.py
@@ -5,10 +5,13 @@ This module implements the FormRequest class which is a more convenient class
 See documentation in docs/topics/request-response.rst
 """"""
 
+import six
 from six.moves.urllib.parse import urljoin, urlencode
+
 import lxml.html
 from parsel.selector import create_root_node
-import six
+from w3lib.html import strip_html5_whitespace
+
 from scrapy.http.request import Request
 from scrapy.utils.python import to_bytes, is_listlike
 from scrapy.utils.response import get_base_url
@@ -51,7 +54,10 @@ class FormRequest(Request):
 
 def _get_form_url(form, url):
     if url is None:
-        return urljoin(form.base_url, form.action)
+        action = form.get('action')
+        if action is None:
+            return form.base_url
+        return urljoin(form.base_url, strip_html5_whitespace(action))
     return urljoin(form.base_url, url)
 
 
"
"scrapy","25","57f87b95d4d705f8afdd8fb9f7551033a7d88ee2","9548691fdd47077a53f85daace091ef4af599cb9","scrapy/http/request/form.py","scrapy/http/request/form.py","diff --git a/scrapy/http/request/form.py b/scrapy/http/request/form.py","tests/test_http_request.py","","diff --git a/scrapy/http/request/form.py b/scrapy/http/request/form.py
index a12a2fd0..4a9bd732 100644
--- a/scrapy/http/request/form.py
+++ b/scrapy/http/request/form.py
@@ -11,6 +11,7 @@ from parsel.selector import create_root_node
 import six
 from scrapy.http.request import Request
 from scrapy.utils.python import to_bytes, is_listlike
+from scrapy.utils.response import get_base_url
 
 
 class FormRequest(Request):
@@ -44,7 +45,7 @@ class FormRequest(Request):
 
 def _get_form_url(form, url):
     if url is None:
-        return form.action or form.base_url
+        return urljoin(form.base_url, form.action)
     return urljoin(form.base_url, url)
 
 
@@ -58,7 +59,7 @@ def _urlencode(seq, enc):
 def _get_form(response, formname, formid, formnumber, formxpath):
     """"""Find the form element """"""
     text = response.body_as_unicode()
-    root = create_root_node(text, lxml.html.HTMLParser, base_url=response.url)
+    root = create_root_node(text, lxml.html.HTMLParser, base_url=get_base_url(response))
     forms = root.xpath('//form')
     if not forms:
         raise ValueError(""No <form> element found in %s"" % response)
"
"scrapy","10","6cc83c041eac36e7688dbd9c4c55487a43b622e0","db408528928b2d15043593032913fe40d6eb6783","scrapy/downloadermiddlewares/redirect.py","scrapy/downloadermiddlewares/redirect.py","diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py","tests/test_downloadermiddleware_redirect.py","","diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py
index 4ed7e4c2..db276eef 100644
--- a/scrapy/downloadermiddlewares/redirect.py
+++ b/scrapy/downloadermiddlewares/redirect.py
@@ -1,9 +1,10 @@
 import logging
 from six.moves.urllib.parse import urljoin
 
+from w3lib.url import safe_url_string
+
 from scrapy.http import HtmlResponse
 from scrapy.utils.response import get_meta_refresh
-from scrapy.utils.python import to_native_str
 from scrapy.exceptions import IgnoreRequest, NotConfigured
 
 logger = logging.getLogger(__name__)
@@ -65,8 +66,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):
         if 'Location' not in response.headers or response.status not in allowed_status:
             return response
 
-        # HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8
-        location = to_native_str(response.headers['location'].decode('latin1'))
+        location = safe_url_string(response.headers['location'])
 
         redirected_url = urljoin(request.url, location)
 
"
"scrapy","13","fa78849e335994a1617ed63221a70940c21cca20","414857a593ad5b82fa21d6344928f43f93dc9f14","scrapy/pipelines/images.py","scrapy/pipelines/images.py","diff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py","tests/test_pipeline_images.py","","diff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py
index 964541d6..af5825c0 100644
--- a/scrapy/pipelines/images.py
+++ b/scrapy/pipelines/images.py
@@ -42,7 +42,7 @@ class ImagesPipeline(FilesPipeline):
     # ImagesPipeline. They may be overridden by settings.
     MIN_WIDTH = 0
     MIN_HEIGHT = 0
-    EXPIRES = 0
+    EXPIRES = 90
     THUMBS = {}
     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'
     DEFAULT_IMAGES_RESULT_FIELD = 'images'
"
"scrapy","19","e328a9b9dfa4fbc79c59ed4f45f757e998301c31","1f743996ff00a7b728d59b93d0967e1eb50072f0","scrapy/http/cookies.py","scrapy/http/cookies.py","diff --git a/scrapy/http/cookies.py b/scrapy/http/cookies.py","tests/test_http_cookies.py","","diff --git a/scrapy/http/cookies.py b/scrapy/http/cookies.py
index e92c3fe7..a1e95102 100644
--- a/scrapy/http/cookies.py
+++ b/scrapy/http/cookies.py
@@ -137,13 +137,29 @@ class WrappedRequest(object):
         """"""
         return self.request.meta.get('is_unverifiable', False)
 
-    # python3 uses request.unverifiable
+    def get_origin_req_host(self):
+        return urlparse_cached(self.request).hostname
+
+    # python3 uses attributes instead of methods
+    @property
+    def full_url(self):
+        return self.get_full_url()
+
+    @property
+    def host(self):
+        return self.get_host()
+
+    @property
+    def type(self):
+        return self.get_type()
+
     @property
     def unverifiable(self):
         return self.is_unverifiable()
 
-    def get_origin_req_host(self):
-        return urlparse_cached(self.request).hostname
+    @property
+    def origin_req_host(self):
+        return self.get_origin_req_host()
 
     def has_header(self, name):
         return name in self.request.headers
"
"scrapy","9","a9c69458ff1667cc4d20ba25bb6a0dd9a5a08ce6","ff3aec661355a82a6f77355a95e1f391fa586c2b","scrapy/mail.py","scrapy/mail.py","diff --git a/scrapy/mail.py b/scrapy/mail.py","tests/test_mail.py","","diff --git a/scrapy/mail.py b/scrapy/mail.py
index c6339f25..0bb39552 100644
--- a/scrapy/mail.py
+++ b/scrapy/mail.py
@@ -21,6 +21,8 @@ else:
 
 from twisted.internet import defer, reactor, ssl
 
+from .utils.misc import arg_to_iter
+
 logger = logging.getLogger(__name__)
 
 
@@ -48,6 +50,10 @@ class MailSender(object):
             msg = MIMEMultipart()
         else:
             msg = MIMENonMultipart(*mimetype.split('/', 1))
+
+        to = list(arg_to_iter(to))
+        cc = list(arg_to_iter(cc))
+
         msg['From'] = self.mailfrom
         msg['To'] = COMMASPACE.join(to)
         msg['Date'] = formatdate(localtime=True)
"
"scrapy","21","43a53aca1207a82b663fe7a90c375546ce340a8e","a8a6f050e71fbb7881076a8d6e2867e868d26016","scrapy/downloadermiddlewares/robotstxt.py","scrapy/downloadermiddlewares/robotstxt.py","diff --git a/scrapy/downloadermiddlewares/robotstxt.py b/scrapy/downloadermiddlewares/robotstxt.py","tests/test_downloadermiddleware_robotstxt.py","","diff --git a/scrapy/downloadermiddlewares/robotstxt.py b/scrapy/downloadermiddlewares/robotstxt.py
index d4a33dc3..698f394a 100644
--- a/scrapy/downloadermiddlewares/robotstxt.py
+++ b/scrapy/downloadermiddlewares/robotstxt.py
@@ -101,4 +101,6 @@ class RobotsTxtMiddleware(object):
         rp_dfd.callback(rp)
 
     def _robots_error(self, failure, netloc):
-        self._parsers.pop(netloc).callback(None)
+        rp_dfd = self._parsers[netloc]
+        self._parsers[netloc] = None
+        rp_dfd.callback(None)
"
"scrapy","30","4d41cc0dc4821da07d467b368528c61ce48a0df2","3e6d6c43ac0763adf2cd92efdb4a1dc2ba165440","scrapy/cmdline.py;scrapy/utils/testproc.py;tests/test_cmdline/__init__.py","scrapy/cmdline.py;scrapy/utils/testproc.py;tests/test_cmdline/__init__.py","diff --git a/scrapy/cmdline.py b/scrapy/cmdline.py;diff --git a/scrapy/utils/testproc.py b/scrapy/utils/testproc.py;diff --git a/tests/test_cmdline/__init__.py b/tests/test_cmdline/__init__.py","tests/test_command_version.py","","diff --git a/scrapy/cmdline.py b/scrapy/cmdline.py
index a619c349..35050c13 100644
--- a/scrapy/cmdline.py
+++ b/scrapy/cmdline.py
@@ -18,10 +18,10 @@ def _iter_command_classes(module_name):
     # TODO: add `name` attribute to commands and and merge this function with
     # scrapy.utils.spider.iter_spider_classes
     for module in walk_modules(module_name):
-        for obj in vars(module).itervalues():
+        for obj in vars(module).values():
             if inspect.isclass(obj) and \
-               issubclass(obj, ScrapyCommand) and \
-               obj.__module__ == module.__name__:
+                    issubclass(obj, ScrapyCommand) and \
+                    obj.__module__ == module.__name__:
                 yield obj
 
 def _get_commands_from_module(module, inproject):
diff --git a/scrapy/utils/testproc.py b/scrapy/utils/testproc.py
index adddad09..f268e91f 100644
--- a/scrapy/utils/testproc.py
+++ b/scrapy/utils/testproc.py
@@ -35,8 +35,8 @@ class TestProcessProtocol(protocol.ProcessProtocol):
 
     def __init__(self):
         self.deferred = defer.Deferred()
-        self.out = ''
-        self.err = ''
+        self.out = b''
+        self.err = b''
         self.exitcode = None
 
     def outReceived(self, data):
diff --git a/tests/test_cmdline/__init__.py b/tests/test_cmdline/__init__.py
index 00fce2fb..28ba7682 100644
--- a/tests/test_cmdline/__init__.py
+++ b/tests/test_cmdline/__init__.py
@@ -11,10 +11,11 @@ class CmdlineTest(unittest.TestCase):
         self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'
 
     def _execute(self, *new_args, **kwargs):
+        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'
         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args
         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)
-        comm = proc.communicate()
-        return comm[0].strip()
+        comm = proc.communicate()[0].strip()
+        return comm.decode(encoding)
 
     def test_default_settings(self):
         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \
"
"scrapy","15","b7925e42202d79d2ba9d00b6aded3a451c92fe81","1aec5200bc81493623f2a4e077b4e80e104e47d5","scrapy/utils/url.py","scrapy/utils/url.py","diff --git a/scrapy/utils/url.py b/scrapy/utils/url.py","tests/test_utils_url.py","","diff --git a/scrapy/utils/url.py b/scrapy/utils/url.py
index c80fc6e7..406eb584 100644
--- a/scrapy/utils/url.py
+++ b/scrapy/utils/url.py
@@ -41,9 +41,16 @@ def url_has_any_extension(url, extensions):
 
 
 def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):
+    # IDNA encoding can fail for too long labels (>63 characters)
+    # or missing labels (e.g. http://.example.com)
+    try:
+        netloc = parts.netloc.encode('idna')
+    except UnicodeError:
+        netloc = parts.netloc
+
     return (
         to_native_str(parts.scheme),
-        to_native_str(parts.netloc.encode('idna')),
+        to_native_str(netloc),
 
         # default encoding for path component SHOULD be UTF-8
         quote(to_bytes(parts.path, path_encoding), _safe_chars),
"
"scrapy","32","342cb622f1ea93268477da557099010bbd72529a","aa6a72707daabfb6217f52e4774f2ff038f83dcc","scrapy/crawler.py","scrapy/crawler.py","diff --git a/scrapy/crawler.py b/scrapy/crawler.py","tests/test_crawler.py","","diff --git a/scrapy/crawler.py b/scrapy/crawler.py
index 9b35b364..2cd65827 100644
--- a/scrapy/crawler.py
+++ b/scrapy/crawler.py
@@ -209,8 +209,8 @@ class CrawlerProcess(CrawlerRunner):
     def __init__(self, settings):
         super(CrawlerProcess, self).__init__(settings)
         install_shutdown_handlers(self._signal_shutdown)
-        configure_logging(settings)
-        log_scrapy_info(settings)
+        configure_logging(self.settings)
+        log_scrapy_info(self.settings)
 
     def _signal_shutdown(self, signum, _):
         install_shutdown_handlers(self._signal_kill)
"
"scrapy","11","241bd00e76df142a24699819f8496bdec8f5c83a","9de6f1ca757b7f200d15e94840c9d431cf202276","scrapy/utils/gz.py","scrapy/utils/gz.py","diff --git a/scrapy/utils/gz.py b/scrapy/utils/gz.py","tests/test_utils_gz.py","","diff --git a/scrapy/utils/gz.py b/scrapy/utils/gz.py
index afc7ed12..73c2eb73 100644
--- a/scrapy/utils/gz.py
+++ b/scrapy/utils/gz.py
@@ -43,7 +43,7 @@ def gunzip(data):
             # contains the whole page content
             if output or getattr(f, 'extrabuf', None):
                 try:
-                    output += f.extrabuf
+                    output += f.extrabuf[-f.extrasize:]
                 finally:
                     break
             else:
"
"scrapy","6","8aa2e4f9976d31bbe3a0014b4f1f96ace1b87043","25f609e2a3c27ca7d7d98dbfddb2c049735935bb","scrapy/pipelines/images.py","scrapy/pipelines/images.py","diff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py","tests/test_pipeline_images.py","","diff --git a/scrapy/pipelines/images.py b/scrapy/pipelines/images.py
index 5796bfb8..bc449431 100644
--- a/scrapy/pipelines/images.py
+++ b/scrapy/pipelines/images.py
@@ -132,6 +132,11 @@ class ImagesPipeline(FilesPipeline):
             background = Image.new('RGBA', image.size, (255, 255, 255))
             background.paste(image, image)
             image = background.convert('RGB')
+        elif image.mode == 'P':
+            image = image.convert(""RGBA"")
+            background = Image.new('RGBA', image.size, (255, 255, 255))
+            background.paste(image, image)
+            image = background.convert('RGB')
         elif image.mode != 'RGB':
             image = image.convert('RGB')
 
"
"scrapy","35","0a5bbbaed3e182d2151b6b34357667901ece353f","c3d3a9491412d2a91b0927a05908593dcd329e4a","scrapy/crawler.py","scrapy/crawler.py","diff --git a/scrapy/crawler.py b/scrapy/crawler.py","tests/test_crawler.py","","diff --git a/scrapy/crawler.py b/scrapy/crawler.py
index dab1043e..b4706919 100644
--- a/scrapy/crawler.py
+++ b/scrapy/crawler.py
@@ -192,8 +192,8 @@ def _get_spider_loader(settings):
             'Please use SPIDER_LOADER_CLASS.',
             category=ScrapyDeprecationWarning, stacklevel=2
         )
-    cls_path = settings.get('SPIDER_LOADER_CLASS',
-                            settings.get('SPIDER_MANAGER_CLASS'))
+    cls_path = settings.get('SPIDER_MANAGER_CLASS',
+                            settings.get('SPIDER_LOADER_CLASS'))
     loader_cls = load_object(cls_path)
     verifyClass(ISpiderLoader, loader_cls)
     return loader_cls.from_settings(settings.frozencopy())
"
"scrapy","33","2d216771976cd7aedfb88ae97418c2d4dcc84917","6dccb3a9b320a8d0808764ac8e6e88e663e2d52c","scrapy/core/engine.py;scrapy/core/scraper.py;scrapy/downloadermiddlewares/robotstxt.py;scrapy/extensions/feedexport.py;scrapy/log.py;scrapy/pipelines/files.py;scrapy/pipelines/media.py;scrapy/utils/signal.py","scrapy/core/engine.py;scrapy/core/scraper.py;scrapy/downloadermiddlewares/robotstxt.py;scrapy/extensions/feedexport.py;scrapy/log.py;scrapy/pipelines/files.py;scrapy/pipelines/media.py;scrapy/utils/signal.py","diff --git a/scrapy/core/engine.py b/scrapy/core/engine.py;diff --git a/scrapy/core/scraper.py b/scrapy/core/scraper.py;diff --git a/scrapy/downloadermiddlewares/robotstxt.py b/scrapy/downloadermiddlewares/robotstxt.py;diff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py;diff --git a/scrapy/log.py b/scrapy/log.py;diff --git a/scrapy/pipelines/files.py b/scrapy/pipelines/files.py;diff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py;diff --git a/scrapy/utils/signal.py b/scrapy/utils/signal.py","tests/test_pipeline_media.py;tests/test_utils_log.py","","diff --git a/scrapy/core/engine.py b/scrapy/core/engine.py
index 40f19e4c..992327bf 100644
--- a/scrapy/core/engine.py
+++ b/scrapy/core/engine.py
@@ -16,7 +16,7 @@ from scrapy.exceptions import DontCloseSpider
 from scrapy.http import Response, Request
 from scrapy.utils.misc import load_object
 from scrapy.utils.reactor import CallLaterOnce
-from scrapy.utils.log import logformatter_adapter
+from scrapy.utils.log import logformatter_adapter, failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -135,13 +135,16 @@ class ExecutionEngine(object):
         d = self._download(request, spider)
         d.addBoth(self._handle_downloader_output, request, spider)
         d.addErrback(lambda f: logger.info('Error while handling downloader output',
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
+                                           extra={'spider': spider}))
         d.addBoth(lambda _: slot.remove_request(request))
         d.addErrback(lambda f: logger.info('Error while removing request from slot',
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
+                                           extra={'spider': spider}))
         d.addBoth(lambda _: slot.nextcall.schedule())
         d.addErrback(lambda f: logger.info('Error while scheduling new request',
-                                           extra={'spider': spider, 'failure': f}))
+                                           exc_info=failure_to_exc_info(f),
+                                           extra={'spider': spider}))
         return d
 
     def _handle_downloader_output(self, response, request, spider):
@@ -153,7 +156,8 @@ class ExecutionEngine(object):
         # response is a Response or Failure
         d = self.scraper.enqueue_scrape(response, request, spider)
         d.addErrback(lambda f: logger.error('Error while enqueuing downloader output',
-                                            extra={'spider': spider, 'failure': f}))
+                                            exc_info=failure_to_exc_info(f),
+                                            extra={'spider': spider}))
         return d
 
     def spider_is_idle(self, spider):
@@ -268,7 +272,11 @@ class ExecutionEngine(object):
 
         def log_failure(msg):
             def errback(failure):
-                logger.error(msg, extra={'spider': spider, 'failure': failure})
+                logger.error(
+                    msg,
+                    exc_info=failure_to_exc_info(failure),
+                    extra={'spider': spider}
+                )
             return errback
 
         dfd.addBoth(lambda _: self.downloader.close())
diff --git a/scrapy/core/scraper.py b/scrapy/core/scraper.py
index e5d8acea..244499be 100644
--- a/scrapy/core/scraper.py
+++ b/scrapy/core/scraper.py
@@ -10,7 +10,7 @@ from twisted.internet import defer
 from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback
 from scrapy.utils.spider import iterate_spider_output
 from scrapy.utils.misc import load_object
-from scrapy.utils.log import logformatter_adapter
+from scrapy.utils.log import logformatter_adapter, failure_to_exc_info
 from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest
 from scrapy import signals
 from scrapy.http import Request, Response
@@ -107,7 +107,8 @@ class Scraper(object):
         dfd.addErrback(
             lambda f: logger.error('Scraper bug processing %(request)s',
                                    {'request': request},
-                                   extra={'spider': spider, 'failure': f}))
+                                   exc_info=failure_to_exc_info(f),
+                                   extra={'spider': spider}))
         self._scrape_next(spider, slot)
         return dfd
 
@@ -153,7 +154,8 @@ class Scraper(object):
         logger.error(
             ""Spider error processing %(request)s (referer: %(referer)s)"",
             {'request': request, 'referer': referer},
-            extra={'spider': spider, 'failure': _failure}
+            exc_info=failure_to_exc_info(_failure),
+            extra={'spider': spider}
         )
         self.signals.send_catch_log(
             signal=signals.spider_error,
@@ -202,7 +204,8 @@ class Scraper(object):
             if download_failure.frames:
                 logger.error('Error downloading %(request)s',
                              {'request': request},
-                             extra={'spider': spider, 'failure': download_failure})
+                             exc_info=failure_to_exc_info(download_failure),
+                             extra={'spider': spider})
             else:
                 errmsg = download_failure.getErrorMessage()
                 if errmsg:
@@ -227,7 +230,8 @@ class Scraper(object):
                     spider=spider, exception=output.value)
             else:
                 logger.error('Error processing %(item)s', {'item': item},
-                             extra={'spider': spider, 'failure': output})
+                             exc_info=failure_to_exc_info(output),
+                             extra={'spider': spider})
         else:
             logkws = self.logformatter.scraped(output, response, spider)
             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})
diff --git a/scrapy/downloadermiddlewares/robotstxt.py b/scrapy/downloadermiddlewares/robotstxt.py
index 9083482f..77e08b7e 100644
--- a/scrapy/downloadermiddlewares/robotstxt.py
+++ b/scrapy/downloadermiddlewares/robotstxt.py
@@ -11,6 +11,7 @@ from six.moves.urllib import robotparser
 from scrapy.exceptions import NotConfigured, IgnoreRequest
 from scrapy.http import Request
 from scrapy.utils.httpobj import urlparse_cached
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -59,7 +60,8 @@ class RobotsTxtMiddleware(object):
         if failure.type is not IgnoreRequest:
             logger.error(""Error downloading %(request)s: %(f_exception)s"",
                          {'request': request, 'f_exception': failure.value},
-                         extra={'spider': spider, 'failure': failure})
+                         exc_info=failure_to_exc_info(failure),
+                         extra={'spider': spider})
 
     def _parse_robots(self, response):
         rp = robotparser.RobotFileParser(response.url)
diff --git a/scrapy/extensions/feedexport.py b/scrapy/extensions/feedexport.py
index 7c6849a7..3bc1c92c 100644
--- a/scrapy/extensions/feedexport.py
+++ b/scrapy/extensions/feedexport.py
@@ -22,6 +22,7 @@ from scrapy.utils.ftp import ftp_makedirs_cwd
 from scrapy.exceptions import NotConfigured
 from scrapy.utils.misc import load_object
 from scrapy.utils.python import get_func_args
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -184,7 +185,8 @@ class FeedExporter(object):
         d.addCallback(lambda _: logger.info(logfmt % ""Stored"", log_args,
                                             extra={'spider': spider}))
         d.addErrback(lambda f: logger.error(logfmt % ""Error storing"", log_args,
-                                            extra={'spider': spider, 'failure': f}))
+                                            exc_info=failure_to_exc_info(f),
+                                            extra={'spider': spider}))
         return d
 
     def item_scraped(self, item, spider):
diff --git a/scrapy/log.py b/scrapy/log.py
index c3f9c422..5dabe569 100644
--- a/scrapy/log.py
+++ b/scrapy/log.py
@@ -1,51 +1,166 @@
-""""""
-This module is kept to provide a helpful warning about its removal.
-""""""
+# -*- coding: utf-8 -*-
 
+import sys
 import logging
 import warnings
+from logging.config import dictConfig
 
 from twisted.python.failure import Failure
+from twisted.python import log as twisted_log
 
+import scrapy
+from scrapy.settings import overridden_settings, Settings
 from scrapy.exceptions import ScrapyDeprecationWarning
 
 logger = logging.getLogger(__name__)
 
-warnings.warn(""Module `scrapy.log` has been deprecated, Scrapy now relies on ""
-              ""the builtin Python library for logging. Read the updated ""
-              ""logging entry in the documentation to learn more."",
-              ScrapyDeprecationWarning, stacklevel=2)
 
+def failure_to_exc_info(failure):
+    """"""Extract exc_info from Failure instances""""""
+    if isinstance(failure, Failure):
+        return (failure.type, failure.value, failure.tb)
 
-# Imports kept for backwards-compatibility
 
-DEBUG = logging.DEBUG
-INFO = logging.INFO
-WARNING = logging.WARNING
-ERROR = logging.ERROR
-CRITICAL = logging.CRITICAL
-SILENT = CRITICAL + 1
+class TopLevelFormatter(logging.Filter):
+    """"""Keep only top level loggers's name (direct children from root) from
+    records.
 
+    This filter will replace Scrapy loggers' names with 'scrapy'. This mimics
+    the old Scrapy log behaviour and helps shortening long names.
 
-def msg(message=None, _level=logging.INFO, **kw):
-    warnings.warn('log.msg has been deprecated, create a python logger and '
-                  'log through it instead',
-                  ScrapyDeprecationWarning, stacklevel=2)
+    Since it can't be set for just one logger (it won't propagate for its
+    children), it's going to be set in the root handler, with a parametrized
+    `loggers` list where it should act.
+    """"""
 
-    level = kw.pop('level', _level)
-    message = kw.pop('format', message)
-    # NOTE: logger.log doesn't handle well passing empty dictionaries with format
-    # arguments because of some weird use-case:
-    # https://hg.python.org/cpython/file/648dcafa7e5f/Lib/logging/__init__.py#l269
-    logger.log(level, message, *[kw] if kw else [])
+    def __init__(self, loggers=None):
+        self.loggers = loggers or []
 
+    def filter(self, record):
+        if any(record.name.startswith(l + '.') for l in self.loggers):
+            record.name = record.name.split('.', 1)[0]
+        return True
 
-def err(_stuff=None, _why=None, **kw):
-    warnings.warn('log.err has been deprecated, create a python logger and '
-                  'use its error method instead',
-                  ScrapyDeprecationWarning, stacklevel=2)
 
-    level = kw.pop('level', logging.ERROR)
-    failure = kw.pop('failure', _stuff) or Failure()
-    message = kw.pop('why', _why) or failure.value
-    logger.log(level, message, *[kw] if kw else [], extra={'failure': failure})
+DEFAULT_LOGGING = {
+    'version': 1,
+    'disable_existing_loggers': False,
+    'loggers': {
+        'scrapy': {
+            'level': 'DEBUG',
+        },
+        'twisted': {
+            'level': 'ERROR',
+        },
+    }
+}
+
+
+def configure_logging(settings=None):
+    """"""Initialize and configure default loggers
+
+    This function does:
+      - Route warnings and twisted logging through Python standard logging
+      - Set FailureFormatter filter on Scrapy logger
+      - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively
+      - Create a handler for the root logger according to given settings
+    """"""
+    if not sys.warnoptions:
+        # Route warnings through python logging
+        logging.captureWarnings(True)
+
+    observer = twisted_log.PythonLoggingObserver('twisted')
+    observer.start()
+
+    dictConfig(DEFAULT_LOGGING)
+
+    if isinstance(settings, dict):
+        settings = Settings(settings)
+
+    if settings:
+        logging.root.setLevel(logging.NOTSET)
+
+        if settings.getbool('LOG_STDOUT'):
+            sys.stdout = StreamLogger(logging.getLogger('stdout'))
+
+        # Set up the default log handler
+        filename = settings.get('LOG_FILE')
+        if filename:
+            encoding = settings.get('LOG_ENCODING')
+            handler = logging.FileHandler(filename, encoding=encoding)
+        elif settings.getbool('LOG_ENABLED'):
+            handler = logging.StreamHandler()
+        else:
+            handler = logging.NullHandler()
+
+        formatter = logging.Formatter(
+            fmt=settings.get('LOG_FORMAT'),
+            datefmt=settings.get('LOG_DATEFORMAT')
+        )
+        handler.setFormatter(formatter)
+        handler.setLevel(settings.get('LOG_LEVEL'))
+        handler.addFilter(TopLevelFormatter(['scrapy']))
+        logging.root.addHandler(handler)
+
+
+def log_scrapy_info(settings):
+    logger.info(""Scrapy %(version)s started (bot: %(bot)s)"",
+                {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})
+
+    logger.info(""Optional features available: %(features)s"",
+                {'features': "", "".join(scrapy.optional_features)})
+
+    d = dict(overridden_settings(settings))
+    logger.info(""Overridden settings: %(settings)r"", {'settings': d})
+
+
+class StreamLogger(object):
+    """"""Fake file-like stream object that redirects writes to a logger instance
+
+    Taken from:
+        http://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/
+    """"""
+    def __init__(self, logger, log_level=logging.INFO):
+        self.logger = logger
+        self.log_level = log_level
+        self.linebuf = ''
+
+    def write(self, buf):
+        for line in buf.rstrip().splitlines():
+            self.logger.log(self.log_level, line.rstrip())
+
+
+class LogCounterHandler(logging.Handler):
+    """"""Record log levels count into a crawler stats""""""
+
+    def __init__(self, crawler, *args, **kwargs):
+        super(LogCounterHandler, self).__init__(*args, **kwargs)
+        self.crawler = crawler
+
+    def emit(self, record):
+        sname = 'log_count/{}'.format(record.levelname)
+        self.crawler.stats.inc_value(sname)
+
+
+def logformatter_adapter(logkws):
+    """"""
+    Helper that takes the dictionary output from the methods in LogFormatter
+    and adapts it into a tuple of positional arguments for logger.log calls,
+    handling backward compatibility as well.
+    """"""
+    if not {'level', 'msg', 'args'} <= set(logkws):
+        warnings.warn('Missing keys in LogFormatter method',
+                      ScrapyDeprecationWarning)
+
+    if 'format' in logkws:
+        warnings.warn('`format` key in LogFormatter methods has been '
+                      'deprecated, use `msg` instead',
+                      ScrapyDeprecationWarning)
+
+    level = logkws.get('level', logging.INFO)
+    message = logkws.get('format', logkws.get('msg'))
+    # NOTE: This also handles 'args' being an empty dict, that case doesn't
+    # play well in logger.log calls
+    args = logkws if not logkws.get('args') else logkws['args']
+
+    return (level, message, args)
diff --git a/scrapy/pipelines/files.py b/scrapy/pipelines/files.py
index c0192b86..250f46ad 100644
--- a/scrapy/pipelines/files.py
+++ b/scrapy/pipelines/files.py
@@ -25,6 +25,7 @@ from scrapy.pipelines.media import MediaPipeline
 from scrapy.exceptions import NotConfigured, IgnoreRequest
 from scrapy.http import Request
 from scrapy.utils.misc import md5sum
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -212,7 +213,8 @@ class FilesPipeline(MediaPipeline):
         dfd.addErrback(
             lambda f:
             logger.error(self.__class__.__name__ + '.store.stat_file',
-                         extra={'spider': info.spider, 'failure': f})
+                         exc_info=failure_to_exc_info(f),
+                         extra={'spider': info.spider})
         )
         return dfd
 
diff --git a/scrapy/pipelines/media.py b/scrapy/pipelines/media.py
index 55ef05ad..21b8b898 100644
--- a/scrapy/pipelines/media.py
+++ b/scrapy/pipelines/media.py
@@ -8,6 +8,7 @@ from twisted.python.failure import Failure
 from scrapy.utils.defer import mustbe_deferred, defer_result
 from scrapy.utils.request import request_fingerprint
 from scrapy.utils.misc import arg_to_iter
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -70,7 +71,7 @@ class MediaPipeline(object):
         dfd.addCallback(self._check_media_to_download, request, info)
         dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)
         dfd.addErrback(lambda f: logger.error(
-            f.value, extra={'spider': info.spider, 'failure': f})
+            f.value, exc_info=failure_to_exc_info(f), extra={'spider': info.spider})
         )
         return dfd.addBoth(lambda _: wad)  # it must return wad at last
 
@@ -127,6 +128,7 @@ class MediaPipeline(object):
                     logger.error(
                         '%(class)s found errors processing %(item)s',
                         {'class': self.__class__.__name__, 'item': item},
-                        extra={'spider': info.spider, 'failure': value}
+                        exc_info=failure_to_exc_info(value),
+                        extra={'spider': info.spider}
                     )
         return item
diff --git a/scrapy/utils/signal.py b/scrapy/utils/signal.py
index d4cc4130..d9a59e16 100644
--- a/scrapy/utils/signal.py
+++ b/scrapy/utils/signal.py
@@ -8,6 +8,7 @@ from twisted.python.failure import Failure
 from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \
     getAllReceivers, disconnect
 from scrapy.xlib.pydispatch.robustapply import robustApply
+from scrapy.utils.log import failure_to_exc_info
 
 logger = logging.getLogger(__name__)
 
@@ -47,7 +48,8 @@ def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):
         if dont_log is None or not isinstance(failure.value, dont_log):
             logger.error(""Error caught on signal handler: %(receiver)s"",
                          {'receiver': recv},
-                         extra={'spider': spider, 'failure': failure})
+                         exc_info=failure_to_exc_info(failure),
+                         extra={'spider': spider})
         return failure
 
     dont_log = named.pop('dont_log', None)
"
"scrapy","17","ebef6d7c6dd8922210db8a4a44f48fe27ee0cd16","65c7c05060fd2d1fc161d4904243d5e0b31e202b","scrapy/utils/response.py","scrapy/utils/response.py","diff --git a/scrapy/utils/response.py b/scrapy/utils/response.py","tests/test_utils_response.py","","diff --git a/scrapy/utils/response.py b/scrapy/utils/response.py
index 73db2641..d9e95146 100644
--- a/scrapy/utils/response.py
+++ b/scrapy/utils/response.py
@@ -47,14 +47,8 @@ def get_meta_refresh(response):
 
 def response_status_message(status):
     """"""Return status code plus status text descriptive message
-
-    >>> response_status_message(200)
-    '200 OK'
-
-    >>> response_status_message(404)
-    '404 Not Found'
     """"""
-    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status))))
+    return '%s %s' % (status, to_native_str(http.RESPONSES.get(int(status), ""Unknown Status"")))
 
 
 def response_httprepr(response):
"
"scrapy","28","e2f31f3018c0037f65982209c22f93b80a5d6e7b","457b97c13ccf9a84f3dc7800c180cf059822c09a","scrapy/dupefilters.py","scrapy/dupefilters.py","diff --git a/scrapy/dupefilters.py b/scrapy/dupefilters.py","tests/test_dupefilters.py","","diff --git a/scrapy/dupefilters.py b/scrapy/dupefilters.py
index 739ba922..9d8966b9 100644
--- a/scrapy/dupefilters.py
+++ b/scrapy/dupefilters.py
@@ -36,6 +36,7 @@ class RFPDupeFilter(BaseDupeFilter):
         self.logger = logging.getLogger(__name__)
         if path:
             self.file = open(os.path.join(path, 'requests.seen'), 'a+')
+            self.file.seek(0)
             self.fingerprints.update(x.rstrip() for x in self.file)
 
     @classmethod
"
"scrapy","20","e328a9b9dfa4fbc79c59ed4f45f757e998301c31","25c56159b86288311630cc0cf6db9d755aeeff1e","scrapy/spiders/sitemap.py","scrapy/spiders/sitemap.py","diff --git a/scrapy/spiders/sitemap.py b/scrapy/spiders/sitemap.py","tests/test_spider.py","","diff --git a/scrapy/spiders/sitemap.py b/scrapy/spiders/sitemap.py
index eede467a..89d96c33 100644
--- a/scrapy/spiders/sitemap.py
+++ b/scrapy/spiders/sitemap.py
@@ -32,7 +32,7 @@ class SitemapSpider(Spider):
 
     def _parse_sitemap(self, response):
         if response.url.endswith('/robots.txt'):
-            for url in sitemap_urls_from_robots(response.body):
+            for url in sitemap_urls_from_robots(response.text):
                 yield Request(url, callback=self._parse_sitemap)
         else:
             body = self._get_sitemap_body(response)
"
"scrapy","37","1d5c270ce8caf954ce83c8db262e2a35707e0c5e","f701f5b0db10faef08e4ed9a21b98fd72f9cfc9a","scrapy/http/request/__init__.py","scrapy/http/request/__init__.py","diff --git a/scrapy/http/request/__init__.py b/scrapy/http/request/__init__.py","tests/test_http_request.py","","diff --git a/scrapy/http/request/__init__.py b/scrapy/http/request/__init__.py
index d09eaf84..76a42819 100644
--- a/scrapy/http/request/__init__.py
+++ b/scrapy/http/request/__init__.py
@@ -66,7 +66,7 @@ class Request(object_ref):
         s = safe_url_string(url, self.encoding)
         self._url = escape_ajax(s)
 
-        if ':' not in self._url:
+        if ('://' not in self._url) and (not self._url.startswith('data:')):
             raise ValueError('Missing scheme in request url: %s' % self._url)
 
     url = property(_get_url, obsolete_setter(_set_url, 'url'))
"
"scrapy","27","280eab241680c93a763a3ef3a9ccd0c257259ca0","d164398a27736f75286cc435eca69b06ff7c1c06","scrapy/downloadermiddlewares/redirect.py","scrapy/downloadermiddlewares/redirect.py","diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py","tests/test_downloadermiddleware_redirect.py","","diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py
index 363e56cb..3cf8d2be 100644
--- a/scrapy/downloadermiddlewares/redirect.py
+++ b/scrapy/downloadermiddlewares/redirect.py
@@ -55,7 +55,9 @@ class RedirectMiddleware(BaseRedirectMiddleware):
 
     def process_response(self, request, response, spider):
         if (request.meta.get('dont_redirect', False) or
-               response.status in getattr(spider, 'handle_httpstatus_list', [])):
+               response.status in getattr(spider, 'handle_httpstatus_list', []) or
+               response.status in request.meta.get('handle_httpstatus_list', []) or
+               request.meta.get('handle_httpstatus_all', False)):
             return response
 
         if request.method == 'HEAD':
"
"scrapy","12","34e7dadf38ba1796094c0c76e92ea8d9837681cc","2c9a38d1f54a12c33d7c9a19e021c840c4a32dee","scrapy/selector/unified.py","scrapy/selector/unified.py","diff --git a/scrapy/selector/unified.py b/scrapy/selector/unified.py","tests/test_selector.py","","diff --git a/scrapy/selector/unified.py b/scrapy/selector/unified.py
index 15f3d26d..64cb0232 100644
--- a/scrapy/selector/unified.py
+++ b/scrapy/selector/unified.py
@@ -46,6 +46,10 @@ class Selector(_ParselSelector, object_ref):
     selectorlist_cls = SelectorList
 
     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):
+        if not(response is None or text is None):
+           raise ValueError('%s.__init__() received both response and text'
+                            % self.__class__.__name__)
+
         st = _st(response, type or self._default_type)
 
         if _root is not None:
"
"scrapy","29","5c4666a3d489bd3efa2e188de58721a125a5bfad","8d45b3c4810cb5304ba1193b45697a0df1157326","scrapy/utils/request.py","scrapy/utils/request.py","diff --git a/scrapy/utils/request.py b/scrapy/utils/request.py","tests/test_utils_request.py","","diff --git a/scrapy/utils/request.py b/scrapy/utils/request.py
index ac415e50..0487d1e1 100644
--- a/scrapy/utils/request.py
+++ b/scrapy/utils/request.py
@@ -79,7 +79,7 @@ def request_httprepr(request):
     parsed = urlparse_cached(request)
     path = urlunparse(('', '', parsed.path or '/', parsed.params, parsed.query, ''))
     s = to_bytes(request.method) + b"" "" + to_bytes(path) + b"" HTTP/1.1\r\n""
-    s += b""Host: "" + to_bytes(parsed.hostname) + b""\r\n""
+    s += b""Host: "" + to_bytes(parsed.hostname or b'') + b""\r\n""
     if request.headers:
         s += request.headers.to_string() + b""\r\n""
     s += b""\r\n""
"
"scrapy","34","e521740b3951bacc80d3c9c1db8652ed5e914b89","773ea5a5ef76426dd91a8669542d2602082a5746","scrapy/item.py","scrapy/item.py","diff --git a/scrapy/item.py b/scrapy/item.py","tests/test_item.py","","diff --git a/scrapy/item.py b/scrapy/item.py
index 8ac27964..138728a9 100644
--- a/scrapy/item.py
+++ b/scrapy/item.py
@@ -6,6 +6,7 @@ See documentation in docs/topics/item.rst
 
 from pprint import pformat
 from collections import MutableMapping
+
 from abc import ABCMeta
 import six
 
@@ -27,7 +28,7 @@ class ItemMeta(ABCMeta):
         new_bases = tuple(base._class for base in bases if hasattr(base, '_class'))
         _class = super(ItemMeta, mcs).__new__(mcs, 'x_' + class_name, new_bases, attrs)
 
-        fields = {}
+        fields = getattr(_class, 'fields', {})
         new_attrs = {}
         for n in dir(_class):
             v = getattr(_class, n)
"
"scrapy","4","c8f3d07e86dd41074971b5423fb932c2eda6db1e","16dad81715d3970149c0cf7a318e73a0d84be1ff","scrapy/contracts/__init__.py","scrapy/contracts/__init__.py","diff --git a/scrapy/contracts/__init__.py b/scrapy/contracts/__init__.py","tests/test_contracts.py","","diff --git a/scrapy/contracts/__init__.py b/scrapy/contracts/__init__.py
index 5eaee3d1..8315d21d 100644
--- a/scrapy/contracts/__init__.py
+++ b/scrapy/contracts/__init__.py
@@ -84,7 +84,7 @@ class ContractsManager(object):
 
         def eb_wrapper(failure):
             case = _create_testcase(method, 'errback')
-            exc_info = failure.value, failure.type, failure.getTracebackObject()
+            exc_info = failure.type, failure.value, failure.getTracebackObject()
             results.addError(case, exc_info)
 
         request.callback = cb_wrapper
"
"scrapy","26","f249b309ab779b5ab518f54f309d7a4ac6661ec7","03f1720afb4a437314659a306286f440df664a0b","scrapy/settings/__init__.py","scrapy/settings/__init__.py","diff --git a/scrapy/settings/__init__.py b/scrapy/settings/__init__.py","tests/test_settings/__init__.py","","diff --git a/scrapy/settings/__init__.py b/scrapy/settings/__init__.py
index 3ae2187a..13656298 100644
--- a/scrapy/settings/__init__.py
+++ b/scrapy/settings/__init__.py
@@ -116,9 +116,9 @@ class BaseSettings(MutableMapping):
     def getbool(self, name, default=False):
         """"""
         Get a setting value as a boolean.
-        
+
         ``1``, ``'1'``, and ``True`` return ``True``, while ``0``, ``'0'``,
-        ``False`` and ``None`` return ``False``. 
+        ``False`` and ``None`` return ``False``.
 
         For example, settings populated through environment variables set to
         ``'0'`` will return ``False`` when using this method.
@@ -203,11 +203,17 @@ class BaseSettings(MutableMapping):
         if basename in self:
             warnings.warn('_BASE settings are deprecated.',
                           category=ScrapyDeprecationWarning)
-            compsett = BaseSettings(self[name + ""_BASE""], priority='default')
-            compsett.update(self[name])
+            # When users defined a _BASE setting, they explicitly don't want to
+            # use any of Scrapy's defaults. Therefore, we only use these entries
+            # from self[name] (where the defaults now live) that have a priority
+            # higher than 'default'
+            compsett = BaseSettings(self[basename], priority='default')
+            for k in self[name]:
+                prio = self[name].getpriority(k)
+                if prio > get_settings_priority('default'):
+                    compsett.set(k, self[name][k], prio)
             return compsett
-        else:
-            return self[name]
+        return self[name]
 
     def getpriority(self, name):
         """"""
"
"scrapy","5","426da0ed07637e7efbcfb0fe49546e187d5d7f67","acd2b8d43b5ebec7ffd364b6f335427041a0b98d","scrapy/http/response/__init__.py","scrapy/http/response/__init__.py","diff --git a/scrapy/http/response/__init__.py b/scrapy/http/response/__init__.py","tests/test_http_response.py","","diff --git a/scrapy/http/response/__init__.py b/scrapy/http/response/__init__.py
index 434d87ea..1974259b 100644
--- a/scrapy/http/response/__init__.py
+++ b/scrapy/http/response/__init__.py
@@ -120,6 +120,8 @@ class Response(object_ref):
         """"""
         if isinstance(url, Link):
             url = url.url
+        elif url is None:
+            raise ValueError(""url can't be None"")
         url = self.urljoin(url)
         return Request(url, callback,
                        method=method,
"
"scrapy","40","7d24df37380cd5a5b7394cd2534e240bd2eff0ca","f1d971a5c0cdfe0f4fe5619146cd6818324fc98e","scrapy/exporters.py","scrapy/exporters.py","diff --git a/scrapy/exporters.py b/scrapy/exporters.py","tests/test_exporters.py","","diff --git a/scrapy/exporters.py b/scrapy/exporters.py
index 145468db..c7c78d05 100644
--- a/scrapy/exporters.py
+++ b/scrapy/exporters.py
@@ -273,10 +273,10 @@ class PythonItemExporter(BaseItemExporter):
             return dict(self._serialize_dict(value))
         if is_listlike(value):
             return [self._serialize_value(v) for v in value]
-        if self.binary:
-            return to_bytes(value, encoding=self.encoding)
-        else:
-            return to_unicode(value, encoding=self.encoding)
+        encode_func = to_bytes if self.binary else to_unicode
+        if isinstance(value, (six.text_type, bytes)):
+            return encode_func(value, encoding=self.encoding)
+        return value
 
     def _serialize_dict(self, value):
         for key, val in six.iteritems(value):
"
"scrapy","24","98c060d0b2cc76934e16abc03a033f21850fd565","0f527849f2e8eddaf5d756b061699f2eca522a18","scrapy/core/downloader/handlers/http11.py","scrapy/core/downloader/handlers/http11.py","diff --git a/scrapy/core/downloader/handlers/http11.py b/scrapy/core/downloader/handlers/http11.py","tests/test_downloader_handlers.py","","diff --git a/scrapy/core/downloader/handlers/http11.py b/scrapy/core/downloader/handlers/http11.py
index d81093a9..729b80b0 100644
--- a/scrapy/core/downloader/handlers/http11.py
+++ b/scrapy/core/downloader/handlers/http11.py
@@ -78,7 +78,7 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):
     for it.
     """"""
 
-    _responseMatcher = re.compile('HTTP/1\.. 200')
+    _responseMatcher = re.compile(b'HTTP/1\.. 200')
 
     def __init__(self, reactor, host, port, proxyConf, contextFactory,
                  timeout=30, bindAddress=None):
@@ -92,11 +92,15 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):
 
     def requestTunnel(self, protocol):
         """"""Asks the proxy to open a tunnel.""""""
-        tunnelReq = 'CONNECT %s:%s HTTP/1.1\r\n' % (self._tunneledHost,
-                                                  self._tunneledPort)
+        tunnelReq = (
+            b'CONNECT ' +
+            to_bytes(self._tunneledHost, encoding='ascii') + b':' +
+            to_bytes(str(self._tunneledPort)) +
+            b' HTTP/1.1\r\n')
         if self._proxyAuthHeader:
-            tunnelReq += 'Proxy-Authorization: %s\r\n' % self._proxyAuthHeader
-        tunnelReq += '\r\n'
+            tunnelReq += \
+                b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\r\n'
+        tunnelReq += b'\r\n'
         protocol.transport.write(tunnelReq)
         self._protocolDataReceived = protocol.dataReceived
         protocol.dataReceived = self.processProxyResponse
"
"scrapy","36","cb8140a42a27ede87b0880372024f2f1804618b8","cf9be5344a89dd8e14f8241ec69de9c984ec1e05","scrapy/utils/misc.py","scrapy/utils/misc.py","diff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py","tests/test_utils_misc/__init__.py","","diff --git a/scrapy/utils/misc.py b/scrapy/utils/misc.py
index 52cfba20..ab7cf9de 100644
--- a/scrapy/utils/misc.py
+++ b/scrapy/utils/misc.py
@@ -137,17 +137,26 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):
     ``*args`` and ``**kwargs`` are forwarded to the constructors.
 
     Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.
+
+    Raises ``TypeError`` if the resulting instance is ``None`` (e.g. if an
+    extension has not been implemented correctly).
     """"""
     if settings is None:
         if crawler is None:
             raise ValueError(""Specify at least one of settings and crawler."")
         settings = crawler.settings
     if crawler and hasattr(objcls, 'from_crawler'):
-        return objcls.from_crawler(crawler, *args, **kwargs)
+        instance = objcls.from_crawler(crawler, *args, **kwargs)
+        method_name = 'from_crawler'
     elif hasattr(objcls, 'from_settings'):
-        return objcls.from_settings(settings, *args, **kwargs)
+        instance = objcls.from_settings(settings, *args, **kwargs)
+        method_name = 'from_settings'
     else:
-        return objcls(*args, **kwargs)
+        instance = objcls(*args, **kwargs)
+        method_name = '__new__'
+    if instance is None:
+        raise TypeError(""%s.%s returned None"" % (objcls.__qualname__, method_name))
+    return instance
 
 
 @contextmanager
"
"black","22","728c56c986bc5aea4d9897d3fce3159f89991b8e","c55d08d0b96c8de8bd867ca315e380d9e9d2d7ec","black.py","black.py","diff --git a/black.py b/black.py","tests/comments3.py;tests/test_black.py","","diff --git a/black.py b/black.py
index dab3f00..6499b22 100644
--- a/black.py
+++ b/black.py
@@ -3,14 +3,25 @@
 import asyncio
 from asyncio.base_events import BaseEventLoop
 from concurrent.futures import Executor, ProcessPoolExecutor
-from functools import partial
+from functools import partial, wraps
 import keyword
 import os
 from pathlib import Path
 import tokenize
 import sys
 from typing import (
-    Dict, Generic, Iterable, Iterator, List, Optional, Set, Tuple, Type, TypeVar, Union
+    Callable,
+    Dict,
+    Generic,
+    Iterable,
+    Iterator,
+    List,
+    Optional,
+    Set,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
 )
 
 from attr import dataclass, Factory
@@ -32,7 +43,9 @@ Depth = int
 NodeType = int
 LeafID = int
 Priority = int
+Index = int
 LN = Union[Leaf, Node]
+SplitFunc = Callable[['Line', bool], Iterator['Line']]
 out = partial(click.secho, bold=True, err=True)
 err = partial(click.secho, fg='red', err=True)
 
@@ -520,7 +533,7 @@ class Line:
 
     depth: int = 0
     leaves: List[Leaf] = Factory(list)
-    comments: Dict[LeafID, Leaf] = Factory(dict)
+    comments: List[Tuple[Index, Leaf]] = Factory(list)
     bracket_tracker: BracketTracker = Factory(BracketTracker)
     inside_brackets: bool = False
     has_for: bool = False
@@ -549,16 +562,31 @@ class Line:
             self.bracket_tracker.mark(leaf)
             self.maybe_remove_trailing_comma(leaf)
             self.maybe_increment_for_loop_variable(leaf)
-            if self.maybe_adapt_standalone_comment(leaf):
-                return
 
         if not self.append_comment(leaf):
             self.leaves.append(leaf)
 
+    def append_safe(self, leaf: Leaf, preformatted: bool = False) -> None:
+        """"""Like :func:`append()` but disallow invalid standalone comment structure.
+
+        Raises ValueError when any `leaf` is appended after a standalone comment
+        or when a standalone comment is not the first leaf on the line.
+        """"""
+        if self.bracket_tracker.depth == 0:
+            if self.is_comment:
+                raise ValueError(""cannot append to standalone comments"")
+
+            if self.leaves and leaf.type == STANDALONE_COMMENT:
+                raise ValueError(
+                    ""cannot append standalone comments to a populated line""
+                )
+
+        self.append(leaf, preformatted=preformatted)
+
     @property
     def is_comment(self) -> bool:
         """"""Is this line a standalone comment?""""""
-        return bool(self) and self.leaves[0].type == STANDALONE_COMMENT
+        return len(self.leaves) == 1 and self.leaves[0].type == STANDALONE_COMMENT
 
     @property
     def is_decorator(self) -> bool:
@@ -622,6 +650,15 @@ class Line:
             and self.leaves[0].value == 'yield'
         )
 
+    @property
+    def contains_standalone_comments(self) -> bool:
+        """"""If so, needs to be split before emitting.""""""
+        for leaf in self.leaves:
+            if leaf.type == STANDALONE_COMMENT:
+                return True
+
+        return False
+
     def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:
         """"""Remove trailing comma if there is one and it's safe.""""""
         if not (
@@ -632,13 +669,13 @@ class Line:
             return False
 
         if closing.type == token.RBRACE:
-            self.leaves.pop()
+            self.remove_trailing_comma()
             return True
 
         if closing.type == token.RSQB:
             comma = self.leaves[-1]
             if comma.parent and comma.parent.type == syms.listmaker:
-                self.leaves.pop()
+                self.remove_trailing_comma()
                 return True
 
         # For parens let's check if it's safe to remove the comma.  If the
@@ -666,7 +703,7 @@ class Line:
                     break
 
         if commas > 1:
-            self.leaves.pop()
+            self.remove_trailing_comma()
             return True
 
         return False
@@ -694,52 +731,49 @@ class Line:
 
         return False
 
-    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:
-        """"""Hack a standalone comment to act as a trailing comment for line splitting.
-
-        If this line has brackets and a standalone `comment`, we need to adapt
-        it to be able to still reformat the line.
-
-        This is not perfect, the line to which the standalone comment gets
-        appended will appear ""too long"" when splitting.
-        """"""
-        if not (
+    def append_comment(self, comment: Leaf) -> bool:
+        """"""Add an inline or standalone comment to the line.""""""
+        if (
             comment.type == STANDALONE_COMMENT
             and self.bracket_tracker.any_open_brackets()
         ):
+            comment.prefix = ''
             return False
 
-        comment.type = token.COMMENT
-        comment.prefix = '\n' + '    ' * (self.depth + 1)
-        return self.append_comment(comment)
-
-    def append_comment(self, comment: Leaf) -> bool:
-        """"""Add an inline comment to the line.""""""
         if comment.type != token.COMMENT:
             return False
 
-        try:
-            after = id(self.last_non_delimiter())
-        except LookupError:
+        after = len(self.leaves) - 1
+        if after == -1:
             comment.type = STANDALONE_COMMENT
             comment.prefix = ''
             return False
 
         else:
-            if after in self.comments:
-                self.comments[after].value += str(comment)
-            else:
-                self.comments[after] = comment
+            self.comments.append((after, comment))
             return True
 
-    def last_non_delimiter(self) -> Leaf:
-        """"""Return the last non-delimiter on the line. Raise LookupError otherwise.""""""
-        for i in range(len(self.leaves)):
-            last = self.leaves[-i - 1]
-            if not is_delimiter(last):
-                return last
+    def comments_after(self, leaf: Leaf) -> Iterator[Leaf]:
+        """"""Generate comments that should appear directly after `leaf`.""""""
+        for _leaf_index, _leaf in enumerate(self.leaves):
+            if leaf is _leaf:
+                break
+
+        else:
+            return
 
-        raise LookupError(""No non-delimiters found"")
+        for index, comment_after in self.comments:
+            if _leaf_index == index:
+                yield comment_after
+
+    def remove_trailing_comma(self) -> None:
+        """"""Remove the trailing comma and moves the comments attached to it.""""""
+        comma_index = len(self.leaves) - 1
+        for i in range(len(self.comments)):
+            comment_index, comment = self.comments[i]
+            if comment_index == comma_index:
+                self.comments[i] = (comma_index - 1, comment)
+        self.leaves.pop()
 
     def __str__(self) -> str:
         """"""Render the line.""""""
@@ -752,7 +786,7 @@ class Line:
         res = f'{first.prefix}{indent}{first.value}'
         for leaf in leaves:
             res += str(leaf)
-        for comment in self.comments.values():
+        for _, comment in self.comments:
             res += str(comment)
         return res + '\n'
 
@@ -809,10 +843,6 @@ class UnformattedLines(Line):
         """"""Does nothing and returns False.""""""
         return False
 
-    def maybe_adapt_standalone_comment(self, comment: Leaf) -> bool:
-        """"""Does nothing and returns False.""""""
-        return False
-
 
 @dataclass
 class EmptyLineTracker:
@@ -1439,23 +1469,24 @@ def split_line(
     If `py36` is True, splitting may generate syntax that is only compatible
     with Python 3.6 and later.
     """"""
-    if isinstance(line, UnformattedLines):
+    if isinstance(line, UnformattedLines) or line.is_comment:
         yield line
         return
 
     line_str = str(line).strip('\n')
-    if len(line_str) <= line_length and '\n' not in line_str:
+    if (
+        len(line_str) <= line_length
+        and '\n' not in line_str  # multiline strings
+        and not line.contains_standalone_comments
+    ):
         yield line
         return
 
+    split_funcs: List[SplitFunc]
     if line.is_def:
         split_funcs = [left_hand_split]
     elif line.inside_brackets:
-        split_funcs = [delimiter_split]
-        if '\n' not in line_str:
-            # Only attempt RHS if we don't have multiline strings or comments
-            # on this line.
-            split_funcs.append(right_hand_split)
+        split_funcs = [delimiter_split, standalone_comment_split, right_hand_split]
     else:
         split_funcs = [right_hand_split]
     for split_func in split_funcs:
@@ -1464,7 +1495,7 @@ def split_line(
         # split altogether.
         result: List[Line] = []
         try:
-            for l in split_func(line, py36=py36):
+            for l in split_func(line, py36):
                 if str(l).strip('\n') == line_str:
                     raise CannotSplit(""Split function returned an unchanged result"")
 
@@ -1517,8 +1548,7 @@ def left_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:
     ):
         for leaf in leaves:
             result.append(leaf, preformatted=True)
-            comment_after = line.comments.get(id(leaf))
-            if comment_after:
+            for comment_after in line.comments_after(leaf):
                 result.append(comment_after, preformatted=True)
     bracket_split_succeeded_or_raise(head, body, tail)
     for result in (head, body, tail):
@@ -1557,8 +1587,7 @@ def right_hand_split(line: Line, py36: bool = False) -> Iterator[Line]:
     ):
         for leaf in leaves:
             result.append(leaf, preformatted=True)
-            comment_after = line.comments.get(id(leaf))
-            if comment_after:
+            for comment_after in line.comments_after(leaf):
                 result.append(comment_after, preformatted=True)
     bracket_split_succeeded_or_raise(head, body, tail)
     for result in (head, body, tail):
@@ -1592,10 +1621,25 @@ def bracket_split_succeeded_or_raise(head: Line, body: Line, tail: Line) -> None
             )
 
 
+def dont_increase_indentation(split_func: SplitFunc) -> SplitFunc:
+    """"""Normalize prefix of the first leaf in every line returned by `split_func`.
+
+    This is a decorator over relevant split functions.
+    """"""
+
+    @wraps(split_func)
+    def split_wrapper(line: Line, py36: bool = False) -> Iterator[Line]:
+        for l in split_func(line, py36):
+            normalize_prefix(l.leaves[0], inside_brackets=True)
+            yield l
+
+    return split_wrapper
+
+
+@dont_increase_indentation
 def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:
     """"""Split according to delimiters of the highest priority.
 
-    This kind of split doesn't increase indentation.
     If `py36` is True, the split will add trailing commas also in function
     signatures that contain `*` and `**`.
     """"""
@@ -1615,11 +1659,24 @@ def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:
     current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
     lowest_depth = sys.maxsize
     trailing_comma_safe = True
+
+    def append_to_line(leaf: Leaf) -> Iterator[Line]:
+        """"""Append `leaf` to current line or to new line if appending impossible.""""""
+        nonlocal current_line
+        try:
+            current_line.append_safe(leaf, preformatted=True)
+        except ValueError as ve:
+            yield current_line
+
+            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
+            current_line.append(leaf)
+
     for leaf in line.leaves:
-        current_line.append(leaf, preformatted=True)
-        comment_after = line.comments.get(id(leaf))
-        if comment_after:
-            current_line.append(comment_after, preformatted=True)
+        yield from append_to_line(leaf)
+
+        for comment_after in line.comments_after(leaf):
+            yield from append_to_line(comment_after)
+
         lowest_depth = min(lowest_depth, leaf.bracket_depth)
         if (
             leaf.bracket_depth == lowest_depth
@@ -1629,7 +1686,6 @@ def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:
             trailing_comma_safe = trailing_comma_safe and py36
         leaf_priority = delimiters.get(id(leaf))
         if leaf_priority == delimiter_priority:
-            normalize_prefix(current_line.leaves[0], inside_brackets=True)
             yield current_line
 
             current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
@@ -1640,7 +1696,40 @@ def delimiter_split(line: Line, py36: bool = False) -> Iterator[Line]:
             and trailing_comma_safe
         ):
             current_line.append(Leaf(token.COMMA, ','))
-        normalize_prefix(current_line.leaves[0], inside_brackets=True)
+        yield current_line
+
+
+@dont_increase_indentation
+def standalone_comment_split(line: Line, py36: bool = False) -> Iterator[Line]:
+    """"""Split standalone comments from the rest of the line.""""""
+    for leaf in line.leaves:
+        if leaf.type == STANDALONE_COMMENT:
+            if leaf.bracket_depth == 0:
+                break
+
+    else:
+        raise CannotSplit(""Line does not have any standalone comments"")
+
+    current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
+
+    def append_to_line(leaf: Leaf) -> Iterator[Line]:
+        """"""Append `leaf` to current line or to new line if appending impossible.""""""
+        nonlocal current_line
+        try:
+            current_line.append_safe(leaf, preformatted=True)
+        except ValueError as ve:
+            yield current_line
+
+            current_line = Line(depth=line.depth, inside_brackets=line.inside_brackets)
+            current_line.append(leaf)
+
+    for leaf in line.leaves:
+        yield from append_to_line(leaf)
+
+        for comment_after in line.comments_after(leaf):
+            yield from append_to_line(comment_after)
+
+    if current_line:
         yield current_line
 
 
"
"black","18","dbe26161fa68632d608a440666a0960a32630902","00a302560b92951c22f0f4c8d618cf63de39bd57","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index e59a1e5..36e49b0 100644
--- a/black.py
+++ b/black.py
@@ -4,6 +4,7 @@ from asyncio.base_events import BaseEventLoop
 from concurrent.futures import Executor, ProcessPoolExecutor
 from enum import Enum, Flag
 from functools import partial, wraps
+import io
 import keyword
 import logging
 from multiprocessing import Manager
@@ -465,8 +466,9 @@ def format_file_in_place(
     """"""
     if src.suffix == "".pyi"":
         mode |= FileMode.PYI
-    with tokenize.open(src) as src_buffer:
-        src_contents = src_buffer.read()
+
+    with open(src, ""rb"") as buf:
+        newline, encoding, src_contents = prepare_input(buf.read())
     try:
         dst_contents = format_file_contents(
             src_contents, line_length=line_length, fast=fast, mode=mode
@@ -475,7 +477,7 @@ def format_file_in_place(
         return False
 
     if write_back == write_back.YES:
-        with open(src, ""w"", encoding=src_buffer.encoding) as f:
+        with open(src, ""w"", encoding=encoding, newline=newline) as f:
             f.write(dst_contents)
     elif write_back == write_back.DIFF:
         src_name = f""{src}  (original)""
@@ -484,7 +486,14 @@ def format_file_in_place(
         if lock:
             lock.acquire()
         try:
-            sys.stdout.write(diff_contents)
+            f = io.TextIOWrapper(
+                sys.stdout.buffer,
+                encoding=encoding,
+                newline=newline,
+                write_through=True,
+            )
+            f.write(diff_contents)
+            f.detach()
         finally:
             if lock:
                 lock.release()
@@ -503,7 +512,7 @@ def format_stdin_to_stdout(
     `line_length`, `fast`, `is_pyi`, and `force_py36` arguments are passed to
     :func:`format_file_contents`.
     """"""
-    src = sys.stdin.read()
+    newline, encoding, src = prepare_input(sys.stdin.buffer.read())
     dst = src
     try:
         dst = format_file_contents(src, line_length=line_length, fast=fast, mode=mode)
@@ -514,11 +523,25 @@ def format_stdin_to_stdout(
 
     finally:
         if write_back == WriteBack.YES:
-            sys.stdout.write(dst)
+            f = io.TextIOWrapper(
+                sys.stdout.buffer,
+                encoding=encoding,
+                newline=newline,
+                write_through=True,
+            )
+            f.write(dst)
+            f.detach()
         elif write_back == WriteBack.DIFF:
             src_name = ""<stdin>  (original)""
             dst_name = ""<stdin>  (formatted)""
-            sys.stdout.write(diff(src, dst, src_name, dst_name))
+            f = io.TextIOWrapper(
+                sys.stdout.buffer,
+                encoding=encoding,
+                newline=newline,
+                write_through=True,
+            )
+            f.write(diff(src, dst, src_name, dst_name))
+            f.detach()
 
 
 def format_file_contents(
@@ -579,6 +602,19 @@ def format_str(
     return dst_contents
 
 
+def prepare_input(src: bytes) -> Tuple[str, str, str]:
+    """"""Analyze `src` and return a tuple of (newline, encoding, decoded_contents)
+
+    Where `newline` is either CRLF or LF, and `decoded_contents` is decoded with
+    universal newlines (i.e. only LF).
+    """"""
+    srcbuf = io.BytesIO(src)
+    encoding, lines = tokenize.detect_encoding(srcbuf.readline)
+    newline = ""\r\n"" if b""\r\n"" == lines[0][-2:] else ""\n""
+    srcbuf.seek(0)
+    return newline, encoding, io.TextIOWrapper(srcbuf, encoding).read()
+
+
 GRAMMARS = [
     pygram.python_grammar_no_print_statement_no_exec_statement,
     pygram.python_grammar_no_print_statement,
@@ -590,8 +626,7 @@ def lib2to3_parse(src_txt: str) -> Node:
     """"""Given a string with source, return the lib2to3 Node.""""""
     grammar = pygram.python_grammar_no_print_statement
     if src_txt[-1] != ""\n"":
-        nl = ""\r\n"" if ""\r\n"" in src_txt[:1024] else ""\n""
-        src_txt += nl
+        src_txt += ""\n""
     for grammar in GRAMMARS:
         drv = driver.Driver(grammar, pytree.convert)
         try:
"
"black","14","3bdd42389128bbbe8b64a8e050563f09bff99979","dd8bde6d2fbfe8a7a11093e761a0cb5837efa96a","black.py;tests/data/python2_unicode_literals.py","black.py;tests/data/python2_unicode_literals.py","diff --git a/black.py b/black.py;diff --git a/tests/data/python2_unicode_literals.py b/tests/data/python2_unicode_literals.py","tests/test_black.py","","diff --git a/black.py b/black.py
index f49e6df..36a180d 100644
--- a/black.py
+++ b/black.py
@@ -20,6 +20,7 @@ from typing import (
     Callable,
     Collection,
     Dict,
+    Generator,
     Generic,
     Iterable,
     Iterator,
@@ -2910,7 +2911,23 @@ def generate_trailers_to_omit(line: Line, line_length: int) -> Iterator[Set[Leaf
 
 def get_future_imports(node: Node) -> Set[str]:
     """"""Return a set of __future__ imports in the file.""""""
-    imports = set()
+    imports: Set[str] = set()
+
+    def get_imports_from_children(children: List[LN]) -> Generator[str, None, None]:
+        for child in children:
+            if isinstance(child, Leaf):
+                if child.type == token.NAME:
+                    yield child.value
+            elif child.type == syms.import_as_name:
+                orig_name = child.children[0]
+                assert isinstance(orig_name, Leaf), ""Invalid syntax parsing imports""
+                assert orig_name.type == token.NAME, ""Invalid syntax parsing imports""
+                yield orig_name.value
+            elif child.type == syms.import_as_names:
+                yield from get_imports_from_children(child.children)
+            else:
+                assert False, ""Invalid syntax parsing imports""
+
     for child in node.children:
         if child.type != syms.simple_stmt:
             break
@@ -2929,15 +2946,7 @@ def get_future_imports(node: Node) -> Set[str]:
             module_name = first_child.children[1]
             if not isinstance(module_name, Leaf) or module_name.value != ""__future__"":
                 break
-            for import_from_child in first_child.children[3:]:
-                if isinstance(import_from_child, Leaf):
-                    if import_from_child.type == token.NAME:
-                        imports.add(import_from_child.value)
-                else:
-                    assert import_from_child.type == syms.import_as_names
-                    for leaf in import_from_child.children:
-                        if isinstance(leaf, Leaf) and leaf.type == token.NAME:
-                            imports.add(leaf.value)
+            imports |= set(get_imports_from_children(first_child.children[3:]))
         else:
             break
     return imports
diff --git a/tests/data/python2_unicode_literals.py b/tests/data/python2_unicode_literals.py
index ae27919..2fe7039 100644
--- a/tests/data/python2_unicode_literals.py
+++ b/tests/data/python2_unicode_literals.py
@@ -1,5 +1,7 @@
 #!/usr/bin/env python2
-from __future__ import unicode_literals
+from __future__ import unicode_literals as _unicode_literals
+from __future__ import absolute_import
+from __future__ import print_function as lol, with_function
 
 u'hello'
 U""hello""
@@ -9,7 +11,9 @@ Ur""hello""
 
 
 #!/usr/bin/env python2
-from __future__ import unicode_literals
+from __future__ import unicode_literals as _unicode_literals
+from __future__ import absolute_import
+from __future__ import print_function as lol, with_function
 
 ""hello""
 ""hello""
"
"black","3","8126b4f6a9342290de4655e6a8a78cd288ce7daa","7a14a37981862ef418f3cdb4a7e2375856f97529","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index d9348a3..26a2915 100644
--- a/black.py
+++ b/black.py
@@ -394,7 +394,7 @@ def target_version_option_callback(
 @click.option(
     ""--config"",
     type=click.Path(
-        exists=False, file_okay=True, dir_okay=False, readable=True, allow_dash=False
+        exists=True, file_okay=True, dir_okay=False, readable=True, allow_dash=False
     ),
     is_eager=True,
     callback=read_pyproject_toml,
"
"black","16","fb34c9e19589d05f92084a28940837151251ebd6","42a3fe53319a8c02858c2a96989ed1339f84515a","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index 9d9bada..e3b3882 100644
--- a/black.py
+++ b/black.py
@@ -2941,11 +2941,24 @@ def gen_python_files_in_dir(
     """"""Generate all files under `path` whose paths are not excluded by the
     `exclude` regex, but are included by the `include` regex.
 
+    Symbolic links pointing outside of the root directory are ignored.
+
     `report` is where output about exclusions goes.
     """"""
     assert root.is_absolute(), f""INTERNAL ERROR: `root` must be absolute but is {root}""
     for child in path.iterdir():
-        normalized_path = ""/"" + child.resolve().relative_to(root).as_posix()
+        try:
+            normalized_path = ""/"" + child.resolve().relative_to(root).as_posix()
+        except ValueError:
+            if child.is_symlink():
+                report.path_ignored(
+                    child,
+                    ""is a symbolic link that points outside of the root directory"",
+                )
+                continue
+
+            raise
+
         if child.is_dir():
             normalized_path += ""/""
         exclude_match = exclude.search(normalized_path)
"
"black","2","c8ca6b2b9ff3510bee12129824cebfc2fc51e5b2","892eddacd215d685e136686b7f629ade70adca83","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index 2df03f7..e55e4fe 100644
--- a/black.py
+++ b/black.py
@@ -3116,18 +3116,49 @@ def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:
     """"""
     container: Optional[LN] = container_of(leaf)
     while container is not None and container.type != token.ENDMARKER:
-        is_fmt_on = False
-        for comment in list_comments(container.prefix, is_endmarker=False):
-            if comment.value in FMT_ON:
-                is_fmt_on = True
-            elif comment.value in FMT_OFF:
-                is_fmt_on = False
-        if is_fmt_on:
+        if fmt_on(container):
             return
 
-        yield container
+        # fix for fmt: on in children
+        if contains_fmt_on_at_column(container, leaf.column):
+            for child in container.children:
+                if contains_fmt_on_at_column(child, leaf.column):
+                    return
+                yield child
+        else:
+            yield container
+            container = container.next_sibling
+
 
-        container = container.next_sibling
+def fmt_on(container: LN) -> bool:
+    is_fmt_on = False
+    for comment in list_comments(container.prefix, is_endmarker=False):
+        if comment.value in FMT_ON:
+            is_fmt_on = True
+        elif comment.value in FMT_OFF:
+            is_fmt_on = False
+    return is_fmt_on
+
+
+def contains_fmt_on_at_column(container: LN, column: int) -> bool:
+    for child in container.children:
+        if (
+            isinstance(child, Node)
+            and first_leaf_column(child) == column
+            or isinstance(child, Leaf)
+            and child.column == column
+        ):
+            if fmt_on(child):
+                return True
+
+    return False
+
+
+def first_leaf_column(node: Node) -> Optional[int]:
+    for child in node.children:
+        if isinstance(child, Leaf):
+            return child.column
+    return None
 
 
 def maybe_make_parens_invisible_in_atom(node: LN, parent: LN) -> bool:
"
"black","8","e6ddb68c786256e1cb0c76b42d10c212ef34cb2a","6b994fdb8ab70ce4c2eafb8f2f0ff2648f3ff1ef","black.py","black.py","diff --git a/black.py b/black.py","tests/data/comments7.py;tests/test_black.py","","diff --git a/black.py b/black.py
index dd6e372..9ecfbe1 100644
--- a/black.py
+++ b/black.py
@@ -2405,10 +2405,17 @@ def bracket_split_build_line(
         if leaves:
             # Since body is a new indent level, remove spurious leading whitespace.
             normalize_prefix(leaves[0], inside_brackets=True)
-            # Ensure a trailing comma when expected.
+            # Ensure a trailing comma for imports, but be careful not to add one after
+            # any comments.
             if original.is_import:
-                if leaves[-1].type != token.COMMA:
-                    leaves.append(Leaf(token.COMMA, "",""))
+                for i in range(len(leaves) - 1, -1, -1):
+                    if leaves[i].type == STANDALONE_COMMENT:
+                        continue
+                    elif leaves[i].type == token.COMMA:
+                        break
+                    else:
+                        leaves.insert(i + 1, Leaf(token.COMMA, "",""))
+                        break
     # Populate the line
     for leaf in leaves:
         result.append(leaf, preformatted=True)
"
"black","23","8de552eb4f0fbf1ad84812cde71489cc00d3ed1f","6316e293ac30a2837ec20eba289fd28a2a18cf89","black.py;blib2to3/pygram.py;tests/function.py","black.py;blib2to3/pygram.py;tests/function.py","diff --git a/black.py b/black.py;diff --git a/blib2to3/pygram.py b/blib2to3/pygram.py;diff --git a/tests/function.py b/tests/function.py","tests/python2.py;tests/test_black.py","","diff --git a/black.py b/black.py
index bb8ec2e..7935cdc 100644
--- a/black.py
+++ b/black.py
@@ -235,23 +235,36 @@ def format_str(src_contents: str, line_length: int) -> FileContent:
     return dst_contents
 
 
+GRAMMARS = [
+    pygram.python_grammar_no_print_statement_no_exec_statement,
+    pygram.python_grammar_no_print_statement,
+    pygram.python_grammar_no_exec_statement,
+    pygram.python_grammar,
+]
+
+
 def lib2to3_parse(src_txt: str) -> Node:
     """"""Given a string with source, return the lib2to3 Node.""""""
     grammar = pygram.python_grammar_no_print_statement
-    drv = driver.Driver(grammar, pytree.convert)
     if src_txt[-1] != '\n':
         nl = '\r\n' if '\r\n' in src_txt[:1024] else '\n'
         src_txt += nl
-    try:
-        result = drv.parse_string(src_txt, True)
-    except ParseError as pe:
-        lineno, column = pe.context[1]
-        lines = src_txt.splitlines()
+    for grammar in GRAMMARS:
+        drv = driver.Driver(grammar, pytree.convert)
         try:
-            faulty_line = lines[lineno - 1]
-        except IndexError:
-            faulty_line = ""<line number missing in source>""
-        raise ValueError(f""Cannot parse: {lineno}:{column}: {faulty_line}"") from None
+            result = drv.parse_string(src_txt, True)
+            break
+
+        except ParseError as pe:
+            lineno, column = pe.context[1]
+            lines = src_txt.splitlines()
+            try:
+                faulty_line = lines[lineno - 1]
+            except IndexError:
+                faulty_line = ""<line number missing in source>""
+            exc = ValueError(f""Cannot parse: {lineno}:{column}: {faulty_line}"")
+    else:
+        raise exc from None
 
     if isinstance(result, Leaf):
         result = Node(syms.file_input, [result])
@@ -903,6 +916,17 @@ def whitespace(leaf: Leaf) -> str:  # noqa C901
         ):
             return NO
 
+        elif (
+            prevp.type == token.RIGHTSHIFT
+            and prevp.parent
+            and prevp.parent.type == syms.shift_expr
+            and prevp.prev_sibling
+            and prevp.prev_sibling.type == token.NAME
+            and prevp.prev_sibling.value == 'print'
+        ):
+            # Python 2 print chevron
+            return NO
+
     elif prev.type in OPENING_BRACKETS:
         return NO
 
@@ -1538,7 +1562,12 @@ def assert_equivalent(src: str, dst: str) -> None:
     try:
         src_ast = ast.parse(src)
     except Exception as exc:
-        raise AssertionError(f""cannot parse source: {exc}"") from None
+        major, minor = sys.version_info[:2]
+        raise AssertionError(
+            f""cannot use --safe with this file; failed to parse source file ""
+            f""with Python {major}.{minor}'s builtin AST. Re-run with --fast ""
+            f""or stop using deprecated Python 2 syntax. AST error message: {exc}""
+        )
 
     try:
         dst_ast = ast.parse(dst)
diff --git a/blib2to3/pygram.py b/blib2to3/pygram.py
index c4ff9d1..bf55ab4 100644
--- a/blib2to3/pygram.py
+++ b/blib2to3/pygram.py
@@ -36,5 +36,12 @@ python_symbols = Symbols(python_grammar)
 python_grammar_no_print_statement = python_grammar.copy()
 del python_grammar_no_print_statement.keywords[""print""]
 
+python_grammar_no_exec_statement = python_grammar.copy()
+del python_grammar_no_exec_statement.keywords[""exec""]
+
+python_grammar_no_print_statement_no_exec_statement = python_grammar.copy()
+del python_grammar_no_print_statement_no_exec_statement.keywords[""print""]
+del python_grammar_no_print_statement_no_exec_statement.keywords[""exec""]
+
 pattern_grammar = driver.load_packaged_grammar(""blib2to3"", _PATTERN_GRAMMAR_FILE)
 pattern_symbols = Symbols(pattern_grammar)
diff --git a/tests/function.py b/tests/function.py
index 7fa6866..888ef9f 100644
--- a/tests/function.py
+++ b/tests/function.py
@@ -14,8 +14,9 @@ def func_no_args():
   for i in range(10):
     print(i)
     continue
+  exec(""new-style exec"", {}, {})
   return None
-async def coroutine(arg):
+async def coroutine(arg, exec=False):
  ""Single-line docstring. Multiline is harder to reformat.""
  async with some_connection() as conn:
      await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)
@@ -93,10 +94,11 @@ def func_no_args():
         print(i)
         continue
 
+    exec(""new-style exec"", {}, {})
     return None
 
 
-async def coroutine(arg):
+async def coroutine(arg, exec=False):
     ""Single-line docstring. Multiline is harder to reformat.""
     async with some_connection() as conn:
         await conn.do_what_i_mean('SELECT bobby, tables FROM xkcd', timeout=2)
"
"black","1","26c9465a22c732ab1e17b0dec578fa3432e9b558","c0a7582e3d4cc8bec3b7f5a6c52b36880dcb57d7","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index 2a913fc..fc1597a 100644
--- a/black.py
+++ b/black.py
@@ -618,7 +618,14 @@ def reformat_many(
     if sys.platform == ""win32"":
         # Work around https://bugs.python.org/issue26903
         worker_count = min(worker_count, 61)
-    executor = ProcessPoolExecutor(max_workers=worker_count)
+    try:
+        executor = ProcessPoolExecutor(max_workers=worker_count)
+    except OSError:
+        # we arrive here if the underlying system does not support multi-processing
+        # like in AWS Lambda, in which case we gracefully fallback to the default
+        # mono-process Executor by using None
+        executor = None
+
     try:
         loop.run_until_complete(
             schedule_formatting(
@@ -633,7 +640,8 @@ def reformat_many(
         )
     finally:
         shutdown(loop)
-        executor.shutdown()
+        if executor is not None:
+            executor.shutdown()
 
 
 async def schedule_formatting(
@@ -643,7 +651,7 @@ async def schedule_formatting(
     mode: Mode,
     report: ""Report"",
     loop: asyncio.AbstractEventLoop,
-    executor: Executor,
+    executor: Optional[Executor],
 ) -> None:
     """"""Run formatting of `sources` in parallel using the provided `executor`.
 
"
"black","7","18119d38466652ae818436cb497f601294ed4558","de806405d2934b629d67e2a6317ad7e826765a20","black.py","black.py","diff --git a/black.py b/black.py","tests/data/tupleassign.py;tests/test_black.py","","diff --git a/black.py b/black.py
index cada4d0..7bfcfca 100644
--- a/black.py
+++ b/black.py
@@ -2726,6 +2726,14 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:
 
     check_lpar = False
     for index, child in enumerate(list(node.children)):
+        # Add parentheses around long tuple unpacking in assignments.
+        if (
+            index == 0
+            and isinstance(child, Node)
+            and child.type == syms.testlist_star_expr
+        ):
+            check_lpar = True
+
         if check_lpar:
             if child.type == syms.atom:
                 if maybe_make_parens_invisible_in_atom(child, parent=node):
@@ -2757,7 +2765,11 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:
                 lpar = Leaf(token.LPAR, """")
                 rpar = Leaf(token.RPAR, """")
                 index = child.remove() or 0
-                node.insert_child(index, Node(syms.atom, [lpar, child, rpar]))
+                prefix = child.prefix
+                child.prefix = """"
+                new_child = Node(syms.atom, [lpar, child, rpar])
+                new_child.prefix = prefix
+                node.insert_child(index, new_child)
 
         check_lpar = isinstance(child, Leaf) and child.value in parens_after
 
"
"black","10","f6643c4f0cfbae1f2493fdfce46cfbae3d26f46b","66aa676278948368dff251dffd58c850cb8b889e","blib2to3/pgen2/driver.py","blib2to3/pgen2/driver.py","diff --git a/blib2to3/pgen2/driver.py b/blib2to3/pgen2/driver.py","tests/test_black.py","","diff --git a/blib2to3/pgen2/driver.py b/blib2to3/pgen2/driver.py
index 72d9f47..6626c05 100644
--- a/blib2to3/pgen2/driver.py
+++ b/blib2to3/pgen2/driver.py
@@ -131,10 +131,8 @@ class Driver(object):
                     current_line = """"
                     current_column = 0
                     wait_for_nl = False
-            elif char == ' ':
+            elif char in ' \t':
                 current_column += 1
-            elif char == '\t':
-                current_column += 4
             elif char == '\n':
                 # unexpected empty line
                 current_column = 0
"
"black","13","b719d85ccc330170e40b2617307a7e3b2a0bab14","883689366ce0f0e0ddd66d81360c61abfd19b01a","blib2to3/pgen2/tokenize.py","blib2to3/pgen2/tokenize.py","diff --git a/blib2to3/pgen2/tokenize.py b/blib2to3/pgen2/tokenize.py","tests/data/python37.py;tests/test_black.py","","diff --git a/blib2to3/pgen2/tokenize.py b/blib2to3/pgen2/tokenize.py
index 9a7664b..1f51ff0 100644
--- a/blib2to3/pgen2/tokenize.py
+++ b/blib2to3/pgen2/tokenize.py
@@ -516,13 +516,14 @@ def generate_tokens(readline):
                         stashed = tok
                         continue
 
-                    if token == 'def':
+                    if token in ('def', 'for'):
                         if (stashed
                                 and stashed[0] == NAME
                                 and stashed[1] == 'async'):
 
-                            async_def = True
-                            async_def_indent = indents[-1]
+                            if token == 'def':
+                                async_def = True
+                                async_def_indent = indents[-1]
 
                             yield (ASYNC, stashed[1],
                                    stashed[2], stashed[3],
"
"black","19","337a4199f90ca48a19cf26511e0cec330b13bd4e","29e97d1d4a7717f1bd0ca35cacf2f2ce6d815b0c","black.py","black.py","diff --git a/black.py b/black.py","tests/comments6.py;tests/test_black.py","","diff --git a/black.py b/black.py
index 15a7547..a03b9aa 100644
--- a/black.py
+++ b/black.py
@@ -1044,6 +1044,10 @@ class EmptyLineTracker:
                 # Don't insert empty lines between decorators.
                 return 0, 0
 
+            if is_decorator and self.previous_line and self.previous_line.is_comment:
+                # Don't insert empty lines between decorator comments.
+                return 0, 0
+
             newlines = 2
             if current_line.depth:
                 newlines -= 1
"
"black","9","026c81b83454f176a9f9253cbfb70be2c159d822","d6db1c12a8e14833fe22da377cddc2bd1f43dc14","black.py","black.py","diff --git a/black.py b/black.py","tests/data/python2_print_function.py;tests/test_black.py","","diff --git a/black.py b/black.py
index b727666..c44bc9b 100644
--- a/black.py
+++ b/black.py
@@ -726,13 +726,13 @@ def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:
     if not target_versions:
         return GRAMMARS
     elif all(not version.is_python2() for version in target_versions):
-        # Python 2-compatible code, so don't try Python 3 grammar.
+        # Python 3-compatible code, so don't try Python 2 grammar
         return [
             pygram.python_grammar_no_print_statement_no_exec_statement,
             pygram.python_grammar_no_print_statement,
         ]
     else:
-        return [pygram.python_grammar]
+        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]
 
 
 def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
"
"black","21","c071af761e1550c6e4ebab8e5af747d2d8fdd48e","8e7848c63efe36f09e4651bece8c0efc34a1c3e1","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index 537ba59..587d9b3 100644
--- a/black.py
+++ b/black.py
@@ -2325,7 +2325,7 @@ def dump_to_file(*output: str) -> str:
     import tempfile
 
     with tempfile.NamedTemporaryFile(
-        mode=""w"", prefix=""blk_"", suffix="".log"", delete=False
+        mode=""w"", prefix=""blk_"", suffix="".log"", delete=False, encoding=""utf8""
     ) as f:
         for lines in output:
             f.write(lines)
"
"black","15","8a8c58252cc023ae250d6febd24f50a8166450d4","df2ae3bbe6c45298aabb6c04e85cb353205626f1","black.py","black.py","diff --git a/black.py b/black.py","tests/data/fmtonoff2.py;tests/test_black.py","","diff --git a/black.py b/black.py
index 7682f7c..7e39c92 100644
--- a/black.py
+++ b/black.py
@@ -29,7 +29,6 @@ from typing import (
     Sequence,
     Set,
     Tuple,
-    Type,
     TypeVar,
     Union,
     cast,
@@ -90,34 +89,6 @@ class CannotSplit(Exception):
     """"""
 
 
-class FormatError(Exception):
-    """"""Base exception for `# fmt: on` and `# fmt: off` handling.
-
-    It holds the number of bytes of the prefix consumed before the format
-    control comment appeared.
-    """"""
-
-    def __init__(self, consumed: int) -> None:
-        super().__init__(consumed)
-        self.consumed = consumed
-
-    def trim_prefix(self, leaf: Leaf) -> None:
-        leaf.prefix = leaf.prefix[self.consumed :]
-
-    def leaf_from_consumed(self, leaf: Leaf) -> Leaf:
-        """"""Returns a new Leaf from the consumed part of the prefix.""""""
-        unformatted_prefix = leaf.prefix[: self.consumed]
-        return Leaf(token.NEWLINE, unformatted_prefix)
-
-
-class FormatOn(FormatError):
-    """"""Found a comment like `# fmt: on` in the file.""""""
-
-
-class FormatOff(FormatError):
-    """"""Found a comment like `# fmt: off` in the file.""""""
-
-
 class WriteBack(Enum):
     NO = 0
     YES = 1
@@ -759,13 +730,15 @@ class DebugVisitor(Visitor[T]):
             out(f"" {node.value!r}"", fg=""blue"", bold=False)
 
     @classmethod
-    def show(cls, code: str) -> None:
+    def show(cls, code: Union[str, Leaf, Node]) -> None:
         """"""Pretty-print the lib2to3 AST of a given string of `code`.
 
         Convenience method for debugging.
         """"""
         v: DebugVisitor[None] = DebugVisitor()
-        list(v.visit(lib2to3_parse(code)))
+        if isinstance(code, str):
+            code = lib2to3_parse(code)
+        list(v.visit(code))
 
 
 KEYWORDS = set(keyword.kwlist)
@@ -1306,55 +1279,6 @@ class Line:
         return bool(self.leaves or self.comments)
 
 
-class UnformattedLines(Line):
-    """"""Just like :class:`Line` but stores lines which aren't reformatted.""""""
-
-    def append(self, leaf: Leaf, preformatted: bool = True) -> None:
-        """"""Just add a new `leaf` to the end of the lines.
-
-        The `preformatted` argument is ignored.
-
-        Keeps track of indentation `depth`, which is useful when the user
-        says `# fmt: on`. Otherwise, doesn't do anything with the `leaf`.
-        """"""
-        try:
-            list(generate_comments(leaf))
-        except FormatOn as f_on:
-            self.leaves.append(f_on.leaf_from_consumed(leaf))
-            raise
-
-        self.leaves.append(leaf)
-        if leaf.type == token.INDENT:
-            self.depth += 1
-        elif leaf.type == token.DEDENT:
-            self.depth -= 1
-
-    def __str__(self) -> str:
-        """"""Render unformatted lines from leaves which were added with `append()`.
-
-        `depth` is not used for indentation in this case.
-        """"""
-        if not self:
-            return ""\n""
-
-        res = """"
-        for leaf in self.leaves:
-            res += str(leaf)
-        return res
-
-    def append_comment(self, comment: Leaf) -> bool:
-        """"""Not implemented in this class. Raises `NotImplementedError`.""""""
-        raise NotImplementedError(""Unformatted lines don't store comments separately."")
-
-    def maybe_remove_trailing_comma(self, closing: Leaf) -> bool:
-        """"""Does nothing and returns False.""""""
-        return False
-
-    def maybe_increment_for_loop_variable(self, leaf: Leaf) -> bool:
-        """"""Does nothing and returns False.""""""
-        return False
-
-
 @dataclass
 class EmptyLineTracker:
     """"""Provides a stateful method that returns the number of potential extra
@@ -1376,9 +1300,6 @@ class EmptyLineTracker:
         This is for separating `def`, `async def` and `class` with extra empty
         lines (two on module-level).
         """"""
-        if isinstance(current_line, UnformattedLines):
-            return 0, 0
-
         before, after = self._maybe_empty_lines(current_line)
         before -= self.previous_after
         self.previous_after = after
@@ -1482,7 +1403,7 @@ class LineGenerator(Visitor[Line]):
     current_line: Line = Factory(Line)
     remove_u_prefix: bool = False
 
-    def line(self, indent: int = 0, type: Type[Line] = Line) -> Iterator[Line]:
+    def line(self, indent: int = 0) -> Iterator[Line]:
         """"""Generate a line.
 
         If the line is empty, only emit if it makes sense.
@@ -1491,67 +1412,39 @@ class LineGenerator(Visitor[Line]):
         If any lines were generated, set up a new current_line.
         """"""
         if not self.current_line:
-            if self.current_line.__class__ == type:
-                self.current_line.depth += indent
-            else:
-                self.current_line = type(depth=self.current_line.depth + indent)
+            self.current_line.depth += indent
             return  # Line is empty, don't emit. Creating a new one unnecessary.
 
         complete_line = self.current_line
-        self.current_line = type(depth=complete_line.depth + indent)
+        self.current_line = Line(depth=complete_line.depth + indent)
         yield complete_line
 
-    def visit(self, node: LN) -> Iterator[Line]:
-        """"""Main method to visit `node` and its children.
-
-        Yields :class:`Line` objects.
-        """"""
-        if isinstance(self.current_line, UnformattedLines):
-            # File contained `# fmt: off`
-            yield from self.visit_unformatted(node)
-
-        else:
-            yield from super().visit(node)
-
     def visit_default(self, node: LN) -> Iterator[Line]:
         """"""Default `visit_*()` implementation. Recurses to children of `node`.""""""
         if isinstance(node, Leaf):
             any_open_brackets = self.current_line.bracket_tracker.any_open_brackets()
-            try:
-                for comment in generate_comments(node):
-                    if any_open_brackets:
-                        # any comment within brackets is subject to splitting
-                        self.current_line.append(comment)
-                    elif comment.type == token.COMMENT:
-                        # regular trailing comment
-                        self.current_line.append(comment)
-                        yield from self.line()
-
-                    else:
-                        # regular standalone comment
-                        yield from self.line()
-
-                        self.current_line.append(comment)
-                        yield from self.line()
-
-            except FormatOff as f_off:
-                f_off.trim_prefix(node)
-                yield from self.line(type=UnformattedLines)
-                yield from self.visit(node)
-
-            except FormatOn as f_on:
-                # This only happens here if somebody says ""fmt: on"" multiple
-                # times in a row.
-                f_on.trim_prefix(node)
-                yield from self.visit_default(node)
+            for comment in generate_comments(node):
+                if any_open_brackets:
+                    # any comment within brackets is subject to splitting
+                    self.current_line.append(comment)
+                elif comment.type == token.COMMENT:
+                    # regular trailing comment
+                    self.current_line.append(comment)
+                    yield from self.line()
 
-            else:
-                normalize_prefix(node, inside_brackets=any_open_brackets)
-                if self.normalize_strings and node.type == token.STRING:
-                    normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)
-                    normalize_string_quotes(node)
-                if node.type not in WHITESPACE:
-                    self.current_line.append(node)
+                else:
+                    # regular standalone comment
+                    yield from self.line()
+
+                    self.current_line.append(comment)
+                    yield from self.line()
+
+            normalize_prefix(node, inside_brackets=any_open_brackets)
+            if self.normalize_strings and node.type == token.STRING:
+                normalize_string_prefix(node, remove_u_prefix=self.remove_u_prefix)
+                normalize_string_quotes(node)
+            if node.type not in WHITESPACE:
+                self.current_line.append(node)
         yield from super().visit_default(node)
 
     def visit_INDENT(self, node: Node) -> Iterator[Line]:
@@ -1648,23 +1541,10 @@ class LineGenerator(Visitor[Line]):
         yield from self.visit_default(leaf)
         yield from self.line()
 
-    def visit_unformatted(self, node: LN) -> Iterator[Line]:
-        """"""Used when file contained a `# fmt: off`.""""""
-        if isinstance(node, Node):
-            for child in node.children:
-                yield from self.visit(child)
-
-        else:
-            try:
-                self.current_line.append(node)
-            except FormatOn as f_on:
-                f_on.trim_prefix(node)
-                yield from self.line()
-                yield from self.visit(node)
-
-            if node.type == token.ENDMARKER:
-                # somebody decided not to put a final `# fmt: on`
-                yield from self.line()
+    def visit_STANDALONE_COMMENT(self, leaf: Leaf) -> Iterator[Line]:
+        if not self.current_line.bracket_tracker.any_open_brackets():
+            yield from self.line()
+        yield from self.visit_default(leaf)
 
     def __attrs_post_init__(self) -> None:
         """"""You are in a twisty little maze of passages.""""""
@@ -1969,6 +1849,9 @@ def container_of(leaf: Leaf) -> LN:
         if parent.children[0].prefix != same_prefix:
             break
 
+        if parent.type == syms.file_input:
+            break
+
         if parent.type in SURROUNDED_BY_BRACKETS:
             break
 
@@ -2106,16 +1989,6 @@ def generate_comments(leaf: LN) -> Iterator[Leaf]:
     """"""
     for pc in list_comments(leaf.prefix, is_endmarker=leaf.type == token.ENDMARKER):
         yield Leaf(pc.type, pc.value, prefix=""\n"" * pc.newlines)
-        if pc.value in FMT_ON:
-            raise FormatOn(pc.consumed)
-
-        if pc.value in FMT_OFF:
-            if pc.type == STANDALONE_COMMENT:
-                raise FormatOff(pc.consumed)
-
-            prev = preceding_leaf(leaf)
-            if not prev or prev.type in WHITESPACE:  # standalone comment in disguise
-                raise FormatOff(pc.consumed)
 
 
 @dataclass
@@ -2188,7 +2061,7 @@ def split_line(
     If `py36` is True, splitting may generate syntax that is only compatible
     with Python 3.6 and later.
     """"""
-    if isinstance(line, UnformattedLines) or line.is_comment:
+    if line.is_comment:
         yield line
         return
 
@@ -2680,28 +2553,29 @@ def normalize_invisible_parens(node: Node, parens_after: Set[str]) -> None:
 
 
 def normalize_fmt_off(node: Node) -> None:
-    """"""Allow `# fmt: off`/`# fmt: on` within bracket pairs.
-
-    Ignores `# fmt: off` and `# fmt: on` outside of brackets.
-
-    Raises :exc:`SyntaxError` if no matching `# fmt: on` is found for a `# fmt: off`
-    given inside brackets.
-    """"""
+    """"""Convert content between `# fmt: off`/`# fmt: on` into standalone comments.""""""
     try_again = True
     while try_again:
-        try_again = hide_fmt_off(node)
+        try_again = convert_one_fmt_off_pair(node)
 
 
-def hide_fmt_off(node: Node) -> bool:
-    bt = BracketTracker()
-    for leaf in node.leaves():
-        bt.mark(leaf)
-        if bt.depth == 0:
-            continue
+def convert_one_fmt_off_pair(node: Node) -> bool:
+    """"""Convert content of a single `# fmt: off`/`# fmt: on` into a standalone comment.
 
+    Returns True if a pair was converted.
+    """"""
+    for leaf in node.leaves():
         previous_consumed = 0
         for comment in list_comments(leaf.prefix, is_endmarker=False):
             if comment.value in FMT_OFF:
+                # We only want standalone comments. If there's no previous leaf or
+                # the previous leaf is indentation, it's a standalone comment in
+                # disguise.
+                if comment.type != STANDALONE_COMMENT:
+                    prev = preceding_leaf(leaf)
+                    if prev and prev.type not in WHITESPACE:
+                        continue
+
                 ignored_nodes = list(generate_ignored_nodes(leaf))
                 first = ignored_nodes[0]  # Can be a container node with the `leaf`.
                 parent = first.parent
@@ -2710,6 +2584,10 @@ def hide_fmt_off(node: Node) -> bool:
                 hidden_value = (
                     comment.value + ""\n"" + """".join(str(n) for n in ignored_nodes)
                 )
+                if hidden_value.endswith(""\n""):
+                    # That happens when one of the `ignored_nodes` ended with a NEWLINE
+                    # leaf (possibly followed by a DEDENT).
+                    hidden_value = hidden_value[:-1]
                 first_idx = None
                 for ignored in ignored_nodes:
                     index = ignored.remove()
@@ -2733,8 +2611,12 @@ def hide_fmt_off(node: Node) -> bool:
 
 
 def generate_ignored_nodes(leaf: Leaf) -> Iterator[LN]:
+    """"""Starting from the container of `leaf`, generate all leaves until `# fmt: on`.
+
+    Stops at the end of the block.
+    """"""
     container: Optional[LN] = container_of(leaf)
-    while container is not None:
+    while container is not None and container.type != token.ENDMARKER:
         for comment in list_comments(container.prefix, is_endmarker=False):
             if comment.value in FMT_ON:
                 return
"
"black","11","283a5d53a8d57e8e186a08c9fbf249e1fbe7bc94","024c9cab55da7bd3236fd88759c9735d6149b464","black.py","black.py","diff --git a/black.py b/black.py","tests/data/comments6.py;tests/test_black.py","","diff --git a/black.py b/black.py
index 52c5b0c..fb8e474 100644
--- a/black.py
+++ b/black.py
@@ -2112,8 +2112,19 @@ def split_line(
         return
 
     line_str = str(line).strip(""\n"")
-    if not line.should_explode and is_line_short_enough(
-        line, line_length=line_length, line_str=line_str
+
+    # we don't want to split special comments like type annotations
+    # https://github.com/python/typing/issues/186
+    has_special_comment = False
+    for leaf in line.leaves:
+        for comment in line.comments_after(leaf):
+            if leaf.type == token.COMMA and is_special_comment(comment):
+                has_special_comment = True
+
+    if (
+        not has_special_comment
+        and not line.should_explode
+        and is_line_short_enough(line, line_length=line_length, line_str=line_str)
     ):
         yield line
         return
@@ -2462,6 +2473,16 @@ def is_import(leaf: Leaf) -> bool:
     )
 
 
+def is_special_comment(leaf: Leaf) -> bool:
+    """"""Return True if the given leaf is a special comment.
+    Only returns true for type comments for now.""""""
+    t = leaf.type
+    v = leaf.value
+    return bool(
+        (t == token.COMMENT or t == STANDALONE_COMMENT) and (v.startswith(""# type:""))
+    )
+
+
 def normalize_prefix(leaf: Leaf, *, inside_brackets: bool) -> None:
     """"""Leave existing extra newlines if not `inside_brackets`. Remove everything
     else.
@@ -2951,6 +2972,7 @@ def ensure_visible(leaf: Leaf) -> None:
 
 def should_explode(line: Line, opening_bracket: Leaf) -> bool:
     """"""Should `line` immediately be split with `delimiter_split()` after RHS?""""""
+
     if not (
         opening_bracket.parent
         and opening_bracket.parent.type in {syms.atom, syms.import_from}
"
"black","6","8c8adedc2a74a494c24f93e405b6418ac32f54cd","f8617f975d56e81cfb4070ce65584f7b29a77e7a","black.py;blib2to3/pgen2/driver.py;blib2to3/pgen2/tokenize.py;tests/data/python37.py","black.py;blib2to3/pgen2/driver.py;blib2to3/pgen2/tokenize.py;tests/data/python37.py","diff --git a/black.py b/black.py;diff --git a/blib2to3/pgen2/driver.py b/blib2to3/pgen2/driver.py;diff --git a/blib2to3/pgen2/tokenize.py b/blib2to3/pgen2/tokenize.py;diff --git a/tests/data/python37.py b/tests/data/python37.py","tests/test_black.py;tests/data/comment_after_escaped_newline.py","","diff --git a/black.py b/black.py
index c96d205..c8aa30b 100644
--- a/black.py
+++ b/black.py
@@ -48,6 +48,7 @@ from blib2to3 import pygram, pytree
 from blib2to3.pgen2 import driver, token
 from blib2to3.pgen2.grammar import Grammar
 from blib2to3.pgen2.parse import ParseError
+from blib2to3.pgen2.tokenize import TokenizerConfig
 
 
 __version__ = ""19.3b0""
@@ -136,19 +137,28 @@ class Feature(Enum):
     NUMERIC_UNDERSCORES = 3
     TRAILING_COMMA_IN_CALL = 4
     TRAILING_COMMA_IN_DEF = 5
+    # The following two feature-flags are mutually exclusive, and exactly one should be
+    # set for every version of python.
+    ASYNC_IS_VALID_IDENTIFIER = 6
+    ASYNC_IS_RESERVED_KEYWORD = 7
 
 
 VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
-    TargetVersion.PY27: set(),
-    TargetVersion.PY33: {Feature.UNICODE_LITERALS},
-    TargetVersion.PY34: {Feature.UNICODE_LITERALS},
-    TargetVersion.PY35: {Feature.UNICODE_LITERALS, Feature.TRAILING_COMMA_IN_CALL},
+    TargetVersion.PY27: {Feature.ASYNC_IS_VALID_IDENTIFIER},
+    TargetVersion.PY33: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
+    TargetVersion.PY34: {Feature.UNICODE_LITERALS, Feature.ASYNC_IS_VALID_IDENTIFIER},
+    TargetVersion.PY35: {
+        Feature.UNICODE_LITERALS,
+        Feature.TRAILING_COMMA_IN_CALL,
+        Feature.ASYNC_IS_VALID_IDENTIFIER,
+    },
     TargetVersion.PY36: {
         Feature.UNICODE_LITERALS,
         Feature.F_STRINGS,
         Feature.NUMERIC_UNDERSCORES,
         Feature.TRAILING_COMMA_IN_CALL,
         Feature.TRAILING_COMMA_IN_DEF,
+        Feature.ASYNC_IS_VALID_IDENTIFIER,
     },
     TargetVersion.PY37: {
         Feature.UNICODE_LITERALS,
@@ -156,6 +166,7 @@ VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
         Feature.NUMERIC_UNDERSCORES,
         Feature.TRAILING_COMMA_IN_CALL,
         Feature.TRAILING_COMMA_IN_DEF,
+        Feature.ASYNC_IS_RESERVED_KEYWORD,
     },
     TargetVersion.PY38: {
         Feature.UNICODE_LITERALS,
@@ -163,6 +174,7 @@ VERSION_TO_FEATURES: Dict[TargetVersion, Set[Feature]] = {
         Feature.NUMERIC_UNDERSCORES,
         Feature.TRAILING_COMMA_IN_CALL,
         Feature.TRAILING_COMMA_IN_DEF,
+        Feature.ASYNC_IS_RESERVED_KEYWORD,
     },
 }
 
@@ -748,20 +760,62 @@ def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
         return tiow.read(), encoding, newline
 
 
-def get_grammars(target_versions: Set[TargetVersion]) -> List[Grammar]:
+@dataclass(frozen=True)
+class ParserConfig:
+    grammar: Grammar
+    tokenizer_config: TokenizerConfig = TokenizerConfig()
+
+
+def get_parser_configs(target_versions: Set[TargetVersion]) -> List[ParserConfig]:
     if not target_versions:
         # No target_version specified, so try all grammars.
         return [
-            pygram.python_grammar_no_print_statement_no_exec_statement,
-            pygram.python_grammar_no_print_statement,
-            pygram.python_grammar,
+            # Python 3.7+
+            ParserConfig(
+                pygram.python_grammar_no_print_statement_no_exec_statement,
+                TokenizerConfig(async_is_reserved_keyword=True),
+            ),
+            # Python 3.0-3.6
+            ParserConfig(
+                pygram.python_grammar_no_print_statement_no_exec_statement,
+                TokenizerConfig(async_is_reserved_keyword=False),
+            ),
+            # Python 2.7 with future print_function import
+            ParserConfig(pygram.python_grammar_no_print_statement),
+            # Python 2.7
+            ParserConfig(pygram.python_grammar),
         ]
     elif all(version.is_python2() for version in target_versions):
         # Python 2-only code, so try Python 2 grammars.
-        return [pygram.python_grammar_no_print_statement, pygram.python_grammar]
+        return [
+            # Python 2.7 with future print_function import
+            ParserConfig(pygram.python_grammar_no_print_statement),
+            # Python 2.7
+            ParserConfig(pygram.python_grammar),
+        ]
     else:
         # Python 3-compatible code, so only try Python 3 grammar.
-        return [pygram.python_grammar_no_print_statement_no_exec_statement]
+        configs = []
+        # If we have to parse both, try to parse async as a keyword first
+        if not supports_feature(target_versions, Feature.ASYNC_IS_VALID_IDENTIFIER):
+            # Python 3.7+
+            configs.append(
+                ParserConfig(
+                    pygram.python_grammar_no_print_statement_no_exec_statement,
+                    TokenizerConfig(async_is_reserved_keyword=True),
+                )
+            )
+        if not supports_feature(target_versions, Feature.ASYNC_IS_RESERVED_KEYWORD):
+            # Python 3.0-3.6
+            configs.append(
+                ParserConfig(
+                    pygram.python_grammar_no_print_statement_no_exec_statement,
+                    TokenizerConfig(async_is_reserved_keyword=False),
+                )
+            )
+        # At least one of the above branches must have been taken, because every Python
+        # version has exactly one of the two 'ASYNC_IS_*' flags
+        return configs
 
 
 def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -> Node:
@@ -769,8 +823,12 @@ def lib2to3_parse(src_txt: str, target_versions: Iterable[TargetVersion] = ()) -
     if src_txt[-1:] != ""\n"":
         src_txt += ""\n""
 
-    for grammar in get_grammars(set(target_versions)):
-        drv = driver.Driver(grammar, pytree.convert)
+    for parser_config in get_parser_configs(set(target_versions)):
+        drv = driver.Driver(
+            parser_config.grammar,
+            pytree.convert,
+            tokenizer_config=parser_config.tokenizer_config,
+        )
         try:
             result = drv.parse_string(src_txt, True)
             break
diff --git a/blib2to3/pgen2/driver.py b/blib2to3/pgen2/driver.py
index 63b60bb..e681b52 100644
--- a/blib2to3/pgen2/driver.py
+++ b/blib2to3/pgen2/driver.py
@@ -29,12 +29,19 @@ from . import grammar, parse, token, tokenize, pgen
 
 class Driver(object):
 
-    def __init__(self, grammar, convert=None, logger=None):
+    def __init__(
+        self,
+        grammar,
+        convert=None,
+        logger=None,
+        tokenizer_config=tokenize.TokenizerConfig(),
+    ):
         self.grammar = grammar
         if logger is None:
             logger = logging.getLogger(__name__)
         self.logger = logger
         self.convert = convert
+        self.tokenizer_config = tokenizer_config
 
     def parse_tokens(self, tokens, debug=False):
         """"""Parse a series of tokens and return the syntax tree.""""""
@@ -97,7 +104,7 @@ class Driver(object):
 
     def parse_stream_raw(self, stream, debug=False):
         """"""Parse a stream and return the syntax tree.""""""
-        tokens = tokenize.generate_tokens(stream.readline)
+        tokens = tokenize.generate_tokens(stream.readline, config=self.tokenizer_config)
         return self.parse_tokens(tokens, debug)
 
     def parse_stream(self, stream, debug=False):
@@ -111,7 +118,10 @@ class Driver(object):
 
     def parse_string(self, text, debug=False):
         """"""Parse a string and return the syntax tree.""""""
-        tokens = tokenize.generate_tokens(io.StringIO(text).readline)
+        tokens = tokenize.generate_tokens(
+            io.StringIO(text).readline,
+            config=self.tokenizer_config,
+        )
         return self.parse_tokens(tokens, debug)
 
     def _partially_consume_prefix(self, prefix, column):
diff --git a/blib2to3/pgen2/tokenize.py b/blib2to3/pgen2/tokenize.py
index 1f51ff0..43e1d59 100644
--- a/blib2to3/pgen2/tokenize.py
+++ b/blib2to3/pgen2/tokenize.py
@@ -31,6 +31,7 @@ __credits__ = \
 
 import re
 from codecs import BOM_UTF8, lookup
+from attr import dataclass
 from blib2to3.pgen2.token import *
 
 from . import token
@@ -137,6 +138,10 @@ single_quoted = (
 
 tabsize = 8
 
+@dataclass(frozen=True)
+class TokenizerConfig:
+    async_is_reserved_keyword: bool = False
+
 class TokenError(Exception): pass
 
 class StopTokenizing(Exception): pass
@@ -334,7 +339,7 @@ def untokenize(iterable):
     ut = Untokenizer()
     return ut.untokenize(iterable)
 
-def generate_tokens(readline):
+def generate_tokens(readline, config: TokenizerConfig = TokenizerConfig()):
     """"""
     The generate_tokens() generator requires one argument, readline, which
     must be a callable object which provides the same interface as the
@@ -356,6 +361,9 @@ def generate_tokens(readline):
     contline = None
     indents = [0]
 
+    # If we know we're parsing 3.7+, we can unconditionally parse `async` and
+    # `await` as keywords.
+    async_is_reserved_keyword = config.async_is_reserved_keyword
     # 'stashed' and 'async_*' are used for async/await parsing
     stashed = None
     async_def = False
@@ -506,7 +514,7 @@ def generate_tokens(readline):
                         yield (STRING, token, spos, epos, line)
                 elif initial.isidentifier():               # ordinary name
                     if token in ('async', 'await'):
-                        if async_def:
+                        if async_is_reserved_keyword or async_def:
                             yield (ASYNC if token == 'async' else AWAIT,
                                    token, spos, epos, line)
                             continue
diff --git a/tests/data/python37.py b/tests/data/python37.py
index 9781ff6..4401b7b 100644
--- a/tests/data/python37.py
+++ b/tests/data/python37.py
@@ -14,6 +14,14 @@ async def func():
                 self.async_inc, arange(8), batch_size=3
             )
         ]
+
+def awaited_generator_value(n):
+    return (await awaitable for awaitable in awaitable_list)
+
+def make_arange(n):
+    return (i * 2 for i in range(n) if await wrap(i))
+
+
 # output
 
 
@@ -39,3 +47,11 @@ async def func():
                 self.async_inc, arange(8), batch_size=3
             )
         ]
+
+
+def awaited_generator_value(n):
+    return (await awaitable for awaitable in awaitable_list)
+
+
+def make_arange(n):
+    return (i * 2 for i in range(n) if await wrap(i))
"
"black","17","bbc09a4f013f2a584f143f3f5e3f76f6082367d4","7fc6ce990669464f5172b63fafa3724f5f308be3","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index 46c0907..c6b018f 100644
--- a/black.py
+++ b/black.py
@@ -607,6 +607,9 @@ def decode_bytes(src: bytes) -> Tuple[FileContent, Encoding, NewLine]:
     """"""
     srcbuf = io.BytesIO(src)
     encoding, lines = tokenize.detect_encoding(srcbuf.readline)
+    if not lines:
+        return """", encoding, ""\n""
+
     newline = ""\r\n"" if b""\r\n"" == lines[0][-2:] else ""\n""
     srcbuf.seek(0)
     with io.TextIOWrapper(srcbuf, encoding) as tiow:
@@ -623,7 +626,7 @@ GRAMMARS = [
 def lib2to3_parse(src_txt: str) -> Node:
     """"""Given a string with source, return the lib2to3 Node.""""""
     grammar = pygram.python_grammar_no_print_statement
-    if src_txt[-1] != ""\n"":
+    if src_txt[-1:] != ""\n"":
         src_txt += ""\n""
     for grammar in GRAMMARS:
         drv = driver.Driver(grammar, pytree.convert)
"
"black","20","2e52a2b3ecc0fe025439c3db05a4457ab14f167b","06e95b1e9bcd43c4574840f8174ba4b2c5d281bd","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index dd2e2d1..eafc9e7 100644
--- a/black.py
+++ b/black.py
@@ -341,8 +341,8 @@ def format_file_in_place(
         with open(src, ""w"", encoding=src_buffer.encoding) as f:
             f.write(dst_contents)
     elif write_back == write_back.DIFF:
-        src_name = f""{src.name}  (original)""
-        dst_name = f""{src.name}  (formatted)""
+        src_name = f""{src}  (original)""
+        dst_name = f""{src}  (formatted)""
         diff_contents = diff(src_contents, dst_contents, src_name, dst_name)
         if lock:
             lock.acquire()
"
"black","12","8b340e210271a8108995fd479c55dbc0a34466bd","b53cb9474348e13533ccba3735191a55ef3da6c4","black.py","black.py","diff --git a/black.py b/black.py","tests/data/bracketmatch.py;tests/test_black.py","","diff --git a/black.py b/black.py
index 85cb45b..0f166c6 100644
--- a/black.py
+++ b/black.py
@@ -877,8 +877,8 @@ class BracketTracker:
     bracket_match: Dict[Tuple[Depth, NodeType], Leaf] = Factory(dict)
     delimiters: Dict[LeafID, Priority] = Factory(dict)
     previous: Optional[Leaf] = None
-    _for_loop_variable: int = 0
-    _lambda_arguments: int = 0
+    _for_loop_depths: List[int] = Factory(list)
+    _lambda_argument_depths: List[int] = Factory(list)
 
     def mark(self, leaf: Leaf) -> None:
         """"""Mark `leaf` with bracket-related metadata. Keep track of delimiters.
@@ -951,16 +951,21 @@ class BracketTracker:
         """"""
         if leaf.type == token.NAME and leaf.value == ""for"":
             self.depth += 1
-            self._for_loop_variable += 1
+            self._for_loop_depths.append(self.depth)
             return True
 
         return False
 
     def maybe_decrement_after_for_loop_variable(self, leaf: Leaf) -> bool:
         """"""See `maybe_increment_for_loop_variable` above for explanation.""""""
-        if self._for_loop_variable and leaf.type == token.NAME and leaf.value == ""in"":
+        if (
+            self._for_loop_depths
+            and self._for_loop_depths[-1] == self.depth
+            and leaf.type == token.NAME
+            and leaf.value == ""in""
+        ):
             self.depth -= 1
-            self._for_loop_variable -= 1
+            self._for_loop_depths.pop()
             return True
 
         return False
@@ -973,16 +978,20 @@ class BracketTracker:
         """"""
         if leaf.type == token.NAME and leaf.value == ""lambda"":
             self.depth += 1
-            self._lambda_arguments += 1
+            self._lambda_argument_depths.append(self.depth)
             return True
 
         return False
 
     def maybe_decrement_after_lambda_arguments(self, leaf: Leaf) -> bool:
         """"""See `maybe_increment_lambda_arguments` above for explanation.""""""
-        if self._lambda_arguments and leaf.type == token.COLON:
+        if (
+            self._lambda_argument_depths
+            and self._lambda_argument_depths[-1] == self.depth
+            and leaf.type == token.COLON
+        ):
             self.depth -= 1
-            self._lambda_arguments -= 1
+            self._lambda_argument_depths.pop()
             return True
 
         return False
"
"black","4","65ea568e3301951f26e0e3b98f6d5dc80132e917","c7495b9aa098ef7a358fc74556359d21c6a4ba11","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py;tests/data/beginning_backslash.py","","diff --git a/black.py b/black.py
index f7022d8..05edf1a 100644
--- a/black.py
+++ b/black.py
@@ -1480,7 +1480,13 @@ class EmptyLineTracker:
         lines (two on module-level).
         """"""
         before, after = self._maybe_empty_lines(current_line)
-        before -= self.previous_after
+        before = (
+            # Black should not insert empty lines at the beginning
+            # of the file
+            0
+            if self.previous_line is None
+            else before - self.previous_after
+        )
         self.previous_after = after
         self.previous_line = current_line
         return before, after
"
"black","5","1bbb01b854d168d76ebe4bf78961c2152ae075d9","9394de150ebf0adc426523f46dc08e8b2b2b0b63","black.py","black.py","diff --git a/black.py b/black.py","tests/test_black.py","","diff --git a/black.py b/black.py
index 635eba2..8318674 100644
--- a/black.py
+++ b/black.py
@@ -1352,7 +1352,10 @@ class Line:
             bracket_depth = leaf.bracket_depth
             if bracket_depth == depth and leaf.type == token.COMMA:
                 commas += 1
-                if leaf.parent and leaf.parent.type == syms.arglist:
+                if leaf.parent and leaf.parent.type in {
+                    syms.arglist,
+                    syms.typedargslist,
+                }:
                     commas += 1
                     break
 
@@ -2488,9 +2491,13 @@ def bracket_split_build_line(
         if leaves:
             # Since body is a new indent level, remove spurious leading whitespace.
             normalize_prefix(leaves[0], inside_brackets=True)
-            # Ensure a trailing comma for imports, but be careful not to add one after
-            # any comments.
-            if original.is_import:
+            # Ensure a trailing comma for imports and standalone function arguments, but
+            # be careful not to add one after any comments.
+            no_commas = original.is_def and not any(
+                l.type == token.COMMA for l in leaves
+            )
+
+            if original.is_import or no_commas:
                 for i in range(len(leaves) - 1, -1, -1):
                     if leaves[i].type == STANDALONE_COMMENT:
                         continue
"
